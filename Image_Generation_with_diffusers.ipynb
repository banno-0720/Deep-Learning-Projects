{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM46GLDAy/AXWws5GM2enzu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/banno-0720/Deep-Learning-Projects/blob/main/Image_Generation_with_diffusers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Generation with Hugging Face Diffusers\n",
        "\n",
        "This notebook demonstrates various types of image generation using the Hugging Face Diffusers library. Each section below includes:\n",
        "\n",
        "1. Overview: What the method is and when to use it.\n",
        "\n",
        "2. Key Components: Main classes and functions involved.\n",
        "\n",
        "3. Code Snippet: Example code to perform the task."
      ],
      "metadata": {
        "id": "RMPRJzy_7iXt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Types of Image Generation**\n",
        "\n",
        "1. [**Unconditional Image Generation:**](https://colab.research.google.com/drive/1EhcLos_JHnr6V_mUAzK5EaoKfIv-y__T#scrollTo=xn4TrSxg18gq) Generates images solely from random noise without any conditioning signals (e.g., prompts).\n",
        "\n",
        "2. [**Text-to-Image Generation:**](https://colab.research.google.com/drive/1EhcLos_JHnr6V_mUAzK5EaoKfIv-y__T#scrollTo=vBGsZUA31Tqt) Produces images conditioned on a text prompt using a text encoder (e.g., CLIP).\n",
        "\n",
        "3. [**Image-to-Image Generation:**](https://colab.research.google.com/drive/1EhcLos_JHnr6V_mUAzK5EaoKfIv-y__T#scrollTo=4l71F2su7NJ_) Transforms or refines an existing image by adding controlled noise and denoising it.\n",
        "\n",
        "4. [**Inpainting:**](https://colab.research.google.com/drive/1EhcLos_JHnr6V_mUAzK5EaoKfIv-y__T#scrollTo=2tLkpDSsFbbt) Fills in or edits masked regions of an image based on a textual description.\n",
        "\n",
        "5. [**Mega Pipeline:**](https://colab.research.google.com/drive/1EhcLos_JHnr6V_mUAzK5EaoKfIv-y__T#scrollTo=sfzYkQlULD5a) Combines text-to-image, image-to-image, and inpainting capabilities into a single unified pipeline."
      ],
      "metadata": {
        "id": "rYytd1fG7s6J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For logging into your hugging-face account use this"
      ],
      "metadata": {
        "id": "99QOai8GYYgW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRYp9uV90M1r"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Unconditional Image Generation\n",
        "\n",
        "**Overview**\n",
        "\n",
        "Trains a diffusion model (DDPM) to generate images from pure noise. Useful for learning core diffusion techniques.\n",
        "\n",
        "**Key Components**\n",
        "\n",
        "- UNet2DModel: Core network predicting noise residuals.\n",
        "\n",
        "- DDPMScheduler: Defines forward (noising) and reverse (denoising) processes.\n",
        "\n",
        "- Accelerator: Simplifies mixed-precision, multi-GPU training."
      ],
      "metadata": {
        "id": "xn4TrSxg18gq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ruz-EHHV0M1v"
      },
      "source": [
        "## Training configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsrVsPWR0M1w"
      },
      "source": [
        "For convenience, create a `TrainingConfig` class containing the training hyperparameters (feel free to adjust them):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwAAjkVt0M1z"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    image_size = 128  # the generated image resolution\n",
        "    train_batch_size = 16\n",
        "    eval_batch_size = 16  # how many images to sample during evaluation\n",
        "    num_epochs = 50\n",
        "    gradient_accumulation_steps = 1\n",
        "    learning_rate = 1e-4\n",
        "    lr_warmup_steps = 500\n",
        "    save_image_epochs = 10\n",
        "    save_model_epochs = 30\n",
        "    mixed_precision = \"fp16\"  # `no` for float32, `fp16` for automatic mixed precision\n",
        "    output_dir = \"ddpm-butterflies-128\"  # the model name locally and on the HF Hub\n",
        "\n",
        "    push_to_hub = True  # whether to upload the saved model to the HF Hub\n",
        "    hub_model_id = \"HimanshuGoyal2004/beautiful_butterflies\"  # the name of the repository to create on the HF Hub\n",
        "    hub_private_repo = None\n",
        "    overwrite_output_dir = True  # overwrite the old model when re-running the notebook\n",
        "    seed = 0\n",
        "\n",
        "\n",
        "config = TrainingConfig()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ej7UEiKL0M11"
      },
      "source": [
        "## Load the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNhq2FKQ0M12"
      },
      "source": [
        "You can easily load the [Smithsonian Butterflies](https://huggingface.co/datasets/huggan/smithsonian_butterflies_subset) dataset with the ðŸ¤— Datasets library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XoAEE4Ii0M12"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "config.dataset_name = \"huggan/smithsonian_butterflies_subset\"\n",
        "dataset = load_dataset(config.dataset_name, split=\"train\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfHUp7Fd0M13"
      },
      "source": [
        "<Tip>\n",
        "\n",
        "ðŸ’¡ You can find additional datasets from the [HugGan Community Event](https://huggingface.co/huggan) or you can use your own dataset by creating a local [`ImageFolder`](https://huggingface.co/docs/datasets/image_dataset#imagefolder). Set `config.dataset_name` to the repository id of the dataset if it is from the HugGan Community Event, or `imagefolder` if you're using your own images.\n",
        "\n",
        "</Tip>\n",
        "\n",
        "ðŸ¤— Datasets uses the [Image](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Image) feature to automatically decode the image data and load it as a [`PIL.Image`](https://pillow.readthedocs.io/en/stable/reference/Image.html) which we can visualize:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oc14xQ6i0M14"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
        "for i, image in enumerate(dataset[:4][\"image\"]):\n",
        "    axs[i].imshow(image)\n",
        "    axs[i].set_axis_off()\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sAKbT6B0M15"
      },
      "source": [
        "<div class=\"flex justify-center\">\n",
        "    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/butterflies_ds.png\"/>\n",
        "</div>\n",
        "\n",
        "The images are all different sizes though, so you'll need to preprocess them first:\n",
        "\n",
        "* `Resize` changes the image size to the one defined in `config.image_size`.\n",
        "* `RandomHorizontalFlip` augments the dataset by randomly mirroring the images.\n",
        "* `Normalize` is important to rescale the pixel values into a [-1, 1] range, which is what the model expects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WorY_o90M16"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "preprocess = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((config.image_size, config.image_size)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5], [0.5]),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drIGfCEZ0M17"
      },
      "source": [
        "Use ðŸ¤— Datasets' [set_transform](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.set_transform) method to apply the `preprocess` function on the fly during training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tux4HJxX0M18"
      },
      "outputs": [],
      "source": [
        "def transform(examples):\n",
        "    images = [preprocess(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
        "    return {\"images\": images}\n",
        "\n",
        "\n",
        "dataset.set_transform(transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QPY-Clc0M1_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=config.train_batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWgytA6L0M1_"
      },
      "source": [
        "## Create a UNet2DModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7aI5l4T0M2A"
      },
      "source": [
        "Pretrained models in ðŸ§¨ Diffusers are easily created from their model class with the parameters you want. For example, to create a [UNet2DModel](https://huggingface.co/docs/diffusers/main/en/api/models/unet2d#diffusers.UNet2DModel):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cfyY0-80M2B"
      },
      "outputs": [],
      "source": [
        "from diffusers import UNet2DModel\n",
        "\n",
        "model = UNet2DModel(\n",
        "    sample_size=config.image_size,  # the target image resolution\n",
        "    in_channels=3,  # the number of input channels, 3 for RGB images\n",
        "    out_channels=3,  # the number of output channels\n",
        "    layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
        "    block_out_channels=(128, 128, 256, 256, 512, 512),  # the number of output channels for each UNet block\n",
        "    down_block_types=(\n",
        "        \"DownBlock2D\",  # a regular ResNet downsampling block\n",
        "        \"DownBlock2D\",\n",
        "        \"DownBlock2D\",\n",
        "        \"DownBlock2D\",\n",
        "        \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
        "        \"DownBlock2D\",\n",
        "    ),\n",
        "    up_block_types=(\n",
        "        \"UpBlock2D\",  # a regular ResNet upsampling block\n",
        "        \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
        "        \"UpBlock2D\",\n",
        "        \"UpBlock2D\",\n",
        "        \"UpBlock2D\",\n",
        "        \"UpBlock2D\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SysBNGh70M2C"
      },
      "outputs": [],
      "source": [
        "sample_image = dataset[0][\"images\"].unsqueeze(0)\n",
        "print(\"Input shape:\", sample_image.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0dTj_Vh0M2F"
      },
      "outputs": [],
      "source": [
        "print(\"Output shape:\", model(sample_image, timestep=0).sample.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Csxe_kJ30M2G"
      },
      "source": [
        "## Create a scheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3m_nnU3F0M2H"
      },
      "source": [
        "The scheduler behaves differently depending on whether you're using the model for training or inference. During inference, the scheduler generates image from the noise. During training, the scheduler takes a model output - or a sample - from a specific point in the diffusion process and applies noise to the image according to a *noise schedule* and an *update rule*.\n",
        "\n",
        "Let's take a look at the [DDPMScheduler](https://huggingface.co/docs/diffusers/main/en/api/schedulers/ddpm#diffusers.DDPMScheduler) and use the `add_noise` method to add some random noise to the `sample_image` from before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFGb26aq0M2H"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from diffusers import DDPMScheduler\n",
        "\n",
        "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
        "noise = torch.randn(sample_image.shape)\n",
        "timesteps = torch.LongTensor([50])\n",
        "noisy_image = noise_scheduler.add_noise(sample_image, noise, timesteps)\n",
        "\n",
        "Image.fromarray(((noisy_image.permute(0, 2, 3, 1) + 1.0) * 127.5).type(torch.uint8).numpy()[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcjDK4Ae0M2I"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "noise_pred = model(noisy_image, timesteps).sample\n",
        "loss = F.mse_loss(noise_pred, noise)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCVE39zW0M2J"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Arqj9ZkA0M2K"
      },
      "outputs": [],
      "source": [
        "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
        "lr_scheduler = get_cosine_schedule_with_warmup(\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=config.lr_warmup_steps,\n",
        "    num_training_steps=(len(train_dataloader) * config.num_epochs),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RFpbdGB0M2M"
      },
      "outputs": [],
      "source": [
        "from diffusers import DDPMPipeline\n",
        "from diffusers.utils import make_image_grid\n",
        "import os\n",
        "\n",
        "def evaluate(config, epoch, pipeline):\n",
        "    # Sample some images from random noise (this is the backward diffusion process).\n",
        "    # The default pipeline output type is `List[PIL.Image]`\n",
        "    images = pipeline(\n",
        "        batch_size=config.eval_batch_size,\n",
        "        generator=torch.Generator(device='cpu').manual_seed(config.seed), # Use a separate torch generator to avoid rewinding the random state of the main training loop\n",
        "    ).images\n",
        "\n",
        "    # Make a grid out of the images\n",
        "    image_grid = make_image_grid(images, rows=4, cols=4)\n",
        "\n",
        "    # Save the images\n",
        "    test_dir = os.path.join(config.output_dir, \"samples\")\n",
        "    os.makedirs(test_dir, exist_ok=True)\n",
        "    image_grid.save(f\"{test_dir}/{epoch:04d}.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STd5fQCj0M2M"
      },
      "source": [
        "Now you can wrap all these components together in a training loop with ðŸ¤— Accelerate for easy TensorBoard logging, gradient accumulation, and mixed precision training. To upload the model to the Hub, write a function to get your repository name and information and then push it to the Hub.\n",
        "\n",
        "<Tip>\n",
        "\n",
        "ðŸ’¡ The training loop below may look intimidating and long, but it'll be worth it later when you launch your training in just one line of code! If you can't wait and want to start generating images, feel free to copy and run the code below. You can always come back and examine the training loop more closely later, like when you're waiting for your model to finish training. ðŸ¤—\n",
        "\n",
        "</Tip>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wv6CBBVM0M2N"
      },
      "outputs": [],
      "source": [
        "from accelerate import Accelerator\n",
        "from huggingface_hub import create_repo, upload_folder\n",
        "from tqdm.auto import tqdm\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "def train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler):\n",
        "    # Initialize accelerator and tensorboard logging\n",
        "    accelerator = Accelerator(\n",
        "        mixed_precision=config.mixed_precision,\n",
        "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
        "        log_with=\"tensorboard\",\n",
        "        project_dir=os.path.join(config.output_dir, \"logs\"),\n",
        "    )\n",
        "    if accelerator.is_main_process:\n",
        "        if config.output_dir is not None:\n",
        "            os.makedirs(config.output_dir, exist_ok=True)\n",
        "        if config.push_to_hub:\n",
        "            repo_id = create_repo(\n",
        "                repo_id=config.hub_model_id or Path(config.output_dir).name, exist_ok=True\n",
        "            ).repo_id\n",
        "        accelerator.init_trackers(\"train_example\")\n",
        "\n",
        "    # Prepare everything\n",
        "    # There is no specific order to remember, you just need to unpack the\n",
        "    # objects in the same order you gave them to the prepare method.\n",
        "    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
        "        model, optimizer, train_dataloader, lr_scheduler\n",
        "    )\n",
        "\n",
        "    global_step = 0\n",
        "\n",
        "    # Now you train the model\n",
        "    for epoch in range(config.num_epochs):\n",
        "        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)\n",
        "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
        "\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            clean_images = batch[\"images\"]\n",
        "            # Sample noise to add to the images\n",
        "            noise = torch.randn(clean_images.shape, device=clean_images.device)\n",
        "            bs = clean_images.shape[0]\n",
        "\n",
        "            # Sample a random timestep for each image\n",
        "            timesteps = torch.randint(\n",
        "                0, noise_scheduler.config.num_train_timesteps, (bs,), device=clean_images.device,\n",
        "                dtype=torch.int64\n",
        "            )\n",
        "\n",
        "            # Add noise to the clean images according to the noise magnitude at each timestep\n",
        "            # (this is the forward diffusion process)\n",
        "            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
        "\n",
        "            with accelerator.accumulate(model):\n",
        "                # Predict the noise residual\n",
        "                noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n",
        "                loss = F.mse_loss(noise_pred, noise)\n",
        "                accelerator.backward(loss)\n",
        "\n",
        "                if accelerator.sync_gradients:\n",
        "                    accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "                lr_scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            progress_bar.update(1)\n",
        "            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
        "            progress_bar.set_postfix(**logs)\n",
        "            accelerator.log(logs, step=global_step)\n",
        "            global_step += 1\n",
        "\n",
        "        # After each epoch you optionally sample some demo images with evaluate() and save the model\n",
        "        if accelerator.is_main_process:\n",
        "            pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler)\n",
        "\n",
        "            if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:\n",
        "                evaluate(config, epoch, pipeline)\n",
        "\n",
        "            if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:\n",
        "                if config.push_to_hub:\n",
        "                    upload_folder(\n",
        "                        repo_id=repo_id,\n",
        "                        folder_path=config.output_dir,\n",
        "                        commit_message=f\"Epoch {epoch}\",\n",
        "                        ignore_patterns=[\"step_*\", \"epoch_*\"],\n",
        "                    )\n",
        "                else:\n",
        "                    pipeline.save_pretrained(config.output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "599Sxlak0M2c"
      },
      "source": [
        " Finally ready to launch the training with ðŸ¤— Accelerate's [notebook_launcher](https://huggingface.co/docs/accelerate/main/en/package_reference/launchers#accelerate.notebook_launcher) function. Pass the function the training loop, all the training arguments, and the number of processes (you can change this value to the number of GPUs available to you) to use for training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eb1ybqzZ0M2c"
      },
      "outputs": [],
      "source": [
        "from accelerate import notebook_launcher\n",
        "\n",
        "args = (config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler)\n",
        "\n",
        "notebook_launcher(train_loop, args, num_processes=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md7ovvOb0M2d"
      },
      "source": [
        "Once training is complete, take a look at the final ðŸ¦‹ images ðŸ¦‹ generated by your diffusion model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBYYnzqm0M2f"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "\n",
        "sample_images = sorted(glob.glob(f\"{config.output_dir}/samples/*.png\"))\n",
        "Image.open(sample_images[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Text to Image Generation"
      ],
      "metadata": {
        "id": "vBGsZUA31Tqt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yW14FA-tDQ5n"
      },
      "source": [
        "## What is Stable Diffusion?\n",
        "\n",
        "Stable Diffusion is based on a particular type of diffusion model called **Latent Diffusion**, proposed in [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHj_sllMaKTD"
      },
      "source": [
        "General diffusion models are machine learning systems that are trained to *denoise* random gaussian noise step by step, to get to a sample of interest, such as an *image*. For a more detailed overview of how they work, check [this colab](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb).\n",
        "\n",
        "Diffusion models have shown to achieve state-of-the-art results for generating image data. But one downside of diffusion models is that the reverse denoising process is slow. In addition, these models consume a lot of memory because they operate in pixel space, which becomes unreasonably expensive when generating high-resolution images. Therefore, it is challenging to train these models and also use them for inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBsdAj9pDPOv"
      },
      "source": [
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "Latent diffusion can reduce the memory and compute complexity by applying the diffusion process over a lower dimensional _latent_ space, instead of using the actual pixel space. This is the key difference between standard diffusion and latent diffusion models: **in latent diffusion the model is trained to generate latent (compressed) representations of the images.**\n",
        "\n",
        "There are three main components in latent diffusion.\n",
        "\n",
        "1. An autoencoder (VAE).\n",
        "2. A [U-Net](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb#scrollTo=wW8o1Wp0zRkq).\n",
        "3. A text-encoder, *e.g.* [CLIP's Text Encoder](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4leRMZzjTsA"
      },
      "source": [
        "**1. The autoencoder (VAE)**\n",
        "\n",
        "The VAE model has two parts, an encoder and a decoder. The encoder is used to convert the image into a low dimensional latent representation, which will serve as the input to the *U-Net* model.\n",
        "The decoder, conversely, transforms the latent representation back into an image.\n",
        "\n",
        " During latent diffusion _training_, the encoder is used to get the latent representations (_latents_) of the images for the forward diffusion process, which applies more and more noise at each step. During _inference_, the denoised latents generated by the reverse diffusion process are converted back into images using the VAE decoder. As we will see during inference we **only need the VAE decoder**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jr5ZCb66kmyE"
      },
      "source": [
        "**2. The U-Net**\n",
        "\n",
        "The U-Net has an encoder part and a decoder part both comprised of ResNet blocks.\n",
        "The encoder compresses an image representation into a lower resolution image representation and the decoder decodes the lower resolution image representation back to the original higher resolution image representation that is supposedly less noisy.\n",
        "More specifically, the U-Net output predicts the noise residual which can be used to compute the predicted denoised image representation.\n",
        "\n",
        "To prevent the U-Net from losing important information while downsampling, short-cut connections are usually added between the downsampling ResNets of the encoder to the upsampling ResNets of the decoder.\n",
        "Additionally, the stable diffusion U-Net is able to condition its output on text-embeddings via cross-attention layers. The cross-attention layers are added to both the encoder and decoder part of the U-Net usually between ResNet blocks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YE7hhg5ArUu4"
      },
      "source": [
        "**3. The Text-encoder**\n",
        "\n",
        "The text-encoder is responsible for transforming the input prompt, *e.g.* \"An astronout riding a horse\" into an embedding space that can be understood by the U-Net. It is usually a simple *transformer-based* encoder that maps a sequence of input tokens to a sequence of latent text-embeddings.\n",
        "\n",
        "Inspired by [Imagen](https://imagen.research.google/), Stable Diffusion does **not** train the text-encoder during training and simply uses an CLIP's already trained text encoder, [CLIPTextModel](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-XnKTVfj2Jm"
      },
      "source": [
        "**Why is latent diffusion fast and efficient?**\n",
        "\n",
        "Since the U-Net of latent diffusion models operates on a low dimensional space, it greatly reduces the memory and compute requirements compared to pixel-space diffusion models. For example, the autoencoder used in Stable Diffusion has a reduction factor of 8. This means that an image of shape `(3, 512, 512)` becomes `(3, 64, 64)` in latent space, which requires `8 Ã— 8 = 64` times less memory.\n",
        "\n",
        "This is why it's possible to generate `512 Ã— 512` images so quickly, even on 16GB Colab GPUs!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zz5Ge_47jUaA"
      },
      "source": [
        "**Stable Diffusion during inference**\n",
        "\n",
        "Putting it all together, let's now take a closer look at how the model works in inference by illustrating the logical flow.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUBqX1sMsDR6"
      },
      "source": [
        "<p align=\"left\">\n",
        "<img src=\"https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/stable_diffusion.png\" alt=\"sd-pipeline\" width=\"500\"/>\n",
        "</p>\n",
        "\n",
        "The stable diffusion model takes both a latent seed and a text prompt as an input. The latent seed is then used to generate random latent image representations of size $64 \\times 64$ where as the text prompt is transformed to text embeddings of size $77 \\times 768$ via CLIP's text encoder.\n",
        "\n",
        "Next the U-Net iteratively *denoises* the random latent image representations while being conditioned on the text embeddings. The output of the U-Net, being the noise residual, is used to compute a denoised latent image representation via a scheduler algorithm. Many different scheduler algorithms can be used for this computation, each having its pros and cons. For Stable Diffusion, we recommend using one of:\n",
        "\n",
        "- [PNDM scheduler](https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_pndm.py) (used by default).\n",
        "- [K-LMS scheduler](https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_lms_discrete.py).\n",
        "- [Heun Discrete scheduler](https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_heun_discrete.py).\n",
        "- [DPM Solver Multistep scheduler](https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_dpmsolver_multistep.py). This scheduler is able to achieve great quality in less steps. You can try with 25 instead of the default 50!\n",
        "\n",
        "Theory on how the scheduler algorithm function is out of scope for this notebook, but in short one should remember that they compute the predicted denoised image representation from the previous noise representation and the predicted noise residual.\n",
        "For more information, we recommend looking into [Elucidating the Design Space of Diffusion-Based Generative Models](https://arxiv.org/abs/2206.00364)\n",
        "\n",
        "The *denoising* process is repeated *ca.* 50 times to step-by-step retrieve better latent image representations.\n",
        "Once complete, the latent image representation is decoded by the decoder part of the variational auto encoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rR2Udg5IYjn"
      },
      "source": [
        "\n",
        "\n",
        "After this brief introduction to Latent and Stable Diffusion, let's see how to make advanced use of ðŸ¤— Hugging Face Diffusers!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZp-ynZLrS-S"
      },
      "source": [
        "## How to write your own inference pipeline with `diffusers`\n",
        "\n",
        "Finally, we show how you can create custom diffusion pipelines with `diffusers`.\n",
        "This is often very useful to dig a bit deeper into certain functionalities of the system and to potentially switch out certain components.\n",
        "\n",
        "In this section, we will demonstrate how to use Stable Diffusion with a different scheduler, namely [Katherine Crowson's](https://github.com/crowsonkb) K-LMS scheduler that was added in [this PR](https://github.com/huggingface/diffusers/pull/185#pullrequestreview-1074247365)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3Xw7qSEdTpt"
      },
      "source": [
        "The [pre-trained model](https://huggingface.co/CompVis/stable-diffusion-v1-3-diffusers/tree/main) includes all the components required to setup a complete diffusion pipeline. They are stored in the following folders:\n",
        "- `text_encoder`: Stable Diffusion uses CLIP, but other diffusion models may use other encoders such as `BERT`.\n",
        "- `tokenizer`. It must match the one used by the `text_encoder` model.\n",
        "- `scheduler`: The scheduling algorithm used to progressively add noise to the image during training.\n",
        "- `unet`: The model used to generate the latent representation of the input.\n",
        "- `vae`: Autoencoder module that we'll use to decode latent representations into real images.\n",
        "\n",
        "We can load the components by referring to the folder they were saved, using the `subfolder` argument to `from_pretrained`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlsKwQijWMpL"
      },
      "outputs": [],
      "source": [
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler\n",
        "\n",
        "# 1. Load the autoencoder model which will be used to decode the latents into image space.\n",
        "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\")\n",
        "\n",
        "# 2. Load the tokenizer and text encoder to tokenize and encode the text.\n",
        "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "\n",
        "# 3. The UNet model for generating the latents.\n",
        "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-0W8UG6VXpD"
      },
      "outputs": [],
      "source": [
        "from diffusers import LMSDiscreteScheduler\n",
        "\n",
        "scheduler = LMSDiscreteScheduler.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device=torch.device(\"cuda\")"
      ],
      "metadata": {
        "id": "pcnkpOmxdKRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3lJEXz7YgnC"
      },
      "outputs": [],
      "source": [
        "vae = vae.to(device)\n",
        "text_encoder = text_encoder.to(device)\n",
        "unet = unet.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ot8RDV-2Y6uE"
      },
      "outputs": [],
      "source": [
        "prompt = [\"a photograph of an astronaut riding a horse\"]\n",
        "\n",
        "height = 512                        # default height of Stable Diffusion\n",
        "width = 512                         # default width of Stable Diffusion\n",
        "\n",
        "num_inference_steps = 100            # Number of denoising steps\n",
        "\n",
        "guidance_scale = 7.5                # Scale for classifier-free guidance\n",
        "\n",
        "generator = torch.manual_seed(32)   # Seed generator to create the inital latent noise\n",
        "\n",
        "batch_size = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZpvyVT1Y6wq"
      },
      "outputs": [],
      "source": [
        "text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "  text_embeddings = text_encoder(text_input.input_ids.to(device))[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkuwhbFrY6zo"
      },
      "outputs": [],
      "source": [
        "max_length = text_input.input_ids.shape[-1]\n",
        "uncond_input = tokenizer(\n",
        "    [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
        ")\n",
        "with torch.no_grad():\n",
        "  uncond_embeddings = text_encoder(uncond_input.input_ids.to(device))[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwnB7CIeY619"
      },
      "outputs": [],
      "source": [
        "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NsfjxA-chAL"
      },
      "outputs": [],
      "source": [
        "latents = torch.randn(\n",
        "  (batch_size, unet.in_channels, height // 8, width // 8),\n",
        "  generator=generator,\n",
        ")\n",
        "latents = latents.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nC401krZfXOr"
      },
      "outputs": [],
      "source": [
        "latents.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6UDqCyKwBpx"
      },
      "outputs": [],
      "source": [
        "scheduler.set_timesteps(num_inference_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTwTq9d-W_NP"
      },
      "outputs": [],
      "source": [
        "latents = latents * scheduler.init_noise_sigma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ylc3AIdZkFhl"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "from torch import autocast\n",
        "\n",
        "for t in tqdm(scheduler.timesteps):\n",
        "  # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
        "  latent_model_input = torch.cat([latents] * 2)\n",
        "\n",
        "  latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
        "\n",
        "  # predict the noise residual\n",
        "  with torch.no_grad():\n",
        "    noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
        "\n",
        "  # perform guidance\n",
        "  noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "  noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "  # compute the previous noisy sample x_t -> x_t-1\n",
        "  latents = scheduler.step(noise_pred, t, latents).prev_sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YRzuJP7kMZo"
      },
      "outputs": [],
      "source": [
        "# scale and decode the image latents with vae\n",
        "latents = 1 / 0.18215 * latents\n",
        "\n",
        "with torch.no_grad():\n",
        "  image = vae.decode(latents).sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAVZStIokTVv"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "image = (image / 2 + 0.5).clamp(0, 1)\n",
        "image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
        "images = (image * 255).round().astype(\"uint8\")\n",
        "pil_images = [Image.fromarray(image) for image in images]\n",
        "pil_images[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Image to Image Generation\n",
        "\n",
        "**Overview**\n",
        "\n",
        "Image-to-Image generation modifies an input image based on a text prompt. It is useful for transforming or stylizing images while preserving their general structure.\n",
        "\n",
        "**Key Components**\n",
        "\n",
        "- StableDiffusionImg2ImgPipeline: Core pipeline for image-to-image generation.\n",
        "\n",
        "- strength: Controls how much of the original image is preserved (0.0 = original image, 1.0 = full random noise)."
      ],
      "metadata": {
        "id": "4l71F2su7NJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import inspect\n",
        "import warnings\n",
        "from typing import List, Optional, Union\n",
        "\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from diffusers import StableDiffusionImg2ImgPipeline"
      ],
      "metadata": {
        "id": "2YJrHBHlB54B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\"\n",
        "model_path = \"CompVis/stable-diffusion-v1-4\"\n",
        "\n",
        "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
        "    model_path,\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "pipe = pipe.to(device)"
      ],
      "metadata": {
        "id": "Fr2QIEzvCFH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\n",
        "\n",
        "response = requests.get(url)\n",
        "init_img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "init_img = init_img.resize((768, 512))\n",
        "init_img"
      ],
      "metadata": {
        "id": "quipipaCC1AP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"A fantasy landscape, trending on artstation\""
      ],
      "metadata": {
        "id": "4V11Z0l-ZUWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, `strength` is a value between 0.0 and 1.0, that controls the amount of noise that is added to the input image. Values that approach 1.0 allow for lots of variations but will also produce images that are not semantically consistent with the input."
      ],
      "metadata": {
        "id": "8wOmwKnkDfpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator = torch.Generator(device=device).manual_seed(1024)\n",
        "image = pipe(prompt=prompt, image=init_img, strength=0.75, guidance_scale=7.5, generator=generator).images[0]"
      ],
      "metadata": {
        "id": "V24njWQBC8eC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image"
      ],
      "metadata": {
        "id": "uMfaTT24DWk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = pipe(prompt=prompt, image=init_img, strength=0.5, guidance_scale=7.5, generator=generator).images[0]"
      ],
      "metadata": {
        "id": "Rfn_M2f9Dsl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image"
      ],
      "metadata": {
        "id": "JlUXXh7MEFPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, when using a lower value for `strength`, the generated image is more closer to the original `image`"
      ],
      "metadata": {
        "id": "NlQlTZDIEQKi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Now using [LMSDiscreteScheduler](https://huggingface.co/docs/diffusers/api/schedulers#diffusers.LMSDiscreteScheduler)"
      ],
      "metadata": {
        "id": "kpBvfS4oVv9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import LMSDiscreteScheduler\n",
        "\n",
        "lms = LMSDiscreteScheduler.from_config(pipe.scheduler.config)\n",
        "pipe.scheduler = lms"
      ],
      "metadata": {
        "id": "0O0Iy1zK0q-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = torch.Generator(device=device).manual_seed(1024)\n",
        "image = pipe(prompt=prompt, image=init_img, strength=0.75, guidance_scale=7.5, generator=generator).images[0]"
      ],
      "metadata": {
        "id": "q7ixTMhfD6Ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image"
      ],
      "metadata": {
        "id": "Tm_SR0gT0s2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. In-Painting Generation\n",
        "**Overview**\n",
        "\n",
        "Inpainting lets you edit specific regions of an image by providing a mask and a prompt. It's useful for repairing or modifying localized parts of an image.\n",
        "\n",
        "**Key Components**\n",
        "\n",
        "- StableDiffusionInpaintPipeline: Takes both an image and a binary mask indicating editable regions.\n",
        "\n",
        "- mask_image: Specifies the area to be regenerated."
      ],
      "metadata": {
        "id": "2tLkpDSsFbbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import inspect\n",
        "from typing import List, Optional, Union\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import PIL\n",
        "import gradio as gr\n",
        "from diffusers import StableDiffusionInpaintPipeline"
      ],
      "metadata": {
        "id": "dxKnZonKuYgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\"\n",
        "model_path = \"runwayml/stable-diffusion-inpainting\"\n",
        "\n",
        "pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
        "    model_path,\n",
        "    torch_dtype=torch.float16,\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "MfUEmooNuyKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "def image_grid(imgs, rows, cols):\n",
        "    assert len(imgs) == rows*cols\n",
        "\n",
        "    w, h = imgs[0].size\n",
        "    grid = PIL.Image.new('RGB', size=(cols*w, rows*h))\n",
        "    grid_w, grid_h = grid.size\n",
        "\n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
        "    return grid\n",
        "\n",
        "\n",
        "def download_image(url):\n",
        "    response = requests.get(url)\n",
        "    return PIL.Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "\n",
        "img_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"\n",
        "mask_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\""
      ],
      "metadata": {
        "id": "hvdHYdtTu6KA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = download_image(img_url).resize((512, 512))\n",
        "image"
      ],
      "metadata": {
        "id": "y4OVr7pPvMWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask_image = download_image(mask_url).resize((512, 512))\n",
        "mask_image"
      ],
      "metadata": {
        "id": "GSdamba-vo0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"a mecha robot sitting on a bench\"\n",
        "\n",
        "guidance_scale=7.5\n",
        "num_samples = 3\n",
        "generator = torch.Generator(device=\"cuda\").manual_seed(0) # change the seed to get different results\n",
        "\n",
        "images = pipe(\n",
        "    prompt=prompt,\n",
        "    image=image,\n",
        "    mask_image=mask_image,\n",
        "    guidance_scale=guidance_scale,\n",
        "    generator=generator,\n",
        "    num_images_per_prompt=num_samples,\n",
        ").images"
      ],
      "metadata": {
        "id": "OkePC-DGvyz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# insert initial image in the list so we can compare side by side\n",
        "images.insert(0, image)"
      ],
      "metadata": {
        "id": "WtfJyo1XhBP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_grid(images, 1, num_samples + 1)"
      ],
      "metadata": {
        "id": "loXzMOIxv9OZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradio Demo"
      ],
      "metadata": {
        "id": "ZupJ7HzIzRQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(dict, prompt):\n",
        "  image =  dict['image'].convert(\"RGB\").resize((512, 512))\n",
        "  mask_image = dict['mask'].convert(\"RGB\").resize((512, 512))\n",
        "  images = pipe(prompt=prompt, image=image, mask_image=mask_image).images\n",
        "  return(images[0])"
      ],
      "metadata": {
        "id": "byoa1q2zyd6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gr.Interface(\n",
        "    predict,\n",
        "    title = 'Stable Diffusion In-Painting',\n",
        "    inputs=[\n",
        "        gr.Image(sources = 'upload', interactive=True, type = 'pil'),\n",
        "        gr.Textbox(label = 'prompt')\n",
        "    ],\n",
        "    outputs = [\n",
        "        gr.Image()\n",
        "        ]\n",
        ").launch(debug=True)"
      ],
      "metadata": {
        "id": "R596bpT2ynqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Using Mega Pipeline\n",
        "**Overview**\n",
        "\n",
        "The Mega Pipeline unifies Text-to-Image, Image-to-Image, and Inpainting under one interface. It provides a flexible and modular interface for switching between generation tasks.\n",
        "\n",
        "**Key Components**\n",
        "\n",
        "- DiffusionPipeline with custom_pipeline=\"stable_diffusion_mega\": Supports multiple generation modes in a single object."
      ],
      "metadata": {
        "id": "sfzYkQlULD5a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "3jjMjR-2Xmvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import DiffusionPipeline\n",
        "import PIL\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import torch\n",
        "\n",
        "\n",
        "def download_image(url):\n",
        "    response = requests.get(url)\n",
        "    return PIL.Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "\n",
        "pipe = DiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", custom_pipeline=\"stable_diffusion_mega\", torch_dtype=torch.float16, variant=\"fp16\")\n",
        "pipe.to(\"cuda\")\n",
        "pipe.enable_attention_slicing()"
      ],
      "metadata": {
        "id": "BsNjbGqCgBWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text-to-Image"
      ],
      "metadata": {
        "id": "fwz53WStX4j6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images = pipe.text2img(\"An astronaut riding a horse\").images\n",
        "images[0].show()"
      ],
      "metadata": {
        "id": "afgfwx2XkF_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image-to-Image"
      ],
      "metadata": {
        "id": "MYac2RyMX-dE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "init_img = download_image(\"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\")\n",
        "prompt = \"A fantasy landscape, trending on artstation\"\n",
        "\n",
        "images = pipe.img2img(prompt=prompt, image=init_img, strength=0.75, guidance_scale=7.5).images\n",
        "images[0].show()"
      ],
      "metadata": {
        "id": "3UZyERI4XxoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In-painting"
      ],
      "metadata": {
        "id": "NoxE1li7YBaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"\n",
        "mask_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"\n",
        "\n",
        "image = download_image(img_url).resize((512, 512))\n",
        "mask = download_image(mask_url).resize((512, 512))\n",
        "prompt = \"a cat sitting on a bench\"\n",
        "\n",
        "images = pipe.inpaint(prompt=prompt, image=image, mask_image=mask, strength=0.75).images\n",
        "images[0].show()"
      ],
      "metadata": {
        "id": "Kec9FuLDXxwG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}