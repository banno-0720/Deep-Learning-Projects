{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMUb0ZMo0ghphkx3FxIj7yL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/banno-0720/Deep-Learning-Projects/blob/main/Agentic_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agentic RAG"
      ],
      "metadata": {
        "id": "B_NrAvrSRRxR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Install Required Dependencies"
      ],
      "metadata": {
        "id": "Unnq4FUZRVXk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbG12J0EtLZi"
      },
      "outputs": [],
      "source": [
        "!pip install smolagents pandas langchain langchain-community sentence-transformers datasets python-dotenv rank_bm25 --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Prepare the Knowledge Base"
      ],
      "metadata": {
        "id": "UIxv64kKRkND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "\n",
        "# Load the Hugging Face documentation dataset\n",
        "knowledge_base = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")\n",
        "\n",
        "# Filter to include only Transformers documentation\n",
        "knowledge_base = knowledge_base.filter(lambda row: row[\"source\"].startswith(\"huggingface/transformers\"))\n",
        "\n",
        "# Convert dataset entries to Document objects with metadata\n",
        "source_docs = [\n",
        "    Document(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"].split(\"/\")[1]})\n",
        "    for doc in knowledge_base\n",
        "]\n",
        "\n",
        "# Split documents into smaller chunks for better retrieval\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,  # Characters per chunk\n",
        "    chunk_overlap=50,  # Overlap between chunks to maintain context\n",
        "    add_start_index=True,\n",
        "    strip_whitespace=True,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],  # Priority order for splitting\n",
        ")\n",
        "docs_processed = text_splitter.split_documents(source_docs)\n",
        "\n",
        "print(f\"Knowledge base prepared with {len(docs_processed)} document chunks\")"
      ],
      "metadata": {
        "id": "S1VlO5zHt0ae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Create a Retriever Tool"
      ],
      "metadata": {
        "id": "wBVGnK52RpF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from smolagents import Tool\n",
        "\n",
        "class RetrieverTool(Tool):\n",
        "    name = \"retriever\"\n",
        "    description = \"Uses semantic search to retrieve the parts of transformers documentation that could be most relevant to answer your query.\"\n",
        "    inputs = {\n",
        "        \"query\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"The query to perform. This should be semantically close to your target documents. Use the affirmative form rather than a question.\",\n",
        "        }\n",
        "    }\n",
        "    output_type = \"string\"\n",
        "\n",
        "    def __init__(self, docs, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        # Initialize the retriever with our processed documents\n",
        "        self.retriever = BM25Retriever.from_documents(\n",
        "            docs, k=10  # Return top 10 most relevant documents\n",
        "        )\n",
        "\n",
        "    def forward(self, query: str) -> str:\n",
        "        \"\"\"Execute the retrieval based on the provided query.\"\"\"\n",
        "        assert isinstance(query, str), \"Your search query must be a string\"\n",
        "\n",
        "        # Retrieve relevant documents\n",
        "        docs = self.retriever.invoke(query)\n",
        "\n",
        "        # Format the retrieved documents for readability\n",
        "        return \"\\nRetrieved documents:\\n\" + \"\".join(\n",
        "            [\n",
        "                f\"\\n\\n===== Document {str(i)} =====\\n\" + doc.page_content\n",
        "                for i, doc in enumerate(docs)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "# Initialize our retriever tool with the processed documents\n",
        "retriever_tool = RetrieverTool(docs_processed)"
      ],
      "metadata": {
        "id": "66Ce566pt1-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Create an Advanced Retrieval Agent"
      ],
      "metadata": {
        "id": "VkNvZA7iR3gl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from smolagents import InferenceClientModel, CodeAgent\n",
        "\n",
        "# Initialize the agent with our retriever tool\n",
        "agent = CodeAgent(\n",
        "    tools=[retriever_tool],  # List of tools available to the agent\n",
        "    model=InferenceClientModel(),  # Default model \"Qwen/Qwen2.5-Coder-32B-Instruct\"\n",
        "    max_steps=4,  # Limit the number of reasoning steps\n",
        "    verbosity_level=2,  # Show detailed agent reasoning\n",
        ")\n",
        "\n",
        "# To use a specific model, you can specify it like this:\n",
        "# model=InferenceClientModel(model_id=\"meta-llama/Llama-3.3-70B-Instruct\")"
      ],
      "metadata": {
        "id": "y9jSzQ0bt5oA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Run the Agent to Answer Questions"
      ],
      "metadata": {
        "id": "cJ_Fry9-R7pi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask a question that requires retrieving information\n",
        "question = \"For a transformers model training, which is slower, the forward or the backward pass?\"\n",
        "\n",
        "# Run the agent to get an answer\n",
        "agent_output = agent.run(question)\n",
        "\n",
        "# Display the final answer\n",
        "print(\"\\nFinal answer:\")\n",
        "print(agent_output)"
      ],
      "metadata": {
        "id": "jbO_naI3t8nB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Practical Applications of Agentic RAG\n",
        "\n",
        "Agentic RAG systems can be applied to various use cases:\n",
        "\n",
        "1. **Technical Documentation Assistance:** Help users navigate complex technical documentation\n",
        "2. **Research Paper Analysis:** Extract and synthesize information from scientific papers\n",
        "3. **Legal Document Review:** Find relevant precedents and clauses in legal documents\n",
        "4. **Customer Support:** Answer questions based on product documentation and knowledge bases\n",
        "5. **Educational Tutoring:** Provide explanations based on textbooks and learning materials"
      ],
      "metadata": {
        "id": "YHclJCz6SAaM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Creating Interactive demo for our Agentic RAG"
      ],
      "metadata": {
        "id": "T6BLnI_0uGZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Setup path to trashify demo folder (we'll store all of our demo requirements in here)\n",
        "demo_path = Path(\"../demos/agentic_rag\")\n",
        "\n",
        "# Create the directory\n",
        "demo_path.mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "XXFm3h9VyxIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ../demos/agentic_rag/app.py\n",
        "\n",
        "\"\"\"\n",
        "Gradio demo that exposes your agentic QA pipeline (uses smolagents CodeAgent + a BM25 retriever).\n",
        "Intended for deployment to Hugging Face Spaces. Set HF_TOKEN in Space secrets or environment.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import traceback\n",
        "import gradio as gr\n",
        "\n",
        "# Basic ML / NLP libs used by your pipeline\n",
        "import datasets\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "\n",
        "# smolagents agent pieces\n",
        "from smolagents import Tool, InferenceClientModel, CodeAgent\n",
        "\n",
        "# -------------------------\n",
        "# Document preparation\n",
        "# -------------------------\n",
        "def prepare_knowledge_base(cache_dir=\"/tmp/hf_kb_cache\"):\n",
        "    \"\"\"\n",
        "    Download and prepare the HF docs dataset, filter to transformers docs,\n",
        "    chunk into smaller documents and return the processed doc list.\n",
        "    This function caches results across runs (simple file-check).\n",
        "    \"\"\"\n",
        "    import os\n",
        "    import pickle\n",
        "\n",
        "    cache_path = os.path.join(cache_dir, \"docs_processed.pkl\")\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "\n",
        "    # If cached, load and return\n",
        "    if os.path.exists(cache_path):\n",
        "        try:\n",
        "            with open(cache_path, \"rb\") as f:\n",
        "                docs_processed = pickle.load(f)\n",
        "            return docs_processed\n",
        "        except Exception:\n",
        "            # fall through to re-create cache\n",
        "            pass\n",
        "\n",
        "    knowledge_base = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")\n",
        "    # Keep only transformers docs (same filter as your original snippet)\n",
        "    knowledge_base = knowledge_base.filter(lambda row: row[\"source\"].startswith(\"huggingface/transformers\"))\n",
        "    source_docs = [\n",
        "        Document(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"].split(\"/\")[1]})\n",
        "        for doc in knowledge_base\n",
        "    ]\n",
        "\n",
        "    # Split into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=500,\n",
        "        chunk_overlap=50,\n",
        "        add_start_index=True,\n",
        "        strip_whitespace=True,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
        "    )\n",
        "    docs_processed = text_splitter.split_documents(source_docs)\n",
        "\n",
        "    import pickle\n",
        "    with open(cache_path, \"wb\") as f:\n",
        "        pickle.dump(docs_processed, f)\n",
        "\n",
        "    return docs_processed\n",
        "\n",
        "# -------------------------\n",
        "# Retriever tool for agent\n",
        "# -------------------------\n",
        "class RetrieverTool(Tool):\n",
        "    name = \"retriever\"\n",
        "    description = \"Uses BM25 retrieval over transformers docs to fetch context relevant to a question.\"\n",
        "    inputs = {\n",
        "        \"query\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"A short query describing the information to retrieve (affirmative form).\",\n",
        "        }\n",
        "    }\n",
        "    output_type = \"string\"\n",
        "\n",
        "    def __init__(self, docs, k=8, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        # Build a BM25 retriever from the processed docs\n",
        "        self.retriever = BM25Retriever.from_documents(docs, k=k)\n",
        "\n",
        "    def forward(self, query: str) -> str:\n",
        "        assert isinstance(query, str), \"query must be a string\"\n",
        "        docs = self.retriever.invoke(query)\n",
        "        formatted = \"\\nRetrieved documents:\\n\" + \"\".join(\n",
        "            [\n",
        "                f\"\\n\\n===== Document {i} =====\\n{doc.page_content}\"\n",
        "                for i, doc in enumerate(docs)\n",
        "            ]\n",
        "        )\n",
        "        return formatted\n",
        "\n",
        "# -------------------------\n",
        "# Agent initialization\n",
        "# -------------------------\n",
        "# Prepare docs\n",
        "DOCS = prepare_knowledge_base()\n",
        "\n",
        "# Initialize tool instance\n",
        "retriever_tool = RetrieverTool(DOCS)\n",
        "\n",
        "# NOTE: On HF Spaces you can set environment variable HF_TOKEN in the UI (Settings -> Secrets).\n",
        "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
        "if HF_TOKEN is None:\n",
        "    print(\"Warning: HF_TOKEN not set. If your chosen model requires authentication, set HF_TOKEN in environment/secrets.\")\n",
        "\n",
        "\n",
        "model = InferenceClientModel()  # default model; you can set model_id arg if needed\n",
        "agent = CodeAgent(\n",
        "    tools=[retriever_tool],\n",
        "    model=model,\n",
        "    max_steps=4,\n",
        "    verbosity_level=1,\n",
        ")\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Gradio interface\n",
        "# -------------------------\n",
        "def run_agent(question: str):\n",
        "    \"\"\"Run the agent and return the final answer (or a helpful error).\"\"\"\n",
        "    if not question or question.strip() == \"\":\n",
        "        return \"Please enter a question.\"\n",
        "\n",
        "    # If agent couldn't be created, return fallback info\n",
        "    if agent is None:\n",
        "        return \"Agent not initialized in this environment. Check logs in the Space and ensure `smolagents` is installed and HF_TOKEN is configured.\"\n",
        "\n",
        "    result = agent.run(question)\n",
        "    return result\n",
        "\n",
        "\n",
        "with gr.Blocks(title=\"Agentic RAG Demo\") as demo:\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        # Transformers docs QA (Agent demo)\n",
        "        Ask the agent a question about Hugging Face Transformers docs.\n",
        "        Example: *For a transformers model training, which is slower, the forward or the backward pass?*\n",
        "        \"\"\"\n",
        "    )\n",
        "    with gr.Row():\n",
        "        inp = gr.Textbox(placeholder=\"Write your question here...\", label=\"Question\", lines=2)\n",
        "        out = gr.Textbox(label=\"Agent answer\", lines=10)\n",
        "    with gr.Row():\n",
        "        run_btn = gr.Button(\"Ask\")\n",
        "        clear_btn = gr.Button(\"Clear\")\n",
        "    run_btn.click(fn=run_agent, inputs=inp, outputs=out)\n",
        "    clear_btn.click(lambda: \"\", None, inp)\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "Lb8W6QFEyMmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ../demos/agentic_rag/requirements.txt\n",
        "\n",
        "# Core\n",
        "smolagents\n",
        "gradio\n",
        "datasets\n",
        "langchain\n",
        "langchain-community\n",
        "sentence-transformers\n",
        "rank_bm25\n",
        "python-dotenv\n",
        "\n",
        "# Extras\n",
        "transformers\n",
        "huggingface-hub\n",
        "accelerate\n",
        "torch\n",
        "pandas"
      ],
      "metadata": {
        "id": "DFPZuXtsycxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ../demos/agentic_rag/README.md\n",
        "---\n",
        "title: Agentic RAG Demo\n",
        "emoji: ðŸ¤–\n",
        "app_file: app.py\n",
        "sdk: gradio\n",
        "sdk_version: 5.34.0\n",
        "license: apache-2.0\n",
        "colorFrom: indigo\n",
        "colorTo: purple\n",
        "colorMode: light\n",
        "---\n",
        "\n",
        "# ðŸ¤– Agentic RAG Demo\n",
        "\n",
        "A small Gradio Space demonstrating an **agentic RAG (Retrieval-Augmented Generation)** pipeline:\n",
        "- a BM25 retriever over a Transformers documentation knowledge base,\n",
        "- a `smolagents` `CodeAgent` (agentic reasoning + tools),\n",
        "- a Gradio UI for asking natural-language questions and receiving agent answers.\n",
        "\n",
        "---\n",
        "\n",
        "## What this Space contains\n",
        "- `app.py` â€” main Gradio app. (The `app_file` above points to this.)\n",
        "- `requirements.txt` â€” Python dependencies used by the Space.\n",
        "- `README.md` â€” this file (with YAML front matter used by Spaces).\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "20g9IsTtzqnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Import the required methods for uploading to the Hugging Face Hub\n",
        "from huggingface_hub import (\n",
        "    create_repo,\n",
        "    get_full_repo_name,\n",
        "    upload_file, # for uploading a single file (if necessary)\n",
        "    upload_folder # for uploading multiple files (in a folder)\n",
        ")\n",
        "\n",
        "# 2. Define the parameters we'd like to use for the upload\n",
        "LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD = \"../demos/agentic_rag\"\n",
        "HF_TARGET_SPACE_NAME = \"agentic_rag_demo\"\n",
        "HF_REPO_TYPE = \"space\" # we're creating a Hugging Face Space\n",
        "HF_SPACE_SDK = \"gradio\"\n",
        "HF_TOKEN = \"\" # optional: set to your Hugging Face token (but I'd advise storing this as an environment variable as previously discussed)\n",
        "\n",
        "# 3. Create a Space repository on Hugging Face Hub\n",
        "print(f\"[INFO] Creating repo on Hugging Face Hub with name: {HF_TARGET_SPACE_NAME}\")\n",
        "create_repo(\n",
        "    repo_id=HF_TARGET_SPACE_NAME,\n",
        "    # token=HF_TOKEN, # optional: set token manually (though it will be automatically recognized if it's available as an environment variable)\n",
        "    repo_type=HF_REPO_TYPE,\n",
        "    private=False, # set to True if you don't want your Space to be accessible to others\n",
        "    space_sdk=HF_SPACE_SDK,\n",
        "    exist_ok=True, # set to False if you want an error to raise if the repo_id already exists\n",
        ")\n",
        "\n",
        "# 4. Get the full repository name (e.g. {username}/{model_id} or {username}/{space_name})\n",
        "full_hf_repo_name = get_full_repo_name(model_id=HF_TARGET_SPACE_NAME)\n",
        "print(f\"[INFO] Full Hugging Face Hub repo name: {full_hf_repo_name}\")\n",
        "\n",
        "# 5. Upload our demo folder\n",
        "print(f\"[INFO] Uploading {LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD} to repo: {full_hf_repo_name}\")\n",
        "folder_upload_url = upload_folder(\n",
        "    repo_id=full_hf_repo_name,\n",
        "    folder_path=LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD,\n",
        "    path_in_repo=\".\", # upload our folder to the root directory (\".\" means \"base\" or \"root\", this is the default)\n",
        "    # token=HF_TOKEN, # optional: set token manually\n",
        "    repo_type=HF_REPO_TYPE,\n",
        "    commit_message=\"Upload Agentic RAG demo (app, requirements, README)\"\n",
        ")\n",
        "print(f\"[INFO] Demo folder successfully uploaded with commit URL: {folder_upload_url}\")"
      ],
      "metadata": {
        "id": "_P_9iZTC1d4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IZm5e-6PTzS4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}