{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/banno-0720/Deep-Learning-Projects/blob/main/Belief_State_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Belief State Transformer Paper Replication\n",
        "\n",
        "Based on Research paper \"THE BELIEF STATE TRANSFORMER\n",
        "\" which was published by Microsoft on February 20, 2025.\n",
        "\n",
        "[Click on this](https://arxiv.org/pdf/2410.23506) for original research paper\n",
        "\n",
        "And [Click on this](https://youtu.be/aqhbRtB2Fyg?si=ABz33R6ZfdWue-mi) for the video on the topic, for better understanding of Belief State Transformers"
      ],
      "metadata": {
        "id": "BeHF1jei0XMS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why replicate a machine learning research paper?\n",
        "\n",
        "A machine learning research paper is often a presentation of months of work and experiments done by some of the best machine learning teams in the world condensed into a few pages of text.\n",
        "\n",
        "And if these experiments lead to better results in an area related to the problem you're working on, it'd be nice to check them out.\n",
        "\n",
        "Also, replicating the work of others is a fantastic way to practice your skills.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-george-hotz-quote.png\" width=600 alt=\"george hotz quote saying to get better at being a machine learning engineer, download a paper, implement it and keep going until you have skills\"/>\n",
        "\n",
        "*George Hotz is founder of [comma.ai](https://comma.ai/), a self-driving car company and livestreams machine learning coding on [Twitch](https://www.twitch.tv/georgehotz) and those videos get posted in full to [YouTube](https://www.youtube.com/c/georgehotzarchive). I pulled this quote from one of his livestreams. The \"٭\" is to note that machine learning engineering often involves the extra step(s) of preprocessing data and making your models available for others to use (deployment).*"
      ],
      "metadata": {
        "id": "IAuGDo2H3btS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## What is a Belief State?\n",
        "\n",
        "For any probability distribution over a set of sequences $P(x_{1:T})$, and for any partial sequence $s = x_{1:t}$, we define a vector $v_s$ to be a **belief state** for $s$ if there exists a randomized function $g$ such that\n",
        "\n",
        "$$\n",
        "g(v_s) \\sim P(x_{t+1:T} \\mid x_{1:t}).\n",
        "$$\n",
        "\n",
        "In other words, sampling $g(v_s)$ yields a sample from the conditional distribution $P(x_{t+1:T} \\mid x_{1:t})$.\n",
        "\n",
        "\n",
        "By definition, a belief state captures all available information relevant for predicting the future tokens.\n",
        "Once the belief state is learned, there is no additional useful information to be gained—everything\n",
        "necessary for future predictions is already encoded within it.\n",
        "\n"
      ],
      "metadata": {
        "id": "WLIFLoKs3k68"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why use Belief State Transformer?\n",
        "\n",
        "In the research paper, we see that they check its performance with\n",
        "1. Star graph problem, in which it comes out to be the clear winner and with much higher accuracy than next-token, data-augmentation, FIM and teacherless models\n",
        "2. TinyStories Dataset, it divides the text into three parts: prefix, suffix, and missing middle part and make the model predict the middle missing part. It gains very high accuracy\n",
        "\n",
        "Due to its self evaluation feature, it does need high computational power but  it has almost perfect accuracy."
      ],
      "metadata": {
        "id": "qvLy6O4c_v4n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Getting Setup"
      ],
      "metadata": {
        "id": "qEpUb_Cb3oE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "RiudUPsNCBXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up device agnostic code\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "id": "o78OyHDDEu4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d230c2b-4cbc-45da-a860-65a8b1be2f05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Get Data"
      ],
      "metadata": {
        "id": "tK1EqpGPEwWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "NESO_B1pdc-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "385c0320-2587-49a4-d7a6-e3e85f6e88af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "vg5_uUaYcoRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "dataset = load_dataset(\"mintujupally/ROCStories\")\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "-6Wtycyi1mST",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab308444-d3cf-416e-a980-f955bf3765e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 78528\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 19633\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Build a reduceded global vocabulary\n",
        "\n",
        "Since we dont have enough computation resources and the vocab size of the datasets is too big to capture it all with the current computation resources which keeps causing out of memory errors.\n",
        "\n",
        "We will reduce the vocab size to a max length of 10000 for the current model(thats the highest i could choose after all experimentation)"
      ],
      "metadata": {
        "id": "jQZ9KqJLfcCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter"
      ],
      "metadata": {
        "id": "VQ9Ryc9ifcKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab(hf_dataset, split=\"train\", max_vocab_size=10000):\n",
        "    counter = Counter()\n",
        "    key = \"text\"\n",
        "    for example in hf_dataset[split]:\n",
        "        # The 'text' field is a list of sentences; join them into a single text.\n",
        "        if isinstance(example[key], list):\n",
        "            text = \" \".join(example[key])\n",
        "        else:\n",
        "            text = example[key]\n",
        "        tokens = text.split()  # Simple tokenization; for production, use a robust tokenizer.\n",
        "        counter.update(tokens)\n",
        "    # Add special tokens needed for our task.\n",
        "    special_tokens = [\"[PAD]\", \"[UNK]\", \"[PREFIX]\", \"[MISSING]\", \"[SUFFIX]\"]\n",
        "    most_common = counter.most_common(max_vocab_size - len(special_tokens))\n",
        "    vocab = {token: idx for idx, token in enumerate(special_tokens)}\n",
        "    for token, freq in most_common:\n",
        "        if token not in vocab:\n",
        "            vocab[token] = len(vocab)\n",
        "    return vocab"
      ],
      "metadata": {
        "id": "1BmOffUYih8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "global_vocab = build_vocab(dataset, split=\"train\", max_vocab_size=10000)\n",
        "print(\"Global vocabulary size:\", len(global_vocab))"
      ],
      "metadata": {
        "id": "VC34TijhiiEL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bad2318e-b2c9-4ecd-9e72-245510cceaf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global vocabulary size: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use this reduced vocabulary size in the rest of the model.\n",
        "vocab_size = len(global_vocab)"
      ],
      "metadata": {
        "id": "Y_sPrYVZiiIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Creating Datasets and Dataloaders"
      ],
      "metadata": {
        "id": "qvQacbq824_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_story(text, min_prefix_tokens=5, min_suffix_tokens=5):\n",
        "    \"\"\"\n",
        "    Transforms a story into prefix, missing middle, and suffix parts.\n",
        "    \"\"\"\n",
        "    tokens = text.split()  # Simple tokenization\n",
        "    n = len(tokens)\n",
        "    if n < (min_prefix_tokens + min_suffix_tokens + 1):\n",
        "        return None\n",
        "    prefix_end = random.randint(min_prefix_tokens, n - min_suffix_tokens - 1)\n",
        "    missing_end = random.randint(prefix_end + 1, n - min_suffix_tokens)\n",
        "    prefix = \" \".join(tokens[:prefix_end])\n",
        "    missing = \" \".join(tokens[prefix_end:missing_end])\n",
        "    suffix = \" \".join(tokens[missing_end:])\n",
        "    return prefix, missing, suffix"
      ],
      "metadata": {
        "id": "oglBOclS4M1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ROCStoriesDataset(Dataset):\n",
        "    def __init__(self, hf_dataset, split=\"train\", vocab=None, max_length=128):\n",
        "        self.data = hf_dataset[split]\n",
        "        self.vocab = vocab  # Use the global reduced vocab for tokenization.\n",
        "        if \"[UNK]\" not in self.vocab:\n",
        "            self.vocab[\"[UNK]\"] = len(self.vocab)\n",
        "        self.max_length = max_length  # Limit sequence length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Use the \"text\" key. If it's a list, join it into a single string.\n",
        "        story_field = self.data[idx][\"text\"]\n",
        "        if isinstance(story_field, list):\n",
        "            text = \" \".join(story_field)\n",
        "        else:\n",
        "            text = story_field\n",
        "\n",
        "        transformed = transform_story(text)\n",
        "        if transformed is None:\n",
        "            transformed = (text, \"\", \"\")\n",
        "        prefix, missing, suffix = transformed\n",
        "\n",
        "        # Concatenate parts with special tokens.\n",
        "        full_input = f\"[PREFIX] {prefix} [MISSING] {missing} [SUFFIX] {suffix}\"\n",
        "        tokens = full_input.split()\n",
        "        # Truncate to max_length tokens.\n",
        "        tokens = tokens[:self.max_length]\n",
        "        indices = [self.vocab.get(token, self.vocab[\"[UNK]\"]) for token in tokens]\n",
        "        x = torch.tensor(indices, dtype=torch.long)\n",
        "        return x"
      ],
      "metadata": {
        "id": "yTWi1lvv4Vz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets and dataloaders using ROCStories.\n",
        "train_dataset = ROCStoriesDataset(dataset, split=\"train\", vocab=global_vocab, max_length=128)\n",
        "test_dataset = ROCStoriesDataset(dataset, split=\"test\", vocab=global_vocab, max_length=128)"
      ],
      "metadata": {
        "id": "T80FvKVb4hC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The batch size is also supposed to be 1. I have tried taking 2 and it caused out of memory error"
      ],
      "metadata": {
        "id": "GgNVd2HTq8Tg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True,\n",
        "                          collate_fn=lambda x: nn.utils.rnn.pad_sequence(x, batch_first=True, padding_value=0))\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False,\n",
        "                        collate_fn=lambda x: nn.utils.rnn.pad_sequence(x, batch_first=True, padding_value=0))"
      ],
      "metadata": {
        "id": "IXIT0RGL7Dh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train dataset size:\", len(train_dataset))\n",
        "print(\"Validation dataset size:\", len(test_dataset))"
      ],
      "metadata": {
        "id": "Pzr-GZvD7G5w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ee72eb4-5cc7-4f92-ad24-549c675d6993"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset size: 78528\n",
            "Validation dataset size: 19633\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Belief State Transformer Overview\n",
        "\n",
        "The Belief State Transformer is designed to capture a compact representation (or *belief state*) of a partial sequence. Given a sequence $ x_{1:T} $ and a partial sequence $ s = x_{1:t} $, the model learns a vector $ v_s $ such that a randomized function $ g(v_s) $ can sample from the conditional distribution $ P(x_{t+1:T} \\mid x_{1:t}) $.\n",
        "\n",
        "The architecture generally consists of:\n",
        "- **Forward Encoder:** Processes the prefix $ x_{1:t} $ to produce forward states.\n",
        "- **Backward Encoder:** Processes the sequence in reverse to capture future context.\n",
        "- **Text Head:** Combines the forward and backward states to predict tokens in the missing (middle) section.\n",
        "- **Efficient Prefix-Suffix Loss Computation:** Computes loss over all valid prefix-suffix pairs using a specialized loss function that handles multiple pairs efficiently.\n",
        "\n",
        "This design allows the model to perform the \"fill-in-the-middle\" task effectively.\n"
      ],
      "metadata": {
        "id": "lYTzIvNk7yVr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Equations\n",
        "\n",
        "For any probability distribution over a set of sequences $ P(x_{1:T}) $, and for any partial sequence $ s = x_{1:t} $, we define a vector $ v_s $ to be a **belief state** for $ s $ if there exists a randomized function $ g $ such that:\n",
        "\n",
        "$$\n",
        "g(v_s) \\sim P(x_{t+1:T} \\mid x_{1:t}).\n",
        "$$\n",
        "\n",
        "The efficient computation of the prefix-suffix loss is given by:\n",
        "\n",
        "1. Compute the forward state $ F $ from the prefix and the backward state $ B $ from the reversed sequence.\n",
        "2. For each valid prefix-suffix pair, extract the corresponding states.\n",
        "3. Concatenate the states and pass through the text head to obtain logits.\n",
        "4. Reshape and compute the cross-entropy loss over all pairs.\n",
        "\n",
        "The pseudocode for the loss function is provided in the research paper.\n"
      ],
      "metadata": {
        "id": "0VEaKmUP8LfT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Creating the Transformer Encoder\n",
        "\n",
        "This shows how generally a transformer encoder works"
      ],
      "metadata": {
        "id": "3suxbS_U9UaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTransformerEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, hidden_dim, num_layers, dropout=0.1):\n",
        "        super(SimpleTransformerEncoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=hidden_dim, dropout=dropout, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len)\n",
        "        embedded = self.embedding(x)  # (batch, seq_len, embed_dim)\n",
        "        # Transformer expects (seq_len, batch, embed_dim)\n",
        "        embedded = embedded.transpose(0, 1)\n",
        "        encoded = self.transformer_encoder(embedded)  # (seq_len, batch, embed_dim)\n",
        "        encoded = encoded.transpose(0, 1)\n",
        "        return self.dropout(encoded)"
      ],
      "metadata": {
        "id": "6Dkn3faZ-lrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer_encoder = SimpleTransformerEncoder(vocab_size, embed_dim=512, num_heads=8, hidden_dim=2048, num_layers=4).to(device)"
      ],
      "metadata": {
        "id": "QAHb_FaY-3yK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Putting It All Together: Belief State Transformer"
      ],
      "metadata": {
        "id": "NnKr9nEq-55V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BeliefStateTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        super(BeliefStateTransformer, self).__init__()\n",
        "        # Forward encoder for the prefix\n",
        "        self.enc_F = nn.Sequential(\n",
        "            nn.Embedding(vocab_size, embed_dim),\n",
        "            nn.Linear(embed_dim, embed_dim)\n",
        "        )\n",
        "        # Backward encoder for the suffix (applied on reversed sequence)\n",
        "        self.enc_B = nn.Sequential(\n",
        "            nn.Embedding(vocab_size, embed_dim),\n",
        "            nn.Linear(embed_dim, embed_dim)\n",
        "        )\n",
        "        # Text head: combines forward and backward states to produce token logits.\n",
        "        self.text_head = nn.Sequential(\n",
        "            nn.Linear(embed_dim * 2, embed_dim),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(embed_dim, vocab_size * 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, T)\n",
        "        f = self.enc_F(x)  # (batch, T, embed_dim)\n",
        "        b = self.enc_B(x)  # (batch, T, embed_dim)\n",
        "        return f, b"
      ],
      "metadata": {
        "id": "egIxyKFAX4fD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def belief_state_objective(all_f, all_b, text_head, x):\n",
        "    \"\"\"\n",
        "    Efficient computation of the prefix-suffix loss.\n",
        "    x: (batch, T) token indices.\n",
        "    all_f: forward states from the encoder.\n",
        "    all_b: backward states from the encoder.\n",
        "    text_head: head network to produce logits.\n",
        "    \"\"\"\n",
        "    bs, T = x.shape\n",
        "    forward_state = all_f  # (batch, T, embed_dim)\n",
        "    backward_state = all_b.flip(1)  # Reverse the sequence along T\n",
        "    ft = torch.arange(T, dtype=torch.int32, device=x.device)\n",
        "    bt = torch.arange(T, dtype=torch.int32, device=x.device)\n",
        "    combinations = torch.cartesian_prod(ft, bt)  # All (i, j) pairs\n",
        "    # Only consider pairs where j - i >= 2 and j < T\n",
        "    combinations = combinations[(combinations[:, 1] - combinations[:, 0] >= 2)]\n",
        "    fb_pairs = combinations.clone()\n",
        "    fb_pairs = fb_pairs[combinations[:, 1] < T]\n",
        "    f_idxs = fb_pairs[:, 0]\n",
        "    b_idxs = fb_pairs[:, 1]\n",
        "    nt_idxs = (combinations[:, 0] + 1)\n",
        "\n",
        "    f_selected = forward_state[:, f_idxs]   # (batch, num_pairs, embed_dim)\n",
        "    b_selected = backward_state[:, b_idxs]    # (batch, num_pairs, embed_dim)\n",
        "\n",
        "    single_labels_f = x[:, nt_idxs].unsqueeze(2)  # (batch, num_pairs, 1)\n",
        "    single_labels_b = x[:, b_idxs].unsqueeze(2)     # (batch, num_pairs, 1)\n",
        "    single_labels = torch.cat((single_labels_f, single_labels_b), dim=2)  # (batch, num_pairs, 2)\n",
        "\n",
        "    # Concatenate forward and backward states\n",
        "    logits = text_head(torch.cat([f_selected, b_selected], dim=2))  # (batch, num_pairs, vocab_size*2)\n",
        "    fb_numpairs = fb_pairs.shape[0]\n",
        "    logits = logits.reshape((bs, fb_numpairs, 2, -1))  # (batch, num_pairs, 2, vocab_size)\n",
        "    logits = logits.reshape((bs * fb_numpairs * 2, -1))\n",
        "    single_labels = single_labels.reshape((bs * fb_numpairs * 2))\n",
        "    loss = nn.CrossEntropyLoss()(logits, single_labels)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "c9QetWDZX_F_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BeliefStateTransformer(vocab_size, embed_dim=512).to(device)"
      ],
      "metadata": {
        "id": "nRSx8qlOYfiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Setting up Training Code"
      ],
      "metadata": {
        "id": "j_onS55rYh9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 3\n",
        "learning_rate = 1e-3\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0.0\n",
        "    # Use tqdm to track progress over batches\n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        # Forward pass: compute forward and backward states.\n",
        "        f, b = model(batch)\n",
        "\n",
        "        # Detach f and b for the objective computation, but retain gradients for backpropagation.\n",
        "        _f = f.detach().requires_grad_()\n",
        "        _b = b.detach().requires_grad_()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = belief_state_objective(_f, _b, model.text_head, batch)\n",
        "        loss.backward()\n",
        "\n",
        "        # Backpropagate the gradients to update encoders (simulate two-stage gradient update)\n",
        "        f.backward(_f.grad)\n",
        "        b.backward(_b.grad)\n",
        "\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_loader):.4f}\")"
      ],
      "metadata": {
        "id": "o3Yg294JY-VO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "75f009f4-a5e3-43e7-86b4-99a47c37acb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3: 100%|██████████| 78528/78528 [35:04<00:00, 37.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3, Loss: 5.8891\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3: 100%|██████████| 78528/78528 [35:03<00:00, 37.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/3, Loss: 5.8020\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3:  30%|███       | 23570/78528 [10:30<24:29, 37.41it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-5cb93310614e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Forward pass: compute forward and backward states.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Detach f and b for the objective computation, but retain gradients for backpropagation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-a6a24199aae4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# x: (batch, T)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_F\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch, T, embed_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_B\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch, T, embed_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I had to stop the model training because i had some work to do and it already took 1 hour 30 mins to reach till here."
      ],
      "metadata": {
        "id": "Cx2IE2Re5-Pd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Make prediction on a custom input"
      ],
      "metadata": {
        "id": "DYrivExblXMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_custom_input(model, input_text, vocab):\n",
        "    \"\"\"\n",
        "    Given a custom input text, perform the fill-in-the-middle prediction,\n",
        "    and map the predicted token index back to its corresponding word.\n",
        "    \"\"\"\n",
        "    # Mimic the same tokenization as used in our dataset.\n",
        "    tokens = input_text.split()\n",
        "    indices = [vocab.get(token, vocab[\"[UNK]\"]) for token in tokens]\n",
        "    x = torch.tensor(indices, dtype=torch.long).unsqueeze(0).to(device)  # (1, seq_len)\n",
        "\n",
        "    # Get forward and backward representations\n",
        "    f, b = model(x)\n",
        "    T = x.shape[1]\n",
        "    if T < 4:\n",
        "        print(\"Input too short for fill-in-the-middle prediction.\")\n",
        "        return\n",
        "    prefix_idx = T // 3\n",
        "    suffix_idx = T - T // 3\n",
        "\n",
        "    # Extract forward state at prefix_idx and backward state at suffix_idx (from reversed sequence)\n",
        "    f_state = f[:, prefix_idx:prefix_idx+1, :]  # (1, 1, embed_dim)\n",
        "    b_state = b.flip(1)[:, suffix_idx:suffix_idx+1, :]  # (1, 1, embed_dim)\n",
        "\n",
        "    # Combine and run through the text head to get logits\n",
        "    combined = torch.cat([f_state, b_state], dim=2)\n",
        "    logits = model.text_head(combined)\n",
        "    logits = logits.reshape(-1, vocab_size)[0]\n",
        "    predicted_token = torch.argmax(logits).item()\n",
        "\n",
        "    # Map the predicted token index back to a word using the vocabulary.\n",
        "    # Build a reverse vocabulary (index -> token).\n",
        "    reverse_vocab = {idx: token for token, idx in vocab.items()}\n",
        "    predicted_word = reverse_vocab.get(predicted_token, \"[UNK]\")\n",
        "\n",
        "    print(\"Predicted token index:\", predicted_token)\n",
        "    print(\"Predicted token word:\", predicted_word)"
      ],
      "metadata": {
        "id": "8vzG0CXwo6CU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example custom input\n",
        "custom_story = \"[PREFIX] Once upon a time, a brave knight [MISSING] [SUFFIX] embarked on a quest.\"\n",
        "predict_custom_input(model, custom_story, global_vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chjP-QBgpD4d",
        "outputId": "11f22e7c-5230-4f53-c337-8923a4701460"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted token index: 15\n",
            "Predicted token word: he\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is just one word prediction lets try multiple words"
      ],
      "metadata": {
        "id": "JncQmJaV7Nrw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def top_k_sampling(logits, k=50, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Sample from the logits using top-k sampling with temperature.\n",
        "\n",
        "    Args:\n",
        "        logits (torch.Tensor): 1D tensor of shape (vocab_size,).\n",
        "        k (int): Number of top candidates to sample from.\n",
        "        temperature (float): Temperature parameter to control randomness.\n",
        "\n",
        "    Returns:\n",
        "        int: The sampled token index.\n",
        "    \"\"\"\n",
        "    logits = logits / temperature\n",
        "    topk_logits, topk_indices = torch.topk(logits, k)\n",
        "    probabilities = F.softmax(topk_logits, dim=-1)\n",
        "    sampled_index = torch.multinomial(probabilities, num_samples=1)\n",
        "    return topk_indices[sampled_index].item()\n",
        "\n",
        "def predict_missing_sequence(model, prefix_text, suffix_text, vocab, max_missing=20, k=50, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Given a prefix and suffix, iteratively generate a missing sequence using top-k sampling.\n",
        "\n",
        "    Args:\n",
        "        model: The BeliefStateTransformer model.\n",
        "        prefix_text (str): The prefix part of the text.\n",
        "        suffix_text (str): The suffix part of the text.\n",
        "        vocab (dict): The vocabulary mapping tokens to indices.\n",
        "        max_missing (int): Maximum number of tokens to generate.\n",
        "        k (int): Number of top candidates to sample from.\n",
        "        temperature (float): Temperature for sampling.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated missing text.\n",
        "    \"\"\"\n",
        "    # Build reverse vocab for mapping indices back to tokens.\n",
        "    reverse_vocab = {idx: token for token, idx in vocab.items()}\n",
        "    missing_tokens = []\n",
        "\n",
        "    # Helper: convert text to tensor of token indices.\n",
        "    def text_to_indices(text):\n",
        "        tokens = text.split()\n",
        "        indices = [vocab.get(token, vocab[\"[UNK]\"]) for token in tokens]\n",
        "        return torch.tensor(indices, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    prefix_tensor = text_to_indices(prefix_text)\n",
        "    suffix_tensor = text_to_indices(suffix_text)\n",
        "\n",
        "    for i in range(max_missing):\n",
        "        # Get forward representation for the current prefix.\n",
        "        f_prefix = model.enc_F(prefix_tensor)  # (1, L_prefix, embed_dim)\n",
        "        f_state = f_prefix[:, -1, :]  # Last token representation\n",
        "\n",
        "        # Get backward representation for the suffix.\n",
        "        b_suffix = model.enc_B(suffix_tensor)  # (1, L_suffix, embed_dim)\n",
        "        b_state = b_suffix.flip(1)[:, 0, :]  # First token of reversed sequence\n",
        "\n",
        "        # Combine to form the belief state.\n",
        "        combined_state = torch.cat([f_state, b_state], dim=1)  # (1, 2*embed_dim)\n",
        "        logits = model.text_head(combined_state)  # (1, vocab_size*2)\n",
        "        # Use only the first half of logits corresponding to our vocabulary.\n",
        "        logits = logits[:, :vocab_size].squeeze(0)  # (vocab_size,)\n",
        "\n",
        "        # Sample the next token using top-k sampling.\n",
        "        next_index = top_k_sampling(logits, k=k, temperature=temperature)\n",
        "        next_token = reverse_vocab.get(next_index, \"[UNK]\")\n",
        "\n",
        "        # Stopping criteria: if a special token is predicted.\n",
        "        if next_token in [\"[SUFFIX]\", \"[PAD]\"]:\n",
        "            break\n",
        "\n",
        "        missing_tokens.append(next_token)\n",
        "        # Append the generated token to the prefix and update prefix_tensor.\n",
        "        prefix_text = prefix_text + \" \" + next_token\n",
        "        prefix_tensor = text_to_indices(prefix_text)\n",
        "\n",
        "    missing_sequence = \" \".join(missing_tokens)\n",
        "    return missing_sequence\n",
        "\n",
        "# Example usage:\n",
        "prefix_example = \"Once upon a time, a brave knight\"\n",
        "suffix_example = \"embarked on a quest.\"\n",
        "generated_missing = predict_missing_sequence(model, prefix_example, suffix_example, global_vocab, max_missing=20, k=50, temperature=1.0)\n",
        "print(\"Generated missing sequence:\", generated_missing)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQzNMoJXpEPp",
        "outputId": "7e8032ad-0ed4-4873-e6a6-03899530bdff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated missing sequence: He would play football player. One day, she couldn't afford the most [UNK] in the most of fun it was\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay well it doesnt work quite well because of lack of training and there is [unk] cuz of lack of vocab_size\n",
        "\n",
        "But i think this is the best for now i can do with the resources that i have."
      ],
      "metadata": {
        "id": "S3HenB9M6iVj"
      }
    }
  ]
}