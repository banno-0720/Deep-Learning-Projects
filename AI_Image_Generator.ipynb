{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOdym5ds9fImZu+crfs2aTE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/banno-0720/Deep-Learning-Projects/blob/main/AI_Image_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AI IMAGE GENERATOR(ZERO SHOT INFERENCE)\n",
        "  This notebook shows how to:\n",
        " 1. Install dependencies  \n",
        " 2. Load and run the Stable Diffusion v1.5 zeroâ€‘shot pipeline  \n",
        " 3. Generate local sample images  \n",
        " 4. Programmatically create a Hugging Face Space and push a Gradio demo  \n",
        " 5. Serve the demo directly in this notebook  \n",
        "\n",
        " **Prerequisites**:  \n",
        " - You have a valid HF token with write access (set in Colab via `userdata.set('HF_TOKEN', '<your-token>')`)\n",
        " - Your HF username is `HimanshuGoyal2004`"
      ],
      "metadata": {
        "id": "dtCdpUq6sBQi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. ğŸ”§ Install Required Libraries\n",
        " We need Diffusers, Transformers, Torch, Accelerate, XFormers (optional), Gradio, and the Hugging Face Hub client.\n"
      ],
      "metadata": {
        "id": "kxa_yCBTsQ56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U diffusers transformers torch torchvision accelerate xformers gradio huggingface_hub"
      ],
      "metadata": {
        "id": "58FTL5q6hAMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. ğŸ”‘ Import Neccessary Libraries & Authenticate to Hugging Face\n",
        " Import necessary modules and log in using your HF token stored in Colab's user data."
      ],
      "metadata": {
        "id": "OKFlwF5LsoGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from google.colab import userdata\n",
        "from diffusers import AutoPipelineForText2Image\n",
        "from huggingface_hub import login, create_repo, get_full_repo_name, upload_folder"
      ],
      "metadata": {
        "id": "l-1m7EyBhG4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve HF token from Colab userdata\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "if not HF_TOKEN or not HF_TOKEN.startswith(\"hf_\"):\n",
        "    raise ValueError(\"Please store your HF token in Colab via userdata.set('HF_TOKEN', '<token>')\")"
      ],
      "metadata": {
        "id": "J8keqk7BtTPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Log in to Hugging Face(Optional)\n",
        "# login(token=HF_TOKEN)"
      ],
      "metadata": {
        "id": "TffJAx9ZtWyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## 3. ğŸš€ Load Zero-Shot Stable Diffusion Pipeline\n",
        "Load the pretrained Stable Diffusion v1.5 pipeline in FP16 on GPU (or FP32 on CPU).  \n",
        "This lets us generate images from text prompts without any fine-tuning."
      ],
      "metadata": {
        "id": "-KtNB440t1Np"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "dtype  = torch.float16 if device == \"cuda\" else torch.float32\n",
        "\n",
        "# Load the pipeline\n",
        "pipeline = AutoPipelineForText2Image.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\",\n",
        "    torch_dtype=dtype\n",
        ").to(device)\n",
        "\n",
        "print(f\"ğŸš© Loaded pipeline on {device} with dtype={dtype}\")"
      ],
      "metadata": {
        "id": "alW6QuochJ9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. ğŸ¨ Generate & Save Local Samples\n",
        " Run zero-shot generation on a few example prompts and save images locally."
      ],
      "metadata": {
        "id": "WtSik_YfuRIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [\n",
        "    \"A serene mountain lake at sunrise\",\n",
        "    \"A futuristic city skyline at night\",\n",
        "    \"An astronaut sculpting a statue on the moon\",\n",
        "]\n",
        "\n",
        "for i, prompt in enumerate(prompts, 1):\n",
        "    img = pipeline(prompt, num_inference_steps=50, guidance_scale=7.5).images[0]\n",
        "    fname = f\"zero_shot_{i}.png\"\n",
        "    img.save(fname)\n",
        "    print(f\"âœ… Saved `{fname}` for prompt: â€œ{prompt}â€\")"
      ],
      "metadata": {
        "id": "cgjcaxzahMPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. ğŸ—ï¸  Prepare Demo Folder for Your Space\n",
        " We will build a local folder `./sd_zero_shot_space/` containing:\n",
        " - `app.py` (Gradio demo)\n",
        " - `requirements.txt`\n",
        " - `README.md` (with YAML frontâ€‘matter for Space config)\n"
      ],
      "metadata": {
        "id": "f5hCjEEWusYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LOCAL_DEMO_FOLDER_PATH = \"./sd_zero_shot_space\"\n",
        "HF_USERNAME            = \"HimanshuGoyal2004\"\n",
        "HF_SPACE_NAME          = \"sd-zero-shot-demo\"\n",
        "HF_REPO_ID             = f\"{HF_USERNAME}/{HF_SPACE_NAME}\"\n",
        "HF_REPO_TYPE           = \"space\"\n",
        "HF_SPACE_SDK           = \"gradio\"\n",
        "HF_PRIVATE             = False\n",
        "\n",
        "os.makedirs(LOCAL_DEMO_FOLDER_PATH, exist_ok=True)\n",
        "\n",
        "# â”€â”€ app.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "app_py = \"\"\"\n",
        "import gradio as gr\n",
        "import torch\n",
        "from diffusers import AutoPipelineForText2Image\n",
        "\n",
        "# Detect device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "dtype  = torch.float16 if device=='cuda' else torch.float32\n",
        "\n",
        "# Load pipeline\n",
        "pipe = AutoPipelineForText2Image.from_pretrained(\n",
        "    'runwayml/stable-diffusion-v1-5', torch_dtype=dtype\n",
        ").to(device)\n",
        "\n",
        "def generate(prompt):\n",
        "    try:\n",
        "        out = pipe(prompt, guidance_scale=7.5, num_inference_steps=30)\n",
        "        return out.images[0]\n",
        "    except Exception as e:\n",
        "        return f\\\"Error: {e}\\\"\n",
        "\n",
        "# Build Gradio interface\n",
        "gr.Interface(\n",
        "    fn=generate,\n",
        "    inputs=gr.Textbox(lines=2, placeholder='Enter prompt here...'),\n",
        "    outputs='image',\n",
        "    title='Stable Diffusion v1.5 Zeroâ€‘Shot',\n",
        "    description='Generate images from text (zeroâ€‘shot).',\n",
        "    allow_flagging='never'\n",
        ").launch()\n",
        "\"\"\"\n",
        "with open(f\"{LOCAL_DEMO_FOLDER_PATH}/app.py\", \"w\") as f:\n",
        "    f.write(app_py.lstrip())\n",
        "\n",
        "# â”€â”€ requirements.txt â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "requirements_txt = \"\"\"\n",
        "torch\n",
        "diffusers\n",
        "transformers\n",
        "accelerate\n",
        "gradio\n",
        "huggingface_hub\n",
        "\"\"\"\n",
        "with open(f\"{LOCAL_DEMO_FOLDER_PATH}/requirements.txt\", \"w\") as f:\n",
        "    f.write(requirements_txt.lstrip())\n",
        "\n",
        "# â”€â”€ README.md â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "readme_md = f\"\"\"\n",
        "---\n",
        "title: \"Zeroâ€‘Shot Stable Diffusion Demo\"\n",
        "emoji: \"ğŸ¨\"\n",
        "colorFrom: \"blue\"\n",
        "colorTo: \"purple\"\n",
        "sdk: \"gradio\"\n",
        "sdk_version: \"5.34.0\"\n",
        "app_file: \"app.py\"\n",
        "pinned: false\n",
        "---\n",
        "\n",
        "# {HF_SPACE_NAME}\n",
        "\n",
        "A zeroâ€‘shot Stable Diffusion v1.5 demo via Gradio.\n",
        "\n",
        "- **Model**: `runwayml/stable-diffusion-v1-5`\n",
        "- **Inference**: Zeroâ€‘shot textâ†’image\n",
        "\n",
        "Try it out live on Spaces!\n",
        "\"\"\"\n",
        "with open(f\"{LOCAL_DEMO_FOLDER_PATH}/README.md\", \"w\") as f:\n",
        "    f.write(readme_md.lstrip())\n",
        "\n",
        "print(f\"ğŸ—‚ï¸ Prepared demo folder at {LOCAL_DEMO_FOLDER_PATH}\")"
      ],
      "metadata": {
        "id": "vzu_v_KEhPkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. ğŸŒ Create & Upload Your Hugging Face Space\n",
        "Programmatically create a new public Gradio Space and push the demo folder into it."
      ],
      "metadata": {
        "id": "2DuQuIIivpr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Space repo (idempotent)\n",
        "\n",
        "create_repo(\n",
        "    repo_id   = HF_REPO_ID,\n",
        "    repo_type = HF_REPO_TYPE,\n",
        "    private   = HF_PRIVATE,\n",
        "    space_sdk = HF_SPACE_SDK,\n",
        "    exist_ok  = True\n",
        ")\n",
        "print(f\"ğŸ”— Created Space repo: {HF_REPO_ID}\")\n",
        "\n",
        "# Upload all files from our demo folder\n",
        "upload_folder(\n",
        "    repo_id     = HF_REPO_ID,\n",
        "    folder_path = LOCAL_DEMO_FOLDER_PATH,\n",
        "    path_in_repo= \"\",\n",
        "    repo_type   = HF_REPO_TYPE,\n",
        "    ignore_patterns=[\"*.pyc\", \"__pycache__\"]\n",
        ")\n",
        "print(f\"ğŸ“¡ Uploaded demo to https://huggingface.co/spaces/{HF_REPO_ID}\")"
      ],
      "metadata": {
        "id": "mJtsZ56Mvxxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. ğŸ”§ (Optional) Enable GPU in Your Space\n",
        " 1. Go to your Space: https://huggingface.co/spaces/{HF_REPO_ID}  \n",
        " 2. Click **Settings â†’ Hardware â†’ Runtime type**  \n",
        " 3. Select **GPU (T4)** for faster inference  \n",
        " 4. Rebuild and try the demo"
      ],
      "metadata": {
        "id": "RgWLZhQ3vEsf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. ğŸ¨ Zero-Shot Inference with Gradio Blocks\n",
        "\n",
        "In this cell, we build a **Gradio Blocks** interface for zero-shot text-to-image generation using our Stable Diffusion pipeline:\n",
        "\n",
        "1. **Device & dtype detection**  \n",
        "   We check for a GPU; if available we use **FP16**, otherwise we fall back to **FP32** on CPU.\n",
        "\n",
        "2. **Pipeline loading**  \n",
        "   We load the pretrained `runwayml/stable-diffusion-v1-5` model into our chosen dtype and device.\n",
        "\n",
        "3. **`generate(prompt)` function**  \n",
        "   - Takes a user text prompt  \n",
        "   - Runs the diffusion pipeline with guidance  \n",
        "   - Returns the generated `PIL.Image` or an error message\n",
        "\n",
        "4. **Gradio Blocks layout**  \n",
        "   - A Markdown header  \n",
        "   - A row containing:  \n",
        "     - A **Textbox** for prompt input  \n",
        "     - A **Button** to trigger generation  \n",
        "   - An **Image** output component\n",
        "\n",
        "5. **Launch**  \n",
        "   The interface appears inline in Colab, allowing you to enter text and see results directly."
      ],
      "metadata": {
        "id": "G1p_R4tvxc2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import torch\n",
        "from diffusers import AutoPipelineForText2Image\n",
        "\n",
        "# 1ï¸âƒ£ Detect device and set dtype\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "dtype  = torch.float16 if device == \"cuda\" else torch.float32\n",
        "\n",
        "# 2ï¸âƒ£ Load the pretrained pipeline in the correct dtype\n",
        "pipe = AutoPipelineForText2Image.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\",\n",
        "    torch_dtype=dtype\n",
        ").to(device)\n",
        "\n",
        "# 3ï¸âƒ£ Define the generation function\n",
        "def generate(prompt):\n",
        "    try:\n",
        "        # Run inference with classifier-free guidance\n",
        "        out = pipe(\n",
        "            prompt,\n",
        "            guidance_scale=7.5,       # Strength of guidance\n",
        "            num_inference_steps=30    # Number of diffusion steps\n",
        "        )\n",
        "        return out.images[0]         # Return the generated image\n",
        "    except Exception as e:\n",
        "        # If something goes wrong, return the error text\n",
        "        return f\"Error during generation:\\n{e}\"\n",
        "\n",
        "# 4ï¸âƒ£ Build the Gradio Blocks interface\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## ğŸ¨ Zero-Shot Stable Diffusion Demo\")\n",
        "    with gr.Row():\n",
        "        inp = gr.Textbox(label=\"Prompt\")   # Input textbox for user prompt\n",
        "        btn = gr.Button(\"Generate\")        # Button to trigger generation\n",
        "    out = gr.Image(label=\"Output\")         # Image component for display\n",
        "    btn.click(fn=generate, inputs=inp, outputs=out)\n",
        "\n",
        "# 5ï¸âƒ£ Launch the interface inline\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "lts6TljLhWau"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}