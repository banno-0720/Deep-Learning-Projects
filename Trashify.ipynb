{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPH67qy007CDUa9fz4Y9Ffz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/banno-0720/Deep-Learning-Projects/blob/main/Trashify.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeLVmtH1H1ra"
      },
      "source": [
        "# 1. Overview\n",
        "\n",
        "Welcome to the Learn Hugging Face Object Detection project!\n",
        "\n",
        "Inside this project, we'll learn bits and pieces about the Hugging Face ecosystem as well as how to build our own custom object detection model.\n",
        "\n",
        "We'll start with a collection of images with bounding box files as our dataset, fine-tune an existing computer vision model to detect items in an image and then share our model as a demo others can use.\n",
        "\n",
        "<figure style=\"text-align: center;\">\n",
        "    <img src=\"https://huggingface.co/datasets/mrdbourke/learn-hf-images/resolve/main/learn-hf-trashify-object-detection/00-what-were-going-to-build.png\"\n",
        "     alt=\"A presentation slide titled 'Project: Trashify' with a trash bin emoji illustrates a machine learning workflow. The first stage, 'Data,' shows an image of a hand holding trash near a bin, with bounding boxes identifying 'bin,' 'trash,' and 'hand,' and is labeled with 'ðŸ¤— Hugging Face Datasets.' This feeds into the 'Model' stage, represented by a neural network diagram and labeled 'ðŸ¤— Hugging Face Transformers.' Finally, the 'Demo' stage displays a screenshot of the 'Trashify Object Detection Demo V4' web application, where the model successfully identifies these elements in an input image, labeled 'ðŸ¤— Hugging Face Hub/Spaces + Gradio.' Arrows indicate the flow from Data to Model to Demo.\"\n",
        "     style=\"width: 100%; max-width: 900px; height: auto;\"/>\n",
        "     <figcaption>We're going to put on our startup hats and build a Trashify object detection model using tools from the Hugging Face ecosystem.</figcaption>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRUB8bFdH1rb"
      },
      "source": [
        "## 1.1 What we're going to build\n",
        "\n",
        "We're going to be bulding Trashify ðŸš®, an **object detection model** which incentivises people to pick up trash in their local area by detecting `bin`, `trash`, `hand`.\n",
        "\n",
        "If all three items are detected, a person gets +1 point!\n",
        "\n",
        "For example, say you were going for a walk around your neighbourhood and took a photo of yourself picking up a piece (with your **hand** or **trash arm**) of **trash** and putting it in the **bin**, you would get a point.\n",
        "\n",
        "With this object detection model, you could deploy it to an application which would automatically detect the target classes and then save the result to an online leaderboard.\n",
        "\n",
        "The incentive would be to score the most points, in turn, picking up the most piecces of trash, in a given area.\n",
        "\n",
        "More specifically, we're going to follow the following steps:\n",
        "\n",
        "1. **[Data](https://huggingface.co/datasets/mrdbourke/trashify_manual_labelled_images): Problem defintion and dataset preparation** - Getting a dataset/setting up the problem space.\n",
        "2. **[Model](https://huggingface.co/docs/transformers/en/model_doc/conditional_detr): Finding, training and evaluating a model** - Finding an object detection model suitable for our problem on Hugging Face and customizing it to our own dataset.\n",
        "3. **[Demo](https://huggingface.co/spaces/mrdbourke/trashify_demo_v3): Creating a demo and put our model into the real world** - Sharing our trained model in a way others can access and use.\n",
        "\n",
        "By the end of this project, you'll have a trained model and [demo on Hugging Face](https://huggingface.co/spaces/mrdbourke/trashify_demo_v3) you can share with others:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFJSfcj6H1rd"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\"\"\"\n",
        "<iframe\n",
        "\tsrc=\"https://mrdbourke-trashify-demo-v4.hf.space\"\n",
        "\tframeborder=\"0\"\n",
        "\twidth=\"850\"\n",
        "\theight=\"850\"\n",
        "></iframe>\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQRCCeZUH1rh"
      },
      "source": [
        "## 1.2 What is object detection?\n",
        "\n",
        "Object detection is the process of identifying and locating an item in an image.\n",
        "\n",
        "Where *item* can mean almost anything.\n",
        "\n",
        "For example:\n",
        "\n",
        "* Detecting car **licence plates** in a video feed (videos are a series of images) for a parking lot entrance.\n",
        "* Detecting **delivery people** walking towards your front door on a security camera.\n",
        "* Detecting **defects** on a manufacturing line.\n",
        "* Detecting [**pot holes** in the road](https://ieeexplore.ieee.org/abstract/document/9968423) so repair works can automatically be scheduled.\n",
        "* Detecting **small pests (Varroa Mite)** on the bodies of bees.\n",
        "* Detecting [**weeds** in a field](https://ai.meta.com/blog/pytorch-drives-next-gen-intelligent-farming-machines/) so you know what to remove and what to keep.\n",
        "\n",
        "--\n",
        "\n",
        "Examples of actual trash identification projects, see:\n",
        "\n",
        "- Google using machine learning for trash identification â€” [https://sustainability.google/operating-sustainably/stories/circular-economy-marketplace/](https://sustainability.google/operating-sustainably/stories/circular-economy-marketplace/)\n",
        "- Trashify website for identifying trash â€” [https://www.trashify.tech/](https://www.trashify.tech/)\n",
        "- Waste management with deep learning â€” [https://www.sciencedirect.com/science/article/abs/pii/S0956053X23001915](https://www.sciencedirect.com/science/article/abs/pii/S0956053X23001915)\n",
        "- Label Studio being used for labelling a trash dataset â€” [https://labelstud.io/blog/ameru-labeling-for-a-greener-world/](https://labelstud.io/blog/ameru-labeling-for-a-greener-world/)\n",
        "\n",
        "<figure style=\"text-align: center;\">\n",
        "    <img src=\"https://huggingface.co/datasets/mrdbourke/learn-hf-images/resolve/main/learn-hf-trashify-object-detection/01-where-object-detection-is-used.png\"\n",
        "     alt=\"A collage of five images illustrating computer vision applications: the top left shows a car at a parking barrier with its license plate highlighted by a green bounding box; the top right displays various trash items on a conveyor belt, each identified and outlined by a colored bounding box and label; the bottom left is a close-up of a bee with a green bounding box highlighting a varroa mite on its back; the bottom middle features a damaged road with two potholes, each outlined by a green bounding box; and the image on the right shows a smart waste bin with an interactive screen and a green bounding box highlighting an item in its collection bag visible through a window.\"\n",
        "     style=\"width: 100%; max-width: 900px; height: auto;\"/>\n",
        "     <figcaption>Different problems where object detection can be used: license plate detection, trash type identification, pest detection on bees and pothole detection.</figcaption>\n",
        "</figure>\n",
        "\n",
        "**Image classification** deals with classifying an image as a whole into a single `class`, object detection endeavours to find the specific target item and *where* it is in an image.\n",
        "\n",
        "One of the most common ways of showing where an item is in an image is by displaying a **bounding box** (a rectangle-like box around the target item).\n",
        "\n",
        "An object detection model will often take an input image tensor in the shape `[3, 640, 640]` (`[colour_channels, height, width]`) and output a tensor in the form `[class_name, x_min, y_min, x_max, y_max]` or `[class_name, x1, y1, x2, y2]` (this is two ways to write the same example format, there are more formats, we'll see these below in @tbl-bbox-formats).\n",
        "\n",
        "Where:\n",
        "\n",
        "* `class_name` = The classification of the target item (e.g. `\"car\"`, `\"person\"`, `\"banana\"`, `\"piece_of_trash\"`, this could be almost anything).\n",
        "* `x_min` = The `x` value of the top left corner of the box.\n",
        "* `y_min` = The `y` value of the top left corner of the box.\n",
        "* `x_max` = The `x` value of the bottom right corner of the box.\n",
        "* `y_max` = The `y` value of the bottom right corner of the box."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnpWJg51H1rj"
      },
      "source": [
        "## 1.3 Why train your own object detection models?\n",
        "\n",
        "You can customize **pre-trained models** for object detection as well as API-powered models and LLMs such as [Gemini](https://ai.google.dev/gemini-api/docs/vision?lang=python#bbox), [LandingAI](https://landing.ai/agentic-object-detection) and [DINO-X](https://github.com/IDEA-Research/DINO-X-API).\n",
        "\n",
        "Depending on your requirements, there are several pros and cons for using your own model versus using an API.\n",
        "\n",
        "Training/fine-tuning your own model:\n",
        "\n",
        "| Pros | Cons |\n",
        "| :----- | :----- |\n",
        "| **Control:** Full control over model lifecycle. | Can be complex to get setup. |\n",
        "| No usage limits (aside from compute constraints). | Requires dedicated compute resources for training/inference. |\n",
        "| Can train once and deploy everywhere/whenever you want (for example, Tesla deploying a model to all self-driving cars). | Requires maintenance over time to ensure performance remains up to par. |\n",
        "| **Privacy:** Data can be kept in-house/app and doesnâ€™t need to go to a third party. | Can require longer development cycles compared to using existing APIs. |\n",
        "| **Speed:** Customizing a small model for a specific use case often means it runs much faster on local hardware, for example, modern object detection models can achieve 70-100+ FPS (frames per second) on modern GPU hardware. | |\n",
        "\n",
        "Using a pre-built model API:\n",
        "\n",
        "| Pros | Cons |\n",
        "| :----- | :----- |\n",
        "| **Ease of use:** often can be setup within a few lines of code. | If the model API goes down, your service goes down. |\n",
        "| No maintenance of compute resources. | Data is required to be sent to a third-party for processing. |\n",
        "| Access to the most advanced models. | The API may have usage limits per day/time period. |\n",
        "| Can scale if usage increases. | Can be much slower than using dedicated models due to requiring an API call. |\n",
        "\n",
        "For this project, we're going to focus on fine-tuning our own model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvY61ujjH1rk"
      },
      "source": [
        "## 1.4 Workflow we're going to follow\n",
        "\n",
        "Start with data (or skip this step and go straight to a model) -> get/customize a model -> build and share a demo.\n",
        "\n",
        "With this in mind, our motto is *data, model, demo!*\n",
        "\n",
        "More specifically, we're going to follow the rough workflow of:\n",
        "\n",
        "1. Create, preprocess and load data using [Hugging Face Datasets](https://huggingface.co/docs/datasets/index).\n",
        "2. Define the model we'd like use with [`transformers.AutoModelForObjectDetection`](https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForObjectDetection) (or another similar model class).\n",
        "3. Define training arguments (these are hyperparameters for our model) with [`transformers.TrainingArguments`](https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments).\n",
        "4. Pass `TrainingArguments` from 3 and target datasets to an instance of [`transformers.Trainer`](https://huggingface.co/docs/transformers/en/main_classes/trainer).\n",
        "5. Train the model by calling [`Trainer.train()`](https://huggingface.co/docs/transformers/v4.40.2/en/main_classes/trainer#transformers.Trainer.train).\n",
        "6. Save the model (to our local machine or to the Hugging Face Hub).\n",
        "7. Evaluate the trained model by making and inspecting predctions on the test data.\n",
        "8. Turn the model into a shareable demo.\n",
        "\n",
        "I say rough because machine learning projects are often non-linear in nature.\n",
        "\n",
        "As in, because machine learning projects involve many experiments, they can kind of be all over the place.\n",
        "\n",
        "But this worfklow will give us some good guidelines to follow.\n",
        "\n",
        "<figure style=\"text-align: center; display: inline-block;\">\n",
        "    <!-- figtemplate -->\n",
        "    <img src=\"https://huggingface.co/datasets/mrdbourke/learn-hf-images/resolve/main/learn-hf-text-classification/01-hugging-face-workflow.png\"\n",
        "     alt=\"The diagram shows the Hugging Face model development workflow, which includes the following steps: start with an idea or problem, get data ready (turn into tensors/create data splits), pick a pretrained model (to suit your problem), train/fine-tune the model on your custom data, evaluate the model, improve through experimentation, save and upload the fine-tuned model to the Hugging Face Hub, and turn your model into a shareable demo. Tools used in this workflow are Datasets/Tokenizers, Transformers/PEFT/Accelerate/timm, Hub/Spaces/Gradio, and Evaluate.\"\n",
        "     style=\"width: 100%; max-width: 900px; height: auto;\"/>\n",
        "     <figcaption style=\"width: 100%; box-sizing: border-box;\">A general Hugging Face workflow from idea to shared model and demo using tools from the Hugging Face ecosystem. You'll notice some of the steps don't match with our workflow outline above. This is because the text-based workflow outline above breaks some of the steps down for educational purposes. These kind of workflows are not set in stone and are more of guide than specific directions. See information on each of the tools in the <a href=\"https://huggingface.co\">Hugging Face documentation</a>.</figcaption>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MF5TShgEvyNo"
      },
      "source": [
        "# 2. Importing necessary libraries\n",
        "\n",
        "If you're running on your local computer, be sure to check out the getting [setup guide](https://www.learnhuggingface.com/extras/setup) to make sure you have everything you need.\n",
        "\n",
        "If you're using Google Colab, many of them the following libraries will be installed by default.\n",
        "\n",
        "However, we'll have to install a few extras to get everything working.\n",
        "\n",
        "We'll need to install the following libraries from the Hugging Face ecosystem:\n",
        "\n",
        "* [`transformers`](https://huggingface.co/docs/transformers/en/installation) - comes pre-installed on Google Colab but if you're running on your local machine, you can install it via `pip install transformers`.\n",
        "* [`datasets`](https://huggingface.co/docs/datasets/installation) - a library for accessing and manipulating datasets on and off the Hugging Face Hub, you can install it via `pip install datasets`.\n",
        "* [`evaluate`](https://huggingface.co/docs/evaluate/installation) - a library for evaluating machine learning model performance with various metrics, you can install it via `pip install evaluate`.\n",
        "* [`accelerate`](https://huggingface.co/docs/accelerate/basic_tutorials/install) - a library for training machine learning models faster, you can install it via `pip install accelerate`.\n",
        "* [`gradio`](https://www.gradio.app/guides/quickstart#installation) - a library for creating interactive demos of machine learning models, you can install it via `pip install gradio`.\n",
        "\n",
        "And the following library is not part of the Hugging Face ecosystem but it is helpful for evaluating our models:\n",
        "\n",
        "* [`torchmetrics`](https://lightning.ai/docs/torchmetrics/stable/) - a library containing many evaluation metrics compatible with PyTorch/Transformers, you can install it via `pip install torchmetrics`.\n",
        "\n",
        "We can also check the versions of our software with `package_name.__version__`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kRX3YggvyNq"
      },
      "outputs": [],
      "source": [
        "# Install/import dependencies (this is mostly for Google Colab, as the other dependences are available by default in Colab)\n",
        "try:\n",
        "  import datasets\n",
        "  import gradio as gr\n",
        "  import torchmetrics\n",
        "  import pycocotools\n",
        "except ModuleNotFoundError:\n",
        "\n",
        "  # If a module isn't found, install it\n",
        "  !pip install -U datasets gradio # -U stands for \"upgrade\" so we'll get the latest version by default\n",
        "  !pip install -U torchmetrics[detection]\n",
        "\n",
        "  import datasets\n",
        "  import gradio as gr\n",
        "\n",
        "  # Required for evalation\n",
        "  import torchmetrics\n",
        "  import pycocotools # make sure we have this for torchmetrics\n",
        "\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "\n",
        "# Check versions (as long as you've got the following versions or higher, you should be good)\n",
        "print(f\"Using transformers version: {transformers.__version__}\")\n",
        "print(f\"Using datasets version: {datasets.__version__}\")\n",
        "print(f\"Using torch version: {torch.__version__}\")\n",
        "print(f\"Using torchmetrics version: {torchmetrics.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-p5u_cjhzoGw"
      },
      "source": [
        "# 3. Getting a dataset\n",
        "\n",
        "Okay, now we're got the required libraries, let's get a dataset.\n",
        "\n",
        "The dataset you often determines the type of model you use as well as the quality of the outputs of that model.\n",
        "\n",
        "Meaning, if you have a high quality dataset, chances are, your future model could also have high quality outputs.\n",
        "\n",
        "It also means if your dataset is of poor quality, your model will likely also have poor quality outputs.\n",
        "\n",
        "For an object detection problem, your dataset will likely come in the form of a group of images as well as a file with annotations belonging to those images.\n",
        "\n",
        "For example, you might have the following setup:\n",
        "\n",
        "```\n",
        "folder_of_images/\n",
        "    image_1.jpeg\n",
        "    image_2.jpeg\n",
        "    image_3.jpeg\n",
        "annotations.json\n",
        "```\n",
        "\n",
        "Where the `annotations.json` contains details about the contains of each image:\n",
        "\n",
        "```{.json filename=\"annotations.json\"}\n",
        "[\n",
        "    {\n",
        "        'image_path': 'image_1.jpeg',\n",
        "        'image_id': 42,\n",
        "        'annotations':\n",
        "            {\n",
        "                'file_name': ['image_1.jpeg'],\n",
        "                'image_id': [42],\n",
        "                'category_id': [1],\n",
        "                'bbox': [\n",
        "                            [360.20001220703125, 528.5, 177.1999969482422, 261.79998779296875],\n",
        "                        ],\n",
        "                'area': [46390.9609375]\n",
        "            },\n",
        "        'label_source': 'manual_prodigy_label',\n",
        "        'image_source': 'manual_taken_photo'\n",
        "    },\n",
        "\n",
        "    ...(more labels down here)\n",
        "]\n",
        "```\n",
        "\n",
        "Don't worry too much about the exact meaning of everything in the above `annotations.json` file for now (this is only one example, there are many different ways object detection information could be displayed).\n",
        "\n",
        "The main point is that each target image is paired with an assosciated label.\n",
        "\n",
        "Now like all good machine learning cooking shows, I've prepared a dataset from earlier.\n",
        "\n",
        "<figure style=\"text-align: center;\">\n",
        "    <a href=\"https://huggingface.co/datasets/mrdbourke/trashify_manual_labelled_images\">\n",
        "    <img src=\"https://huggingface.co/datasets/mrdbourke/learn-hf-images/resolve/main/learn-hf-trashify-object-detection/03-trashify-dataset-on-huggingface.png\"\n",
        "     alt=\"A screenshot of a Hugging Face webpage displaying the 'mrdbourke/trashify_manual_labelled_images' dataset, showing the 'Dataset card' tab with the 'Dataset Viewer' section visible, which lists image thumbnails, 'image_id', 'annotations', 'label_source', and 'image_source' for each entry; on the right sidebar, dataset statistics like 'Downloads last month' (106), 'Size of downloaded dataset files' (1.03 GB), and 'Number of rows' (1,128) are displayed, along with a 'Use this dataset' button; below the viewer, a 'Load data' section shows Python code to import and load the dataset using the 'datasets' library.\"\n",
        "     style=\"width: 100%; max-width: 900px; height: auto;\"/>\n",
        "     </a>\n",
        "     <figcaption>Our Trashify dataset is available on Hugging Face. These images have been labelled manually with bounding boxes for different classes.</figcaption>\n",
        "</figure>\n",
        "\n",
        "It's stored on Hugging Face Datasets (also called the Hugging Face Hub) under the name [`mrdbourke/trashify_manual_labelled_images`](https://huggingface.co/datasets/mrdbourke/trashify_manual_labelled_images).\n",
        "\n",
        "You can draw boxes manually with a tool called [Prodigy](https://prodi.gy/features/computer-vision)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTeSsSiRzoGx"
      },
      "source": [
        "\n",
        "## 3.1 Loading the dataset\n",
        "\n",
        "To load a dataset stored on the Hugging Face Hub we can use the [`datasets.load_dataset(path=NAME_OR_PATH_OF_DATASET)`](https://huggingface.co/docs/datasets/en/package_reference/loading_methods#datasets.load_dataset) function and pass it the name/path of the dataset we want to load.\n",
        "\n",
        "In our case, our dataset name is `mrdbourke/trashify_manual_labelled_images` (you can also change this for your own dataset).\n",
        "\n",
        "And since our dataset is hosted on Hugging Face, when we run the following code for the first time, it will download it.\n",
        "\n",
        "If your target dataset is quite large, this download may take a while.\n",
        "\n",
        "However, once the dataset is downloaded, subsequent reloads will be mush faster.\n",
        "\n",
        "Let's load our dataset and check it out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vAKDnQhzoGy"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load our Trashify dataset\n",
        "dataset = load_dataset(path=\"mrdbourke/trashify_manual_labelled_images\")\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQ9-BnSRzoGz"
      },
      "outputs": [],
      "source": [
        "print(f\"[INFO] Length of original dataset: {len(dataset['train'])}\")\n",
        "print(f\"[INFO] Dataset features:\")\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "pprint(dataset['train'].features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldYYmSVBzoG0"
      },
      "source": [
        "## 3.2 Viewing a single sample from our data\n",
        "\n",
        "Let's check out a single sample from our dataset.\n",
        "\n",
        "We can index on a single sample of the `\"train\"` set just like indexing on a Python list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1sAP_DwzoG0"
      },
      "outputs": [],
      "source": [
        "# View a single sample of the dataset\n",
        "dataset[\"train\"][42]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrJijvk_zoG1"
      },
      "source": [
        "We see a few more details here compared to just looking at the features.\n",
        "\n",
        "We notice the `image` is a [`PIL.Image`](https://pillow.readthedocs.io/en/stable/reference/Image.html) with size `960x1280` (width x height).\n",
        "\n",
        "And the `file_name` is a UUID (Universially Unique Identifier, made with [`uuid.uuid4()`](https://docs.python.org/3/library/uuid.html#uuid.uuid4)).\n",
        "\n",
        "The `bbox` field in the `annotations` key contains a list of bounding boxes assosciated with the image.\n",
        "\n",
        "In this case, there are 3 different bounding boxes.\n",
        "\n",
        "With the `category_id` values of `5`, `1`, `0` (we'll map these to class names shortly).\n",
        "\n",
        "Let's inspect a single bounding box."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTCFrEvtzoG1"
      },
      "outputs": [],
      "source": [
        "dataset[\"train\"][42][\"annotations\"][\"bbox\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19_vXniwzoG1"
      },
      "source": [
        "This array gives us the coordinates of a single bounding box in the format `XYWH`.\n",
        "\n",
        "Where:\n",
        "\n",
        "* `X` is the x-coordinate of the top left corner of the box (`333.1`).\n",
        "* `Y` is the y-coordinate of the top left corner of the box (`611.2`).\n",
        "* `W` is the width of the box (`244.9`).\n",
        "* `H` is the height of the box (`321.3`).\n",
        "\n",
        "All of these values are in absolute pixel values (meaning an x-coordinate of `333.1` is `333.1` pixels across on the x-axis)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zddfu7ACzoG1"
      },
      "source": [
        "## 3.3 Extracting the category names from our data\n",
        "\n",
        "Before we start to visualize our sample image and bounding boxes, let's extract the category names from our dataset.\n",
        "\n",
        "We can do so by accessing the `features` attribute our of `dataset` and then following it through to find the `category_id` feature, this contains a list of our text-based class names.\n",
        "\n",
        "Let's access the class names in our dataset and save them to a variable `categories`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yljRJC75zoG2"
      },
      "outputs": [],
      "source": [
        "# Get the categories from the dataset\n",
        "categories = dataset[\"train\"].features[\"annotations\"].feature[\"category_id\"]\n",
        "\n",
        "# Get the names attribute\n",
        "categories.names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQCIpuXhzoG2"
      },
      "source": [
        "We get the following class names:\n",
        "\n",
        "* `bin` - A rubbish bin or trash can.\n",
        "* `hand` - A person's hand.\n",
        "* `not_bin` - Negative version of `bin` for items that look like a `bin` but shouldn't be identified as one.\n",
        "* `not_hand` - Negative version of `hand` for items that look like a `hand` but shouldn't be identified as one.\n",
        "* `not_trash` - Negative version of `trash` for items that look like `trash` but shouldn't be identified as it.\n",
        "* `trash` - An item of trash you might find on a walk such as an old plastic bottle, food wrapper, cigarette butt or used coffee cup.\n",
        "* `trash_arm` - A mechanical arm used for picking up trash.\n",
        "\n",
        "The goal of our computer vision model will be: given an image, detect items belonging to these target classes if they are present."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qV6PkwfhzoG3"
      },
      "source": [
        "## 3.4 Creating a mapping from numbers to labels\n",
        "\n",
        "Now we've got our text-based class names, let's create a mapping from label to ID and ID to label.\n",
        "\n",
        "For each of these, Hugging Face use the terminology `label2id` and `id2label` respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A38XayuWzoG3"
      },
      "outputs": [],
      "source": [
        "# Map ID's to class names and vice versa\n",
        "id2label = {i: class_name for i, class_name in enumerate(categories.names)}\n",
        "label2id = {value: key for key, value in id2label.items()}\n",
        "\n",
        "print(f\"Label to ID mapping:\\n{label2id}\\n\")\n",
        "print(f\"ID to label mapping:\\n{id2label}\")\n",
        "# id2label, label2id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_jGjnxmzoG3"
      },
      "source": [
        "## 3.5 Creating a colour palette\n",
        "\n",
        "Ok we know which class name matches to which ID, now let's create a dictionary of different colours we can use to display our bounding boxes.\n",
        "\n",
        "It's one thing to plot bounding boxes, it's another thing to make them look nice.\n",
        "\n",
        "And we always want our plots looking nice!\n",
        "\n",
        "We'll colour the positive classes `bin`, `hand`, `trash`, `trash_arm` in nice bright colours.\n",
        "\n",
        "And the negative classes `not_bin`, `not_hand`, `not_trash` in a light red colour to indicate they're the negative versions.\n",
        "\n",
        "Our colour dictionary will map `class_name` -> `(red, green, blue)`  (or [RGB](https://en.wikipedia.org/wiki/RGB_color_model)) colour values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMtFPn9fzoG4"
      },
      "outputs": [],
      "source": [
        "# Make colour dictionary\n",
        "colour_palette = {\n",
        "    'bin': (0, 0, 224),         # Bright Blue (High contrast with greenery) in format (red, green, blue)\n",
        "    'not_bin': (255, 80, 80),   # Light Red to indicate negative class\n",
        "\n",
        "    'hand': (148, 0, 211),      # Dark Purple (Contrasts well with skin tones)\n",
        "    'not_hand': (255, 80, 80),  # Light Red to indicate negative class\n",
        "\n",
        "    'trash': (0, 255, 0),       # Bright Green (For trash-related items)\n",
        "    'not_trash': (255, 80, 80), # Light Red to indicate negative class\n",
        "\n",
        "    'trash_arm': (255, 140, 0), # Deep Orange (Highly visible)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6Hqz5mBzoG5"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Normalize RGB values to 0-1 range\n",
        "def normalize_rgb(rgb_tuple):\n",
        "    return tuple(x/255 for x in rgb_tuple)\n",
        "\n",
        "# Turn colors into normalized RGB values for matplotlib\n",
        "colors_and_labels_rgb = [(key, normalize_rgb(value)) for key, value in colour_palette.items()]\n",
        "\n",
        "# Create figure and axis\n",
        "fig, ax = plt.subplots(1, 7, figsize=(8, 1))\n",
        "\n",
        "# Flatten the axis array for easier iteration\n",
        "ax = ax.flatten()\n",
        "\n",
        "# Plot each color square\n",
        "for idx, (label, color) in enumerate(colors_and_labels_rgb):\n",
        "    ax[idx].add_patch(plt.Rectangle(xy=(0, 0),\n",
        "                                    width=1,\n",
        "                                    height=1,\n",
        "                                    facecolor=color))\n",
        "    ax[idx].set_title(label)\n",
        "    ax[idx].set_xlim(0, 1)\n",
        "    ax[idx].set_ylim(0, 1)\n",
        "    ax[idx].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbDbOhs6da9W"
      },
      "source": [
        "# 4. Plotting a single image and visualizing the boxes\n",
        "\n",
        "Let's take a random sample from our `dataset` and plot the image as well as the box on it.\n",
        "\n",
        "To save some space in our notebook (plotting many images can increase the size of our notebook dramatically), we'll create two small helper functions:\n",
        "\n",
        "1. `half_image` - Halves the size of a given image.\n",
        "2. `half_boxes` - Divides the input coordinates of a given input box by 2.\n",
        "\n",
        "These functions aren't 100% necessary in our workflow.\n",
        "\n",
        "They're just to make the images slightly smaller so they fit better in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sv8zvWooda9g"
      },
      "outputs": [],
      "source": [
        "import PIL\n",
        "\n",
        "def half_image(image: PIL.Image) -> PIL.Image:\n",
        "    \"\"\"\n",
        "    Resizes a given input image by half and returns the smaller version.\n",
        "    \"\"\"\n",
        "    return image.resize(size=(image.size[0] // 2, image.size[1] // 2))\n",
        "\n",
        "def half_boxes(boxes):\n",
        "    \"\"\"\n",
        "    Halves an array/tensor of input boxes and returns them. Necessary for plotting them on a half-sized image.\n",
        "\n",
        "    For example:\n",
        "\n",
        "    boxes = [100, 100, 100, 100]\n",
        "    half_boxes = half_boxes(boxes)\n",
        "    print(half_boxes)\n",
        "\n",
        "    >>> [50, 50, 50, 50]\n",
        "    \"\"\"\n",
        "    if isinstance(boxes, list):\n",
        "        # If boxes are list of lists, then we have multiple boxes\n",
        "        for box in boxes:\n",
        "            if isinstance(box, list):\n",
        "                return [[coordinate // 2 for coordinate in box] for box in boxes]\n",
        "            else:\n",
        "                return [coordinate // 2 for coordinate in boxes]\n",
        "\n",
        "    if isinstance(boxes, np.ndarray):\n",
        "        return (boxes // 2)\n",
        "\n",
        "    if isinstance(boxes, torch.Tensor):\n",
        "        return (boxes // 2)\n",
        "\n",
        "# Test the functions\n",
        "image_test = dataset[\"train\"][42][\"image\"]\n",
        "image_test_half = half_image(image_test)\n",
        "print(f\"[INFO] Original image size: {image_test.size} | Half image size: {image_test_half.size}\")\n",
        "\n",
        "boxes_test_list = [100, 100, 100, 100]\n",
        "print(f\"[INFO] Original boxes: {boxes_test_list} | Half boxes: {half_boxes(boxes_test_list)}\")\n",
        "\n",
        "boxes_test_torch = torch.tensor([100.0, 100.0, 100.0, 100.0])\n",
        "print(f\"[INFO] Original boxes: {boxes_test_torch} | Half boxes: {half_boxes(boxes_test_torch)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEquxPwIda9h"
      },
      "source": [
        "To plot an image and its assosciated boxes, we'll do the following steps:\n",
        "\n",
        "1. Select a random sample from the `dataset`.\n",
        "2. Extract the `\"image\"` (our image is in `PIL` format) and `\"bbox\"` keys from the random sample.\n",
        "    * We can also *optionally* halve the size of our image/boxes to save space. In our case, we will halve our image and boxes.\n",
        "3. Turn the box coordinates into a `torch.tensor` (we'll be using `torchvision` utilities to plot the image and boxes).\n",
        "4. Convert the box format from `XYXY` to `XYWH` using [`torchvision.ops.box_convert`](https://pytorch.org/vision/main/generated/torchvision.ops.box_convert.html) (we do this because `torchvision.utils.draw_bounding_boxes` requires `XYXY` format as input).\n",
        "5. Get a list of label names (e.g. `\"bin\", \"trash\"`, etc) assosciated with each of the boxes as well as a list of colours to match (these will be from our `colour_palette`).\n",
        "6. Draw the boxes on the target image by:\n",
        "    * Turning the image into a tensor with [`torchvision.transforms.functional.pil_to_tensor`](https://pytorch.org/vision/main/generated/torchvision.transforms.functional.pil_to_tensor.html).\n",
        "    * Draw the bounding boxes on our image tensor with [`torchvision.utils.draw_bounding_boxes`](https://pytorch.org/vision/main/generated/torchvision.utils.draw_bounding_boxes.html).\n",
        "    * Turn the image and bounding box tensors back into a `PIL` image with [`torchvision.transforms.functional.pil_to_tensor`](https://pytorch.org/vision/main/generated/torchvision.transforms.functional.pil_to_tensor.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0v_JrQ9qda9i"
      },
      "outputs": [],
      "source": [
        "# Plotting a bounding box on a single image\n",
        "import random\n",
        "\n",
        "import torch\n",
        "\n",
        "from torchvision.ops import box_convert\n",
        "from torchvision.utils import draw_bounding_boxes\n",
        "\n",
        "from torchvision.transforms.functional import pil_to_tensor, to_pil_image\n",
        "\n",
        "# 1. Select a random sample from our dataset\n",
        "random_index = random.randint(0, len(dataset[\"train\"]))\n",
        "print(f\"[INFO] Showing training sample from index: {random_index}\")\n",
        "random_sample = dataset[\"train\"][random_index]\n",
        "\n",
        "# 2. Get image and boxes from random sample\n",
        "random_sample_image = random_sample[\"image\"]\n",
        "random_sample_boxes = random_sample[\"annotations\"][\"bbox\"]\n",
        "\n",
        "# Optional: Half the image and boxes for space saving (all of the following code will work with/without half size images)\n",
        "half_random_sample_image = half_image(random_sample_image)\n",
        "half_random_sample_boxes = half_boxes(random_sample_boxes)\n",
        "\n",
        "# 3. Turn box coordinates in a tensor\n",
        "boxes_xywh = torch.tensor(half_random_sample_boxes)\n",
        "print(f\"Boxes in XYWH format: {boxes_xywh}\")\n",
        "\n",
        "# 4. Convert boxes from XYWH -> XYXY\n",
        "# torchvision.utils.draw_bounding_boxes requires input boxes in XYXY format (X_min, y_min, X_max, y_max)\n",
        "boxes_xyxy = box_convert(boxes=boxes_xywh,\n",
        "                         in_fmt=\"xywh\",\n",
        "                         out_fmt=\"xyxy\")\n",
        "print(f\"Boxes XYXY: {boxes_xyxy}\")\n",
        "\n",
        "# 5. Get label names of target boxes and colours to match\n",
        "random_sample_label_names = [categories.int2str(x) for x in random_sample[\"annotations\"][\"category_id\"]]\n",
        "random_sample_colours = [colour_palette[label_name] for label_name in random_sample_label_names]\n",
        "print(f\"Label names: {random_sample_label_names}\")\n",
        "print(f\"Colour names: {random_sample_colours}\")\n",
        "\n",
        "# 6. Draw the boxes on the image as a tensor and then turn it into a PIL image\n",
        "to_pil_image(\n",
        "    pic=draw_bounding_boxes(\n",
        "        image=pil_to_tensor(pic=half_random_sample_image),\n",
        "        boxes=boxes_xyxy,\n",
        "        colors=random_sample_colours,\n",
        "        labels=random_sample_label_names,\n",
        "        width=2,\n",
        "        label_colors=random_sample_colours\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sh7oxDh0da9j"
      },
      "source": [
        "# 5. Different bounding box formats\n",
        "\n",
        "When drawing our bounding box, we discussed the terms `XYXY` and `XYWH`.\n",
        "\n",
        "One of the most confusing things in the world of object detection is the different formats bounding boxes come in.\n",
        "\n",
        "Are your boxes in `XYXY`, `XYWH` or `CXCYWH`?\n",
        "\n",
        "Are they in absolute format?\n",
        "\n",
        "Or normalized format?\n",
        "\n",
        "The following table contains a non-exhaustive list of some of the most common bounding box formats you'll come across in the wild.\n",
        "\n",
        "| **Box format** | **Description** | **Absolute Example** | **Normalized Example** | **Source** |\n",
        "| ----- | ----- | ----- | ----- | ----- |\n",
        "| XYXY | Describes the top left corner coordinates `(x1, y1)` as well as the bottom right corner coordinates of a box. <br> Also referred to as: <br> `[x1, y1, x2, y2]`  <br> or <br> `[x_min, y_min, x_max, y_max]` | `[8.9, 275.3, 867.5, 964.0]` | `[0.009, 0.215, 0.904, 0.753]` | [PASCAL VOC Dataset](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/htmldoc/devkit_doc.html#SECTION00053000000000000000) uses the absolute version of this format, [`torchvision.utils.draw_bounding_boxes`](https://pytorch.org/vision/main/generated/torchvision.utils.draw_bounding_boxes.html#draw-bounding-boxes) defaults to the absolute version of this format. |\n",
        "| XYWH | Describes the top left corner coordinates `(x1, y1)` as well as the width (`box_width`) and height (`box_height`) of the target box. The bottom right corners `(x2, y2)` are found by adding the width and height to the top left corner coordinates `(x1 + box_width, y1 + box_height)`. <br> Also referred to as: <br> `[x1, y1, box_width, box_height]` <br> or <br> `[x_min, y_min, box_width, box_height]` | `[8.9, 275.3, 858.6, 688.7]` | `[0.009, 0.215, 0.894, 0.538]` | The [COCO (Common Objects in Context) dataset](https://cocodataset.org/#format-data) uses the absolute version of this format, see the section under \"bbox\". |\n",
        "| CXCYWH | Describes the center coordinates of the bounding box `(center_x, center_y)` as well as the width (`box_width`) and height (`box_height`) of the target box. <br> Also referred to as: <br> `[center_x, center_y, box_width, box_height]` | `[438.2, 619.65, 858.6, 688.7]` | `[0.456, 0.484, 0.894, 0.538]` | Normalized version introduced in the [YOLOv3 (You Only Look Once) paper](https://arxiv.org/abs/1804.02767) and is used by many later forms of YOLO. |\n",
        "\n",
        ": Different bounding box formats {#tbl-bbox-formats}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiFimL8nda9y"
      },
      "source": [
        "## 5.1 Absolute or normalized format?\n",
        "\n",
        "In **absolute** coordinate form, bounding box values are in the same format as the width and height dimensions (e.g. our image is `960x1280` pixels).\n",
        "\n",
        "For example in `XYXY` format: `[\"bin\", 8.9, 275.3, 867.5, 964.0]`\n",
        "\n",
        "An `(x1, y1)` (or `(x_min, y_min)`) coordinate of `(8.9, 275.3)` means the top left corner is `8.9` pixels in on the x-axis, and `275.3` pixels down on the y-axis.\n",
        "\n",
        "In **normalized** coordinate form, values are between `[0, 1]` and are proportions of the image width and height.\n",
        "\n",
        "For example in `XYXY` format: `[\"bin\", 0.009, 0.215, 0.904, 0.753]`\n",
        "\n",
        "A normalized `(x1, y1)` (or `(x_min, y_min)`) coordinate of `(0.009, 0.215)` means the top left corner is `0.009 * image_width` pixels in on the x-axis and `0.215 * image_height` down on the y-axis.\n",
        "\n",
        "To convert absolute coordinates to normalized, you can divide x-axis values by the image width and y-axis values by the image height.\n",
        "\n",
        "$$\n",
        "x_{\\text{normalized}} = \\frac{x_{\\text{absolute}}}{\\text{image\\_width}} \\quad y_{\\text{normalized}} = \\frac{y_{\\text{absolute}}}{\\text{image\\_height}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wu59Fw4uda9z"
      },
      "source": [
        "## 5.2 Which bounding box format should you use?\n",
        "\n",
        "The bounding box format you use will depend on the framework, model and existing data you're trying to use.\n",
        "\n",
        "For example, the take the following frameworks:\n",
        "\n",
        "* **PyTorch** - If you're using PyTorch pre-trained models, for example, [`torchvision.models.detection.fasterrcnn_resnet50_fpn`](https://pytorch.org/vision/main/models/generated/torchvision.models.detection.fasterrcnn_resnet50_fpn.html#torchvision.models.detection.fasterrcnn_resnet50_fpn), you'll want **absolute** `XYXY` (`[x1, y1, x2, y2]`) format.\n",
        "* **Hugging Face Transformers** - If you're using a Hugging Face Transformers model such as [Conditional DETR](https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr), you'll want to take note that [outputs from the model can be of one type](https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrForObjectDetection.forward) (e.g. `CXCYWH`) but they can be [post-processed into another type](https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrFeatureExtractor.post_process_object_detection) (e.g. **absolute** `XYXY`).\n",
        "* **Ultralytics YOLO** - If you're using a YOLO-like model such as [Ultralytics YOLO](https://docs.ultralytics.com/datasets/detect/#ultralytics-yolo-format), you'll want **normalized** `CXCYWH` (`[center_x, center_y, width, height]`) format.\n",
        "* **Google Gemini** - If you're using [Google Gemini to predict bounding boxes on your images](https://ai.google.dev/gemini-api/docs/vision?lang=python#bbox), then you'll want to pay attention to the special `[y_min, x_min, y_max, x_max]` (`YXYX`) normalized coordinates.\n",
        "\n",
        "Or if you note that someone has said their model is pre-trained on the COCO dataset, chances are the data has been formatted in `XYWH` format (see @tbl-bbox-formats)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QejfcoHpda91"
      },
      "source": [
        "# 6. Getting an object detection model\n",
        "\n",
        "There are two main ways of getting an object detection model:\n",
        "\n",
        "1. **Building it yourself.** For example, constructing it layer by layer, testing it and training it on your target problem.\n",
        "2. **Using an existing one.** For example, find an existing model on a problem space similar to your own and then adapt it via **transfer learning** to your own task.\n",
        "\n",
        "In our case, we're going to focus on the latter.\n",
        "\n",
        "We'll be taking a pre-trained object detection model and fine-tuning it on our Trashify ðŸš® dataset so it outputs the boxes and labels we're after."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3Uox9euda92"
      },
      "source": [
        "## 6.1 Places to get object detection models\n",
        "\n",
        "Instead of building your own machine learning model from scratch, it's common practice to take an existing model that works on similar problem space to yours and then **fine-tune** it to your own use case.\n",
        "\n",
        "There are several places to get object detection models:\n",
        "\n",
        "| **Location** | **Description** |\n",
        "| ----- | ----- |\n",
        "| [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=object-detection&sort=trending) | One the best places on the internet to find open-source machine learning models of nearly any kind. You can find pre-trained object detection models here such as [`facebook/detr-resnet-50`](https://huggingface.co/facebook/detr-resnet-50), a model from Facebook (Meta) and [`microsoft/conditional-detr-resnet-50`](https://huggingface.co/microsoft/conditional-detr-resnet-50), a model from Microsoft and the model we're going to use as our base model. Many of the models are permissively licensed, meaning you can use them for your own projects.  |\n",
        "| [`torchvision`](https://pytorch.org/vision/stable/models.html) | PyTorch's built-in domain library for computer vision has several [pre-trained object detection models](https://pytorch.org/vision/stable/models.html#object-detection-instance-segmentation-and-person-keypoint-detection) which you can use in your own workflows. |\n",
        "| [paperswithcode.com/task/object-detection](https://paperswithcode.com/task/object-detection) | Whilst not a direct place to download object detection models from, paperswithcode contains benchmarks for many machine learning tasks (including object detection) which shows the current state of the art (best performing) models and usually includes links to where to get the code. |\n",
        "| [Detectron2](https://github.com/facebookresearch/detectron2) | Detectron2 is an open-source library to help with many of the tasks in detecting items in images. Inside you'll find several pre-trained and adaptable models as well as utilities such as data loaders for object detection and segmentation tasks. |\n",
        "| YOLO Series | A running series of [\"You Only Look Once\" models](https://arxiv.org/abs/1506.02640). Usually, the higher the number, the better performing. For example, [`YOLOv11`](https://github.com/ultralytics/ultralytics) by Ultralytics should outperform [`YOLOv10`](https://github.com/THU-MIG/yolov10), however, this often requires testing on your own dataset. Beware of the license, it is under the [AGPL-3.0 license](https://en.wikipedia.org/wiki/GNU_Affero_General_Public_License) which may cause issues in some organizations. |\n",
        "| [`mmdetection` library](https://github.com/open-mmlab/mmdetection) | An open-source library from the OpenMMLab which contains many different open-source models as well as detection-specific utilties. |\n",
        "\n",
        ": Places to get pre-trained object detection models {#tbl-detection-models}\n",
        "\n",
        "When you find a pre-trained object detection model, you'll often see statements such as:\n",
        "\n",
        "> *Conditional DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images).*\n",
        ">\n",
        "> Source: [https://huggingface.co/microsoft/conditional-detr-resnet-50](https://huggingface.co/microsoft/conditional-detr-resnet-50)\n",
        "\n",
        "This means the model has already been trained on the [COCO object detection dataset](https://cocodataset.org/#home) which contains 118,000 images and [80 classes](https://cocodataset.org/#explore) such as `[\"cake\", \"person\", \"skateboard\"...]`.\n",
        "\n",
        "This is a good thing.\n",
        "\n",
        "It means that the model should have a fairly good starting point when we try to adapt it to our own project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnlEqsq0da93"
      },
      "source": [
        "## 6.2 Downloading our model from Hugging Face\n",
        "\n",
        "For our Trashify ðŸš® project we're going to be using the pre-trained object detection model [`microsoft/conditional-detr-resnet-50`](https://huggingface.co/microsoft/conditional-detr-resnet-50) which was originally introduced in the paper [*Conditional DETR for Fast Training Convergence*](https://arxiv.org/abs/2108.06152).\n",
        "\n",
        "To use this model, there are some helpful documentation resources we should be aware of:\n",
        "\n",
        "| **Resource** | **Description** |\n",
        "|:----- |:----- |\n",
        "| [Conditional DETR documentation](https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#conditional-detr) | Contains detailed information on each of the `transformers.ConditionalDetr` classes. |\n",
        "| [`transformers.ConditionalDetrConfig`](https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrConfig) | Contains the configuration settings for our model such as number of layers and other hyperparameters. |\n",
        "| [`transformers.ConditionalDetrImageProcessor`](https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrImageProcessor) | Contains several preprocessing on post processing functions and settings for data going into and out of our model. Here we can set values such as `size` in the [`preprocess`](https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrImageProcessor.preprocess) method which will resize our images to a certain size. We can also use the [`post_process_object_detection`](https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrImageProcessor.post_process_object_detection) method to process the raw outputs of our model into a more usable format. |\n",
        "| [`transformers.ConditionalDetrModelForObjectdetection`](https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrForObjectDetection) | This will enable us to load the Conditional DETR model weights and enable to pass data through them via the [`forward`](https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrForObjectDetection.forward) method. |\n",
        "| [`transformers.AutoImageProcessor`](https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoImageProcessor) | This will enable us to create an instance of `transformers.ConditionalDetrImageProcessor` by passing the model name `microsoft/conditional-detr-resnet-50` to the `from_pretrained` method. Hugging Face Transformers uses several [Auto Classes](https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForObjectDetection) for various problem spaces and models. |\n",
        "| [`transformers.AutoModelForObjectDetection`](https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForObjectDetection) | Enables us to load the model architecture and weights for the Conditional DETR architecture by passing the model name `microsoft/conditional-detr-resnet-50` to the [`from_pretrained` method](https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForObjectDetection.from_pretrained). |\n",
        "\n",
        "\n",
        "We'll get hands-on which each of these throughout the project.\n",
        "\n",
        "For now, if you'd like to read up more on each, I'd highly recommend it.\n",
        "\n",
        "Knowing how to navigate and read through a framework's documentation is a very helpful skill to have.\n",
        "\n",
        "We can load our model with [`transformers.AutoModelForObjectDetection.from_pretrained`](https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForObjectDetection.from_pretrained) and passing in the following parameters:\n",
        "\n",
        "* `pretrained_model_name_or_path` - Our target model, which can be a local path or Hugging Face model name (e.g. `microsoft/conditional-detr-resnet-50`).\n",
        "* `label2id` - A dictionary mapping our class names/labels to their numerical ID, this is so our model will know how many classes to output.\n",
        "* `id2label` - A dictionary mapping numerical IDs to our class names/labels, so our model will know how many classes we're working with and what their IDs are.\n",
        "* `ignore_mismatched_sizes=True` (default) - We'll set this to `True` so that our model can be instatiated with a varying number of classes compared to what it may have been trained on (e.g. if our model was trained on the 91 classes from COCO, we only need 7).\n",
        "* `backbone=\"resnet50\"` (default) - We'll tell our model what kind of computer vision backbone to use for extracting features from our images.\n",
        "\n",
        "See the [full documentation](https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForObjectDetection.from_pretrained) for a full list of parameters we can use."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The term \"DETR\" stands for \"DEtection TRansformer\".\n",
        "\n",
        "Where \"Transformer\" refers to the [Transformer neural network architecture](https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)), specifically the [Vision Transformer](https://en.wikipedia.org/wiki/Vision_transformer) (or ViT) rather than the Hugging Face `transformers` library (quite confusing, yes).\n",
        "\n",
        "So DETR means \"performing detection with the Transformer architecture\".\n",
        "\n",
        "And the \"ResNet\" part stands for \"[Residual Neural Network](https://en.wikipedia.org/wiki/Residual_neural_network)\" which is a common computer vision backbone. The \"50\" refers to the number of layers in the network. Saying \"ResNet-50\" means the 50 layer version of ResNet. ResNet-101 and ResNet-18 are two other larger and smaller variants."
      ],
      "metadata": {
        "id": "DcbQH2PwS6hp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PseXmCLfda95"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.nn.modules.module\") # turn off warnings for loading the model (feel free to comment this if you want to see the warnings)\n",
        "\n",
        "from transformers import AutoModelForObjectDetection\n",
        "\n",
        "MODEL_NAME = \"PekingU/rtdetr_v2_r50vd\"\n",
        "\n",
        "model = AutoModelForObjectDetection.from_pretrained(\n",
        "    pretrained_model_name_or_path=MODEL_NAME,\n",
        "    label2id=label2id,\n",
        "    id2label=id2label,\n",
        "    # Original model was trained with a different number of output classes to ours\n",
        "    # So we'll ignore any mismatched sizes (e.g. 91 vs. 7)\n",
        "    # Try turning this to False and see what happens\n",
        "    ignore_mismatched_sizes=True,\n",
        ")\n",
        "\n",
        "# Uncomment to see full model architecture\n",
        "# model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lg-NE3s6da96"
      },
      "source": [
        "You might've noticed a warning about the model needing to be trained on a down-stream task:\n",
        "\n",
        "> Some weights of ConditionalDetrForObjectDetection were not initialized from the model checkpoint at microsoft/conditional-detr-resnet-50 and are newly initialized because the shapes did not match:\n",
        "> - class_labels_classifier.bias: found shape torch.Size([91]) in the checkpoint and torch.Size([7]) in the model instantiated\n",
        "> - class_labels_classifier.weight: found shape torch.Size([91, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated\n",
        "> You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
        "\n",
        "This is because our model has a different number of target classes (7 in total) comapred to the original model (91 in total, from the COCO dataset).\n",
        "\n",
        "So in order to get this pretrained model to work on our dataset, we'll need to **fine-tune** it.\n",
        "\n",
        "You might also notice that if you set `ignore_mismatched_sizes=False`, you'll get an error:\n",
        "\n",
        "> RuntimeError: Error(s) in loading state_dict for ConditionalDetrForObjectDetection:\n",
        "\tsize mismatch for class_labels_classifier.weight: copying a param with shape torch.Size([91, 256]) from checkpoint, the shape in current model is torch.Size([7, 256]).\n",
        "\tsize mismatch for class_labels_classifier.bias: copying a param with shape torch.Size([91]) from checkpoint, the shape in current model is torch.Size([7]).\n",
        "\tYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.\n",
        "\n",
        "This is a similar warning to the one above.\n",
        "\n",
        "Keep this is mind for when you're working with pretrained models.\n",
        "\n",
        "If you are using data slightly different to what the model was trained on, you may need to alter the setup hyperparameters as well as fine-tune it on your own data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYEZPWTUda-k"
      },
      "source": [
        "## 6.3 Inspecting our model's layers\n",
        "\n",
        "The following subset of layers has been truncated for brevity.\n",
        "\n",
        "```python\n",
        "# Shortened version of the model architecture, print the full model to see all layers\n",
        "ConditionalDetrForObjectDetection(\n",
        "  (model): ConditionalDetrModel(\n",
        "    (backbone): ConditionalDetrConvModel(\n",
        "      (conv_encoder): ConditionalDetrConvEncoder(\n",
        "        (model): FeatureListNet(\n",
        "          (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "          (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
        "              ...\n",
        "          (layer1): Sequential(\n",
        "            (0): Bottleneck(\n",
        "              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
        "              (bn1): ConditionalDetrFrozenBatchNorm2d())))\n",
        "            ...\n",
        "      (position_embedding): ConditionalDetrSinePositionEmbedding()\n",
        "    )\n",
        "    (input_projection): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
        "    (query_position_embeddings): Embedding(300, 256)\n",
        "    (encoder): ConditionalDetrEncoder(\n",
        "      (layers): ModuleList(\n",
        "        (0-5): 6 x ConditionalDetrEncoderLayer(\n",
        "          (self_attn): DetrAttention(\n",
        "              ...\n",
        "          )\n",
        "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True))))\n",
        "    (decoder): ConditionalDetrDecoder(\n",
        "      (layers): ModuleList(\n",
        "        (0): ConditionalDetrDecoderLayer(...)\n",
        "      (layernorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
        "      (query_scale): MLP(\n",
        "        (layers): ModuleList(\n",
        "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)))\n",
        "      (ref_point_head): MLP(\n",
        "        ...\n",
        "      ))))\n",
        "  (class_labels_classifier): Linear(in_features=256, out_features=7, bias=True)\n",
        "  (bbox_predictor): ConditionalDetrMLPPredictionHead(\n",
        "    (layers): ModuleList(\n",
        "      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
        "      (2): Linear(in_features=256, out_features=4, bias=True)))))\n",
        "```\n",
        "\n",
        "If we check out a few of our model's layers, we can see that it is a combination of [convolutional](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html), [attention](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html), [MLP (multi-layer perceptron)](https://en.wikipedia.org/wiki/Multilayer_perceptron) and [linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) layers.\n",
        "\n",
        "\n",
        "We'll feed our input image into our model and layer by layer it will manipulate the pixel values to try and extract patterns in a way so that its internal parameters matches the image to its input annotations.\n",
        "\n",
        "More specifically, if we dive into the final two layer sections:\n",
        "\n",
        "1. `class_labels_classifier` = classification head with `out_features=7` (one for each of our labels, `'bin', 'hand', 'not_bin', 'not_hand', 'not_trash', 'trash', 'trash_arm']`).\n",
        "2. `bbox_predictor` = regression head with `out_features=4` (one for each of our bbox coordinates, e.g. `[center_x, center_y, width, height]`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dzduq_u5da-l"
      },
      "outputs": [],
      "source": [
        "print(f\"[INFO] Final classification layer: {model.class_embed}\\n\")\n",
        "print(f\"[INFO] Final box regression layer: {model.bbox_embed}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZykhEQRjda-m"
      },
      "source": [
        "These two layers are what are going to output the final predictions of our model in structure similar to our annotations.\n",
        "\n",
        "The `class_labels_classifier` will output the predicted class label of a given bounding box output from `bbox_predictor`.\n",
        "\n",
        "In essence, we are trying to get all of the pretrained patterns (also called **parameters**/**weights & biases**) of the previous layers to conform to the ideal outputs we'd like at the end."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPk2RkTAda-n"
      },
      "source": [
        "## 6.4 Counting the number of parameters in our model\n",
        "\n",
        "**Parameters** are individual values which contribute to a model's final output.\n",
        "\n",
        "Parameters are also referred to as weights and biases.\n",
        "\n",
        "You can think of these individual weights as small pushes and pulls on the input data to get it to match the input annotations.\n",
        "\n",
        "If our weights were perfect, we could input an image and always get back the correct bounding boxes and class labels.\n",
        "\n",
        "It's very unlikely to ever have perfect weights (unless your dataset is very small) but we can make them quite good (and useful).\n",
        "\n",
        "When you have a good set of weights, this is known as a good [**representation**](https://en.wikipedia.org/wiki/Feature_learning).\n",
        "\n",
        "Right now, our weights have been trained on COCO, a collection of 91 different common objects.\n",
        "\n",
        "So they have a fairly good representation of detecting general common objects, however, we'd like to **fine-tune** these weights to detect our target objects.\n",
        "\n",
        "Importantly, our model will not be starting from scratch when it begins to train.\n",
        "\n",
        "It will instead take off from its existing knowledge of detecting common objects in images and try to adhere to our task.\n",
        "\n",
        "When it comes to parameters and weights, generally, more is better.\n",
        "\n",
        "Meaning the more parameters your model has, the better representation it can learn.\n",
        "\n",
        "For example, [ResNet50](https://huggingface.co/microsoft/resnet-50) (our computer vision backbone) has ~25 million parameters, about 100 MB in `float32` precision or 50MB in `float16` precision.\n",
        "\n",
        "Whereas a model such as [Llama-3.1-405B](https://huggingface.co/meta-llama/Llama-3.1-405B) has ~405 billion parameters, about 1.45 TB in `float32` precision or 740 GB in `float16` precision, about 16,000x more than ResNet50.\n",
        "\n",
        "However, as we can see having more parameters comes with the tradeoff of size and latency.\n",
        "\n",
        "For each new parameter requires to be stored and it also adds an extra computation unit to your model.\n",
        "\n",
        "In the case of Trashify, since we'd like our model to run on-device (e.g. make predictions live on an iPhone), we'd opt for the smallest number of parameters we could get acceptable results from.\n",
        "\n",
        "If performance is your number 1 criteria and size and latency don't matter, then you'd likely opt for the model with the largest number of parameters (though always evaluate these models on your own data, larger models are *generally* better, not *always* better).\n",
        "\n",
        "Since our model is built using PyTorch, let's write a small function to count the number of:  \n",
        "\n",
        "* Trainable parameters (parameters which will be tweaked during training)\n",
        "* Non-trainable parameters (parameters which will *not* be tweaked during training)\n",
        "* Total parameters (trainable parameters + non-trainable parameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVonYEfSda_I"
      },
      "outputs": [],
      "source": [
        "# Count the number of parameters in the model\n",
        "def count_parameters(model):\n",
        "    \"\"\"Takes in a PyTorch model and returns the number of parameters.\"\"\"\n",
        "    trainable_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    non_trainable_parameters = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
        "    total_parameters = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Total parameters: {total_parameters:,}\")\n",
        "    print(f\"Trainable parameters (will be updated): {trainable_parameters:,}\")\n",
        "    print(f\"Non-trainable parameters (will not be updated): {non_trainable_parameters:,}\")\n",
        "\n",
        "count_parameters(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV3BoH6mda_J"
      },
      "source": [
        "Cool!\n",
        "\n",
        "It looks like our model has a total of `43,396,813` parameters, of which, most of them are trainable.\n",
        "\n",
        "This means that when we fine-tune our model later on, we'll be tweaking the majority of the parameters to try and represent our data.\n",
        "\n",
        "In practice, this is known as **full fine-tuning**, trying to fine-tune a large portion of the model to our data.\n",
        "\n",
        "There are other methods for fine-tuning, such as **feature extraction** (where you only fine-tune the final layers of the model) and **partial fine-tuning** (where you fine-tune a portion of the model).\n",
        "\n",
        "And even methods such as [**LoRA** (Low-Rank Adaptation)](https://huggingface.co/papers/2106.09685) which fine-tunes an adaptor matrix as a compliment to the model's parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbrw6Wxwda_K"
      },
      "source": [
        "### 6.5 Creating a function to build our model\n",
        "\n",
        "Since machine learning is very experimental, we may want to create multiple instances of our `model` to test various things.\n",
        "\n",
        "So let's functionize the creation of a new model with parameters for our target model name, `id2label` and `label2id` dictionaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgDZnVuVda_L"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForObjectDetection\n",
        "\n",
        "# Setup the model\n",
        "def create_model(pretrained_model_name_or_path: str = MODEL_NAME,\n",
        "                 label2id: dict = label2id,\n",
        "                 id2label: dict = id2label):\n",
        "    \"\"\"Creates and returns an instance of AutoModelForObjectDetection.\n",
        "\n",
        "    Args:\n",
        "        pretrained_model_name_or_path (str): The name or path of the pretrained model to load.\n",
        "            Defaults to MODEL_NAME.\n",
        "        label2id (dict): A dictionary mapping class labels to IDs. Defaults to label2id.\n",
        "        id2label (dict): A dictionary mapping class IDs to labels. Defaults to id2label.\n",
        "\n",
        "    Returns:\n",
        "        AutoModelForObjectDetection: A pretrained model for object detection with number of output\n",
        "            classes equivalent to len(label2id).\n",
        "    \"\"\"\n",
        "    model = AutoModelForObjectDetection.from_pretrained(\n",
        "        pretrained_model_name_or_path=pretrained_model_name_or_path,\n",
        "        label2id=label2id,\n",
        "        id2label=id2label,\n",
        "        ignore_mismatched_sizes=True, # default\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRaQZda8da_M"
      },
      "outputs": [],
      "source": [
        "# Create a new model instance\n",
        "model = create_model()\n",
        "# model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjIhwIj_da_N"
      },
      "source": [
        "## 6.6 Trying to pass a single sample through our model (part 1)\n",
        "\n",
        "Okay, now we've got a model, let's put some data through it!\n",
        "\n",
        "When we call our `model`, because it's a PyTorch Module ([`torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module)) it will by default run the `forward` method.\n",
        "\n",
        "In PyTorch, `forward` overrides the special `__call__` method on functions.\n",
        "\n",
        "So we can pass data into our model by running:\n",
        "\n",
        "```python\n",
        "model(input_data)\n",
        "```\n",
        "\n",
        "Which is equivalent to running:\n",
        "\n",
        "```python\n",
        "model.forward(input_data)\n",
        "```\n",
        "\n",
        "To see what happens when we call our model, let's inspect the `forward` method's docstring with [`model.forward?`](https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrModel.forward).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Dga2MAJdbA8"
      },
      "outputs": [],
      "source": [
        "#| output: false\n",
        "# What happens when we call our model?\n",
        "# Note: for PyTorch modules, `forward` overrides the __call__ method,\n",
        "# so calling the model is equivalent to calling the forward method.\n",
        "model.forward?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOuKdv5CdbBf"
      },
      "outputs": [],
      "source": [
        "#| output: false\n",
        "# Do a single forward pass with the model\n",
        "# random_sample_outputs = model(pixel_values=random_sample[\"image\"],\n",
        "#                               pixel_mask=None)\n",
        "# random_sample_outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1N2fowli18f6"
      },
      "source": [
        "<details>\n",
        "\n",
        "<summary>Output of random_sample_outputs</summary>\n",
        "\n",
        "```\n",
        "---------------------------------------------------------------------------\n",
        "AttributeError                            Traceback (most recent call last)\n",
        "Cell In[34], line 2\n",
        "      1 # Do a single forward pass with the model\n",
        "----> 2 random_sample_outputs = model(pixel_values=random_sample[\"image\"],\n",
        "      3                               pixel_mask=None)\n",
        "      4 random_sample_outputs\n",
        "\n",
        "File ~/miniconda3/envs/ai/lib/python3.11/site-packages/torch/nn/modules/module.py:1739, in Module._wrapped_call_impl(self, *args, **kwargs)\n",
        "   1737     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n",
        "   1738 else:\n",
        "-> 1739     return self._call_impl(*args, **kwargs)\n",
        "\n",
        "File ~/miniconda3/envs/ai/lib/python3.11/site-packages/torch/nn/modules/module.py:1750, in Module._call_impl(self, *args, **kwargs)\n",
        "   1745 # If we don't have any hooks, we want to skip the rest of the logic in\n",
        "   1746 # this function, and just call forward.\n",
        "   1747 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n",
        "   1748         or _global_backward_pre_hooks or _global_backward_hooks\n",
        "   1749         or _global_forward_hooks or _global_forward_pre_hooks):\n",
        "-> 1750     return forward_call(*args, **kwargs)\n",
        "   1752 result = None\n",
        "   1753 called_always_called_hooks = set()\n",
        "\n",
        "File ~/miniconda3/envs/ai/lib/python3.11/site-packages/transformers/models/conditional_detr/modeling_conditional_detr.py:1717, in ConditionalDetrForObjectDetection.forward(self, pixel_values, pixel_mask, decoder_attention_mask, encoder_outputs, inputs_embeds, decoder_inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\n",
        "   1714 return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "   1716 # First, sent images through CONDITIONAL_DETR base model to obtain encoder + decoder outputs\n",
        "-> 1717 outputs = self.model(\n",
        "   1718     pixel_values,\n",
        "   1719     pixel_mask=pixel_mask,\n",
        "   1720     decoder_attention_mask=decoder_attention_mask,\n",
        "   1721     encoder_outputs=encoder_outputs,\n",
        "   1722     inputs_embeds=inputs_embeds,\n",
        "   1723     decoder_inputs_embeds=decoder_inputs_embeds,\n",
        "   1724     output_attentions=output_attentions,\n",
        "   1725     output_hidden_states=output_hidden_states,\n",
        "   1726     return_dict=return_dict,\n",
        "   1727 )\n",
        "   1729 sequence_output = outputs[0]\n",
        "   1731 # class logits + predicted bounding boxes\n",
        "\n",
        "File ~/miniconda3/envs/ai/lib/python3.11/site-packages/torch/nn/modules/module.py:1739, in Module._wrapped_call_impl(self, *args, **kwargs)\n",
        "   1737     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n",
        "   1738 else:\n",
        "-> 1739     return self._call_impl(*args, **kwargs)\n",
        "\n",
        "File ~/miniconda3/envs/ai/lib/python3.11/site-packages/torch/nn/modules/module.py:1750, in Module._call_impl(self, *args, **kwargs)\n",
        "   1745 # If we don't have any hooks, we want to skip the rest of the logic in\n",
        "   1746 # this function, and just call forward.\n",
        "   1747 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n",
        "   1748         or _global_backward_pre_hooks or _global_backward_hooks\n",
        "   1749         or _global_forward_hooks or _global_forward_pre_hooks):\n",
        "-> 1750     return forward_call(*args, **kwargs)\n",
        "   1752 result = None\n",
        "   1753 called_always_called_hooks = set()\n",
        "\n",
        "File ~/miniconda3/envs/ai/lib/python3.11/site-packages/transformers/models/conditional_detr/modeling_conditional_detr.py:1521, in ConditionalDetrModel.forward(self, pixel_values, pixel_mask, decoder_attention_mask, encoder_outputs, inputs_embeds, decoder_inputs_embeds, output_attentions, output_hidden_states, return_dict)\n",
        "   1516 output_hidden_states = (\n",
        "   1517     output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "   1518 )\n",
        "   1519 return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "-> 1521 batch_size, num_channels, height, width = pixel_values.shape\n",
        "   1522 device = pixel_values.device\n",
        "   1524 if pixel_mask is None:\n",
        "\n",
        "AttributeError: 'Image' object has no attribute 'shape'\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "We get an error:\n",
        "\n",
        "> AttributeError: 'Image' object has no attribute 'shape'\n",
        "\n",
        "Hmmm... it seems we've tried to pass a `PIL.Image` to our model rather than a `torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`.\n",
        "\n",
        "It looks like our input data might require some preprocessing before we can pass it to our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0wKWxa5dbBh"
      },
      "source": [
        "# 7. Aside: Processor to Model Pattern\n",
        "\n",
        "Many Hugging Face data loading and modelling workflows as well as machine learning workflows in general follow the pattern of:\n",
        "\n",
        "* Data -> Preprocessor -> Model\n",
        "\n",
        "<figure style=\"text-align: center;\">\n",
        "    <img src=\"https://huggingface.co/datasets/mrdbourke/learn-hf-images/resolve/main/learn-hf-trashify-object-detection/04-data-preprocess-model.png\"\n",
        "     alt=\"A diagram illustrating a three-stage object detection pipeline: the 'Data' stage shows an image of a hand discarding trash into a red-lidded bin, with bounding boxes labeling 'bin' (green), 'trash' (blue), and 'hand' (purple), alongside its corresponding JSON-like annotation data; an arrow points to the 'Preprocess' stage, represented by a green rounded rectangle labeled 'RTDetrImageProcessor'; another arrow leads to the 'Model' stage, depicted by a stylized neural network icon and labeled 'RTDetrV2ForObjectDetection'\"\n",
        "     style=\"width: 100%; max-width: 900px; height: auto;\"/>\n",
        "     <figcaption>Workflow we'll follow to create our own custom object detection model. We'll start with images labelled with boxes of trash, bin and hand (and other classes), preprocess the process to be ready for use with a model and then we'll train the model on our preprocessed custom data.</figcaption>\n",
        "</figure>\n",
        "\n",
        "Meaning, the raw input data gets preprocessed or transformed in some way before being passed to a model.\n",
        "\n",
        "Preprocessors and models are often loaded with an [Auto Class](https://huggingface.co/docs/transformers/en/model_doc/auto).\n",
        "\n",
        "An Auto Class pairs a preprocessor and model based on their model name or key.\n",
        "\n",
        "For example:\n",
        "\n",
        "```python\n",
        "from transformers import AutoProcessor, AutoModel\n",
        "\n",
        "# Load raw data\n",
        "raw_data = load_data()\n",
        "\n",
        "# Define target model name\n",
        "MODEL_NAME = \"...\"\n",
        "\n",
        "# Load preprocessor and model (these two are often paired)\n",
        "preprocessor = AutoProcessor.from_pretrained(MODEL_NAME)\n",
        "model = AutoModel.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Preprocess data\n",
        "preprocessed_data = preprocessor.preprocess(raw_data)\n",
        "\n",
        "# Pass preprocessed data to model\n",
        "output = model(preprocessed_data)\n",
        "```\n",
        "\n",
        "This is the same for our Trashify ðŸš® project.\n",
        "\n",
        "We've got our raw data (images and bounding boxes), however, they need to be preprocessed in order for our model to be able to handle them.\n",
        "\n",
        "Previously we tried to pass a sample of raw data to our model and this errored.\n",
        "\n",
        "We can fix this by first preprocessing our raw data with our model's pair preprocessor and *then* passing to our model again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiEjM7X-dbBh"
      },
      "source": [
        "# 8. Loading our model's processor\n",
        "\n",
        "Time to get our raw data ready for our model!\n",
        "\n",
        "To begin, let's load our model's processor.\n",
        "\n",
        "We'll use this to prepare our input images for the model.\n",
        "\n",
        "To do so, we'll use [`transformers.AutoImageProcessor`](https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoImageProcessor) and pass our target model name to the `from_pretrained` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jx2MsAn6dbBi"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoImageProcessor\n",
        "\n",
        "MODEL_NAME = \"PekingU/rtdetr_v2_r50vd\"\n",
        "# MODEL_NAME = \"facebook/detr-resnet-50\" # Could also use this model as an another experiment\n",
        "\n",
        "# Load the image processor\n",
        "image_processor = AutoImageProcessor.from_pretrained(pretrained_model_name_or_path=MODEL_NAME,\n",
        "                                                     use_fast=True) # load the fast version of the processor\n",
        "\n",
        "# Check out the image processor\n",
        "image_processor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TA3Iy7V6dbBs"
      },
      "source": [
        "Ok, a few things going on here.\n",
        "\n",
        "These parameters will transform our input images before we pass them to our model.\n",
        "\n",
        "One of the first things to see is the `image_processor` is expecting our bounding boxes to be in [COCO (or `coco_detection`) format](https://cocodataset.org/#format-data) (this is the default).\n",
        "\n",
        "We'll see what this looks like later on but our processor wants this format because that's the format our model has been trained on (it's generally best practice to input data to a model in the same way its been trained on, otherwise you might get poor results).\n",
        "\n",
        "Another thing to notice is that our input images will be resized to the values of the `size` parameter.\n",
        "\n",
        "In our case, it's currently:\n",
        "\n",
        "```python\n",
        "\"size\": {\n",
        "    \"longest_edge\": 1333,\n",
        "    \"shortest_edge\": 800\n",
        "}\n",
        "```\n",
        "\n",
        "Which means that the longest edge will have size less or equal to `1333` and the shortest edge less or equal to `800`.\n",
        "\n",
        "For simplicity, we'll change this shortly to make both sides the same size.\n",
        "\n",
        "You can read more about what each of these does in the [`transformers.ConditionalDetrImageProcessor` documentation](https://huggingface.co/docs/transformers/en/model_doc/conditional_detr#transformers.ConditionalDetrImageProcessor).\n",
        "\n",
        "Let's update our instance of `transformers.ConditionalDetrImageProcessor` with a few custom parameters:\n",
        "\n",
        "* `do_convert_annotations=True` - This is the default and it will convert our boxes to the format `CXCYWH` or `(center_x, center_y, width, height)` (see @tbl-bbox-formats) in the range `[0, 1]`.\n",
        "* `size` - We'll update the `size` dictionary so all of our images have `\"longest_edge\": 640` and `\"shortest_edge: 640\"`. We'll use a value of `640` which is a common size in world of object detection. But there are also other sizes such as `300x300`, `480x480`, `512x512`, `800x800` and more."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depending on what task you're working on, you might want to tweak the image resolution you're working with.\n",
        "\n",
        "For example, I like this quote from [Lucas Beyer](https://lucasb.eyer.be/articles/vit_cnn_speed.html), a former research scientist at DeepMind and engineer at OpenAI:\n",
        "\n",
        "> My conservative claim is that you can always stretch to a square, and for:\n",
        ">\n",
        "> natural images, meaning most photos, 224pxÂ² is enough;\n",
        "> text in photos, phone screens, diagrams and charts, 448pxÂ² is enough;\n",
        "> desktop screens and single-page documents, 896pxÂ² is enough.\n",
        "\n",
        "Typically, in the case of object detection, you'll want to use a higher value.\n",
        "\n",
        "But this is another thing that is open to experimentation."
      ],
      "metadata": {
        "id": "v8t3_lnZWkS9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWhvKXAjdbBv"
      },
      "outputs": [],
      "source": [
        "# Set image size\n",
        "IMAGE_SIZE = 640 # we could try other sizes here: 300x300, 480x480, 512x512, 640x640, 800x800 (best to experiment and see which works best)\n",
        "\n",
        "# Create a new instance of the image processor with the desired image size\n",
        "image_processor = AutoImageProcessor.from_pretrained(\n",
        "    pretrained_model_name_or_path=MODEL_NAME,\n",
        "    use_fast=True, # use the fast preprocessor\n",
        "    format=\"coco_detection\", # this is the default\n",
        "    do_convert_annotations=True, # defaults to True, converts boxes to (center_x, center_y, width, height) in range [0, 1]\n",
        "    size={\"shortest_edge\": IMAGE_SIZE,\n",
        "          \"longest_edge\": IMAGE_SIZE},\n",
        "    return_segmentation_masks=True,\n",
        "    do_pad=True # make sure all images have 640x640 size thanks to padding\n",
        ")\n",
        "\n",
        "# Optional: View the docstring of our image_processor.preprocess function\n",
        "# image_processor.preprocess?\n",
        "\n",
        "# Check out our new image processor size\n",
        "image_processor.size"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now our images will be resized to a square of size `640x640` when we pass them to our model.\n",
        "\n",
        "How about we try to preprocess our `random_sample`?\n",
        "\n",
        "To do so, we can pass its `\"image\"` key and `\"annotations\"` key to our `image_processor`'s [`preprocess`](https://huggingface.co/docs/transformers/en/model_doc/conditional_detr#transformers.ConditionalDetrImageProcessor.preprocess) method (we can also just called `image_processor` directly as it will call `preprocess` via the `__call__` method)."
      ],
      "metadata": {
        "id": "Q0-tkYXzO3mS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_processor"
      ],
      "metadata": {
        "id": "jIyscSGxO6Ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j51puOtEdbBx"
      },
      "outputs": [],
      "source": [
        "# Try to process a single image and annotation pair (spoiler: this will error)\n",
        "# random_sample_preprocessed = image_processor.preprocess(images=random_sample[\"image\"],\n",
        "#                                                         annotations=random_sample[\"annotations\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQmgu-0OdbBy"
      },
      "source": [
        "We get an error:\n",
        "\n",
        "> ValueError: Invalid COCO detection annotations. Annotations must a dict (single image) or list of dicts (batch of images) with the following keys: `image_id` and `annotations`, with the latter being a list of annotations in the COCO format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk4jsGR0dbBz"
      },
      "source": [
        "## 8.1 Preprocessing a single image\n",
        "\n",
        "Okay so it turns out that our annotations aren't in the format that the `preprocess` method was expecting.\n",
        "\n",
        "Since our pre-trained model was trained on the COCO dataset, the `preprocess` method expects input data to be in line with the COCO format.\n",
        "\n",
        "We can fix this later on by adjusting our annotations.\n",
        "\n",
        "How about we try to preprocess just a single image instead?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DX0ExXN5dbB1"
      },
      "outputs": [],
      "source": [
        "# Preprocess our target sample\n",
        "random_sample_preprocessed_image_only = image_processor.preprocess(images=random_sample[\"image\"],\n",
        "                                                                   annotations=None, # no annotations this time\n",
        "                                                                   return_tensors=\"pt\") # return as PyTorch tensors\n",
        "\n",
        "# Uncomment to see the full output\n",
        "# print(random_sample_preprocessed_image_only)\n",
        "\n",
        "# Print out the keys of the preprocessed image\n",
        "print(random_sample_preprocessed_image_only.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbE_HjjZdbCB"
      },
      "source": [
        "Nice! It looks like the `preprocess` method works on a single image.\n",
        "\n",
        "And it seems like we get a dictionary output with the following keys:\n",
        "\n",
        "* `pixel_values` - the processed pixel values of the input image.\n",
        "* `pixel_mask` - a mask multiplier for the pixel values as to whether they should be paid attention to or not (a value of `0` means the pixel value should be ignored by the model and a value of `1` means the pixel value should be paid attention to by the model)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5seka1wdbCU"
      },
      "outputs": [],
      "source": [
        "# Uncomment to inspect all preprocessed pixel values\n",
        "# print(random_sample_preprocessed_image_only[\"pixel_values\"][0])\n",
        "\n",
        "print(f\"[INFO] Original image shape: {random_sample['image'].size} -> [width, height]\")\n",
        "print(f\"[INFO] Preprocessed image shape: {random_sample_preprocessed_image_only['pixel_values'].shape} -> [batch_size, colour_channles, height, width]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-87CQjJdbCX"
      },
      "source": [
        "Ok wonderful, it looks like our image has been downsized to `[3, 640, 480]` (1 item in the batch, 3 colour channels, 640 pixels high, 480 pixels wide).\n",
        "\n",
        "This is down from its original size of `[960, 1280]` (1280 pixels high, 960 pixels wide)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNZ5ynm2dbCZ"
      },
      "source": [
        "## 8.2 Trying to pass a single sample through our model (part 2)\n",
        "\n",
        "This is exciting!\n",
        "\n",
        "We've processed an image into the format our model is expecting.\n",
        "\n",
        "How about we try another forward by calling `model.forward(pixel_values, pixel_mask)`?\n",
        "\n",
        "Which is the same as calling `model(pixel_values, pixel_mask)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpmnK5vFdbCc"
      },
      "outputs": [],
      "source": [
        "# Do a single forward pass with the model\n",
        "random_sample_outputs = model(\n",
        "    pixel_values=random_sample_preprocessed_image_only[\"pixel_values\"], # model expects input [batch_size, color_channels, height, width]\n",
        "    # pixel_mask=random_sample_preprocessed_image_only[\"pixel_mask\"], # some object detection models expect masks\n",
        ")\n",
        "\n",
        "# Inspect the outputs\n",
        "# random_sample_outputs # uncomment to see full outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our model processed our `random_sample_preprocessed_image_only[\"pixel_values\"]` and returned a [`RTDetrV2ObjectDetectionOutput`](https://huggingface.co/docs/transformers/main/en/model_doc/rt_detr_v2#transformers.RTDetrV2ForObjectDetection.forward) object as output."
      ],
      "metadata": {
        "id": "DO-95f4FRt3n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMz6GirwdbCd"
      },
      "outputs": [],
      "source": [
        "# Check the keys of the output\n",
        "random_sample_outputs.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiqG9-0gdbCe"
      },
      "source": [
        "Breaking these down:\n",
        "\n",
        "* `logits` - The raw outputs from the model, these are the classification [logits](https://datascience.stackexchange.com/questions/31041/what-does-logits-in-machine-learning-mean) we can later apply a [**softmax function**](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html)/[**sigmoid function**](https://en.wikipedia.org/wiki/Sigmoid_function) to to get **prediction probabilties**.\n",
        "* `pred_boxes` - Normalized box coordinates in `CXCYWH` (`(center_x, center_y, width, height)`) format.\n",
        "* `last_hidden_state` - Last hidden state of the last decoder layer of the model.\n",
        "* `encoder_last_hidden_state` - Last hidden state of the last encoder layer of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqlStKptdbCg"
      },
      "outputs": [],
      "source": [
        "# Inspect logits output shape\n",
        "output_logits = random_sample_outputs.logits\n",
        "print(f\"[INFO] Output logits shape: {output_logits.shape} -> [1 image, 300 boxes, 7 classes]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We get an output from our model that coincides with the shape of our data.\n",
        "\n",
        "The final value of `7` in the `output_logits` tensor is equivalent to the number of classes we have.\n",
        "\n",
        "And the `300` is the number of boxes our model predicts for each image (this is defined by the `num_queries` parameter of the [`transformers.RTDetrV2Config`](https://huggingface.co/docs/transformers/main/en/model_doc/rt_detr_v2#transformers.RTDetrV2Config), where `num_queries=300` is the default)."
      ],
      "metadata": {
        "id": "KvnzKrM8SQ_z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-qddNafdbC9"
      },
      "outputs": [],
      "source": [
        "# Inspect predicted boxes output shape\n",
        "output_pred_boxes = random_sample_outputs.pred_boxes\n",
        "print(f\"[INFO] Output predicted boxes shape: {output_pred_boxes.shape} -> [1 image, 300 boxes, 4 coordinates (center_x, center_y, width, height)]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngW4ph1RdbC-"
      },
      "source": [
        "Reading the [documentation for the `forward` method](https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrForObjectDetection.forward), we can determine the output format of our models predicted boxes:\n",
        "\n",
        "> Returns:\n",
        ">\n",
        "> pred_boxes (torch.FloatTensor of shape (batch_size, num_queries, 4)) â€” Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding possible padding). You can use [`post_process_object_detection()`](https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrImageProcessor.post_process_object_detection) to retrieve the unnormalized bounding boxes.\n",
        "\n",
        "This is good to know!\n",
        "\n",
        "It means that the raw output boxes from our model come in normalized `CXCYWH` format (see @tbl-bbox-formats for more).\n",
        "\n",
        "How about we inspect a single box?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmHx54yZdbDQ"
      },
      "outputs": [],
      "source": [
        "# Single example predicted bounding box coordinates\n",
        "print(f\"[INFO] Example output box: {output_pred_boxes[:, 0, :][0].detach()} -> (center_x, center_y, width, height)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGvDOd0gKxWi"
      },
      "source": [
        "# 9. Preprocessing our annotations\n",
        "\n",
        "One of the most tricky parts of any machine learning problem is getting your data in the right format.\n",
        "\n",
        "We've done it for our images.\n",
        "\n",
        "Now let's do it for our annotations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXlivP0KKxWj"
      },
      "source": [
        "## 9.1 Trying to preprocess a single annotation\n",
        "\n",
        "Recall in a previous section we tried to preprocess a single image and its annotation.\n",
        "\n",
        "And we got an error.\n",
        "\n",
        "Let's make sure we're not crazy and this is still the case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLiSXd1vKxWk"
      },
      "outputs": [],
      "source": [
        "# Preprocess a single image and annotation pair\n",
        "# image_processor.preprocess(\n",
        "#     images=random_sample[\"image\"],\n",
        "#     annotations=random_sample[\"annotations\"]\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuohj-zlKxWk"
      },
      "source": [
        "We still get an error:\n",
        "\n",
        "> ValueError: Invalid COCO detection annotations. Annotations must a dict (single image) or list of dicts (batch of images) with the following keys: `image_id` and `annotations`, with the latter being a list of annotations in the COCO format.\n",
        "\n",
        "In this section, we're going to fix it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jo1h-ssQKxWl"
      },
      "source": [
        "## 9.2 Discussing the format our annotations need to be in\n",
        "\n",
        "According the error we got in the previous segment, the [`transformers.ConditionalDetrImageProcessor.preprocess`](https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrImageProcessor.preprocess) method expects input annotations in COCO format.\n",
        "\n",
        "In the documentation we can read that the `annotations` parameter taks in a list of dictionaries with the following keys:\n",
        "\n",
        "* `\"image_id\"` (`int`): The image id.\n",
        "* `\"annotations\"` (`List[Dict]`): List of annotations for an image. Each annotation should be a dictionary. An image can have no annotations, in which case the list should be empty.\n",
        "\n",
        "As for the `\"annotations\"` field, this should be a list of dictionaries containing individual annotations in [COCO format](https://cocodataset.org/#format-data):\n",
        "\n",
        "```python\n",
        "# COCO format, see: https://cocodataset.org/#format-data  \n",
        "[{\n",
        "    \"image_id\": 42,\n",
        "    \"annotations\": [{\n",
        "        \"id\": 123456,\n",
        "        \"category_id\": 1,\n",
        "        \"iscrowd\": 0,\n",
        "        \"segmentation\": [\n",
        "            [42.0, 55.6, ... 99.3, 102.3]\n",
        "        ],\n",
        "        \"image_id\": 42, # this matches the 'image_id' field above\n",
        "        \"area\": 135381.07,\n",
        "        \"bbox\": [523.70,\n",
        "                 545.09,\n",
        "                 402.79,\n",
        "                 336.11]\n",
        "    },\n",
        "    # Next annotation in the same format as the previous one (one annotation per dict).\n",
        "    # For example, if an image had 4 bounding boxes, there would be a list of 4 dictionaries\n",
        "    # each containing a single annotation.\n",
        "    ...]\n",
        "}]\n",
        "```\n",
        "\n",
        "Let's breakdown each of the fields in the COCO annotation:\n",
        "\n",
        "| Field | Requirement | Data Type | Description |\n",
        "|-----|-----|-----|-----|\n",
        "| `image_id` (top-level) | Required | Integer | ID of the target image. |\n",
        "| `annotations`| Required | List[Dict] | List of dictionaries with one box annotation per dict. Can be empty if there are no boxes. |\n",
        "| `id` | Not required | Integer | ID of the particular annotation. |\n",
        "| `category_id` | Required | Integer | ID of the class the box relates to (e.g. `{0: 'bin', 1: 'hand', 2: 'not_bin', 3: 'not_hand', 4: 'not_trash', 5: 'trash'}`). |\n",
        "| `segmentation` | Not required | List or None | Segmentation mask related to an annotation instance. Focus is on boxes, not segmentation. |\n",
        "| `image_id` (inside `annotations` field) | Required | Integer | ID of the target image the particular box relates to, should match `image_id` on the top-level field.   |\n",
        "| `area`  | Not required    | Float | Area of the target bounding box (e.g. box height * width). |\n",
        "| `bbox`  | Required  | List[Float] | Coordinates of the target bounding box in `XYWH` (`[x, y, width, height]`) format. `(x, y)` are the top left corner coordinates, `width` and `height` are dimensions. |\n",
        "| `is_crowd` | Not required | Int | Boolean flag (0 or 1) to indicate whether or not an object is multiple (a crowd) of the same thing. For example, a crowd of \"people\" or a group of \"apples\" rather than a single apple. |\n",
        "\n",
        ": COCO data format keys breakdown {#tbl-coco-format}\n",
        "\n",
        "And now our annotation data comes in the format:\n",
        "\n",
        "```python\n",
        "{'image': <PIL.Image.Image image mode=RGB size=960x1280>,\n",
        " 'image_id': 292,\n",
        " 'annotations': {'file_name': ['00347467-13f1-4cb9-94aa-4e4369457e0c.jpeg',\n",
        "   '00347467-13f1-4cb9-94aa-4e4369457e0c.jpeg'],\n",
        "  'image_id': [292, 292],\n",
        "  'category_id': [1, 0],\n",
        "  'bbox': [[523.7000122070312,\n",
        "    545.0999755859375,\n",
        "    402.79998779296875,\n",
        "    336.1000061035156],\n",
        "   [10.399999618530273,\n",
        "    163.6999969482422,\n",
        "    943.4000244140625,\n",
        "    1101.9000244140625]],\n",
        "  'iscrowd': [0, 0],\n",
        "  'area': [135381.078125, 1039532.4375]},\n",
        " 'label_source': 'manual_prodigy_label',\n",
        " 'image_source': 'manual_taken_photo'}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoypg295KxWm"
      },
      "source": [
        "## 9.3 Creating dataclasses to represent the COCO bounding box format\n",
        "\n",
        "Let's write some code to transform our existing annotation data into the format required by `transformers.ConditionalDetrImageProcessor.preprocess`.\n",
        "\n",
        "We'll start by creating two [Python dataclasses](https://docs.python.org/3/library/dataclasses.html#module-dataclasses) to house our desired COCO annotation format.\n",
        "\n",
        "To do this we'll:\n",
        "\n",
        "1. Create `SingleCOCOAnnotation` which contains the format structure of a single COCO annotation.\n",
        "2. Create `ImageCOCOAnnotations` which contains all of the annotations for a given image in COCO format. This may be a single instance of `SingleCOCOAnnotation` or multiple.\n",
        "\n",
        "We'll decorate both of these with the [`@dataclass`](https://docs.python.org/3/library/dataclasses.html#dataclasses.dataclass) decorator.\n",
        "\n",
        "Using a `@dataclass` gives several benefits:\n",
        "\n",
        "* Type hints - we can define the types of objects we want in the class definition, for example, we want `image_id` to be an `int`.\n",
        "* Helpful built-in methods - we can use methods such as [`asdict`](https://docs.python.org/3/library/dataclasses.html#dataclasses.asdict) to convert our `@dataclass` into a dictionary (COCO wants lists of dictionaries).\n",
        "* Data validation - we can use methods such as [`__post_init__`](https://docs.python.org/3/library/dataclasses.html#dataclasses.__post_init__) to run checks on our `@dataclass` as it's initialized, for example, we always want the length of `bbox` to be 4 (bounding box coordinates in `XYWH` format)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pv-K6p8CKxWn"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass, asdict\n",
        "from typing import List, Tuple\n",
        "\n",
        "# 1. Create a dataclass for a single COCO annotation\n",
        "@dataclass\n",
        "class SingleCOCOAnnotation:\n",
        "    \"\"\"An instance of a single COCO annotation.\n",
        "\n",
        "    Represent a COCO-formatted (see: https://cocodataset.org/#format-data) single instance of an object\n",
        "    in an image.\n",
        "\n",
        "    Attributes:\n",
        "        image_id: Unique integer identifier for the image which the annotation belongs to.\n",
        "        category_id: Integer identifier for the target object label/category (e.g. \"0\" for \"bin\").\n",
        "        bbox: List of floats containing target bounding box coordinates in absolute XYWH format ([x_top_left, y_top_left, width, height]).\n",
        "        area: Area of the target bounding box. Defaults to 0.0.\n",
        "        iscrowd: Boolean flag (0 or 1) indicating whether the target is a crowd of objects, for example, a group of\n",
        "            apples rather than a single apple. Defaults to 0.\n",
        "    \"\"\"\n",
        "    image_id: int\n",
        "    category_id: int\n",
        "    bbox: List[float] # bboxes in XYWH format ([x_top_left, y_top_left, width, height])\n",
        "    area: float = 0.0\n",
        "    iscrowd: int = 0\n",
        "\n",
        "    # Make sure the bbox is always a list of 4 values (XYWH format)\n",
        "    def __post_init__(self):\n",
        "        if len(self.bbox) != 4:\n",
        "            raise ValueError(f\"bbox must contain exactly 4 values, current length: {len(self.bbox)}\")\n",
        "\n",
        "\n",
        "# 2. Create a dataclass for a collection of COCO annotations for a single image\n",
        "@dataclass\n",
        "class ImageCOCOAnnotations:\n",
        "    \"\"\"A collection of COCO annotations for a single image_id.\n",
        "\n",
        "    Attributes:\n",
        "        image_id: Unique integer identifier for the image which the annotations belong to.\n",
        "        annotations: List of SingleCOCOAnnotation instances.\n",
        "    \"\"\"\n",
        "    image_id: int\n",
        "    annotations: List[SingleCOCOAnnotation]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcRRNR8vKxWo"
      },
      "outputs": [],
      "source": [
        "# One of the benefits of using a dataclass is that we can inspect the attributes with the `?` syntax\n",
        "SingleCOCOAnnotation?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXpr35IbKxWp"
      },
      "source": [
        "We can also see the error handling of our `__post_init__` method in action by trying to create an instance of `SingleCOCOAnnotation` with an incorrect number of bbox values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GL_Ae_-GKxWp"
      },
      "outputs": [],
      "source": [
        "# Let's try our SingleCOCOAnnotation dataclass (this will error since the bbox doesn't have 4 values)\n",
        "# SingleCOCOAnnotation(image_id=42,\n",
        "#                      category_id=0,\n",
        "#                      bbox=[100, 100, 100]) # missing a 4th value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDfVAeBFKxWq"
      },
      "source": [
        "And now if we pass the correct number of values to our `SingleCOCOAnnotation`, it should work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZL1_sSJKxWr"
      },
      "outputs": [],
      "source": [
        "SingleCOCOAnnotation(image_id=42,\n",
        "                     category_id=0,\n",
        "                     bbox=[100, 100, 100, 100]) # correct number of values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKP5qe8KKxWs"
      },
      "source": [
        "## 9.4 Creating a function to format our annotations as COCO format\n",
        "\n",
        "Now we've got the COCO data format in our `SingleCOCOAnnotation` and `ImageCOCOAnnotation` dataclasses, let's write a function to take our existing image annotations and format them in COCO style.\n",
        "\n",
        "Our `format_image_annotations_as_coco` function will:\n",
        "\n",
        "1. Take in an `image_id` to represent a unique identifier for the image as well as lists of category integers, area values and bounding box coordinates.\n",
        "2. Perform a list comprehension on a zipped version of each category, area and bounding box coordinate value in the input lists creating an instance of `SingleCOCOAnnotation` as a dictionary (using the `asdict` method) each time, this will give us a list of `SingleCOCOAnnotation` formatted dictionaries.\n",
        "3. Return a dictionary version of `ImageCOCOAnnotations` using `asdict` passing it the `image_id` as well as list of `SingleCOCOAnnotation` dictionaries from 2.\n",
        "\n",
        "Why does our function take in lists of categories, areas and bounding boxes?\n",
        "\n",
        "Because that's the current format our existing annotations are in (how we downloaded them from Hugging Face in the beginning).\n",
        "\n",
        "Let's do it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sk3CUPD7KxWt"
      },
      "outputs": [],
      "source": [
        "# 1. Take in a unique image_id as well as lists of categories, areas, and bounding boxes\n",
        "def format_image_annotations_as_coco(\n",
        "        image_id: int,\n",
        "        categories: List[int],\n",
        "        areas: List[float],\n",
        "        bboxes: List[Tuple[float, float, float, float]] # bboxes in XYWH format ([x_top_left, y_top_left, width, height])\n",
        ") -> dict:\n",
        "    \"\"\"Formats lists of image annotations into COCO format.\n",
        "\n",
        "    Takes in parallel lists of categories, areas, and bounding boxes and\n",
        "    then formats them into a COCO-style dictionary of annotations.\n",
        "\n",
        "    Args:\n",
        "        image_id: Unique integer identifier for an image.\n",
        "        categories: List of integer category IDs for each annotation.\n",
        "        areas: List of float areas for each annotation.\n",
        "        bboxes: List of tuples containing bounding box coordinates in XYWH format\n",
        "            ([x_top_left, y_top_left, width, height]).\n",
        "\n",
        "    Returns:\n",
        "        A dictionary of image annotations in COCO format with the following structure:\n",
        "        {\n",
        "            \"image_id\": int,\n",
        "            \"annotations\": [\n",
        "                {\n",
        "                    \"image_id\": int,\n",
        "                    \"category_id\": int,\n",
        "                    \"bbox\": List[float],\n",
        "                    \"area\": float\n",
        "                },\n",
        "                ...more annotations here\n",
        "            ]\n",
        "        }\n",
        "\n",
        "    Note:\n",
        "        All input lists much be the same length and in the same order.\n",
        "        Otherwise, there will be mismatched annotations.\n",
        "    \"\"\"\n",
        "\n",
        "    # 2. Turn input lists into a list of dicts in SingleCOCOAnnotation format\n",
        "    coco_format_annotations = [\n",
        "        asdict(SingleCOCOAnnotation(\n",
        "            image_id=image_id,\n",
        "            category_id=category,\n",
        "            bbox=list(bbox),\n",
        "            area=area,\n",
        "        ))\n",
        "        for category, area, bbox in zip(categories, areas, bboxes)\n",
        "    ]\n",
        "\n",
        "    # 3. Return a of annotations with format {\"image_id\": ..., \"annotations\": [...]} (required COCO format)\n",
        "    return asdict(ImageCOCOAnnotations(image_id=image_id,\n",
        "                                       annotations=coco_format_annotations))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6j6DKVyKxWu"
      },
      "outputs": [],
      "source": [
        "# Create a not so random sample and inspect it\n",
        "random_sample = dataset[\"train\"][77]\n",
        "random_sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjputxAqKxWw"
      },
      "outputs": [],
      "source": [
        "# Extract image_id, categories, areas, and bboxes from the random sample\n",
        "random_sample_image_id = random_sample[\"image_id\"]\n",
        "random_sample_categories = random_sample[\"annotations\"][\"category_id\"]\n",
        "random_sample_areas = random_sample[\"annotations\"][\"area\"]\n",
        "random_sample_bboxes = random_sample[\"annotations\"][\"bbox\"]\n",
        "\n",
        "# Format the random sample annotations as COCO format\n",
        "random_sample_coco_annotations = format_image_annotations_as_coco(image_id=random_sample_image_id,\n",
        "                                                                  categories=random_sample_categories,\n",
        "                                                                  areas=random_sample_areas,\n",
        "                                                                  bboxes=random_sample_bboxes)\n",
        "random_sample_coco_annotations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yclNSU8iKxWx"
      },
      "source": [
        "Woohoo!\n",
        "\n",
        "Looks like we may have just fixed our `ValueError` from before:\n",
        "\n",
        "> ValueError: Invalid COCO detection annotations. Annotations must a dict (single image) or list of dicts (batch of images) with the following keys: `image_id` and `annotations`, with the latter being a list of annotations in the COCO format.\n",
        "\n",
        "Our COCO formatted annotations have the `image_id` and `annotations` keys and our `annotations` are a list of annotations in COCO format.\n",
        "\n",
        "Perfect!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ds0XGSZsKxWy"
      },
      "source": [
        "## 9.5 Preprocess a single image and set of COCO format annotations\n",
        "\n",
        "Now we've preprocessed our annotations to be in COCO format, we can use them with [`transformers.ConditionalDetrImageProcessor.preprocess`](https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrImageProcessor.preprocess).\n",
        "\n",
        "Let's pass our `random_sample` image and COCO formatted annotations to the `preprocess` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQE4HgWWKxWz"
      },
      "outputs": [],
      "source": [
        "# Preprocess random sample image and assosciated annotations\n",
        "random_sample_preprocessed = image_processor.preprocess(images=random_sample[\"image\"],\n",
        "                                                        annotations=random_sample_coco_annotations,\n",
        "                                                        do_convert_annotations=True, # defaults to True, this will convert our annotations to normalized CXCYWH format\n",
        "                                                        return_tensors=\"pt\" # can return as tensors or not, \"pt\" returns as PyTorch tensors\n",
        "                                                        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvCHfzj7KxW0"
      },
      "outputs": [],
      "source": [
        "# Optional: Disable warnings about `max_size` parameter being deprecated\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", message=\"The `max_size` parameter is deprecated*\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wieOFmkYKxW2"
      },
      "outputs": [],
      "source": [
        "# Check the keys of our preprocessed example\n",
        "random_sample_preprocessed.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zg_r03VEKxW3"
      },
      "source": [
        "Wonderful, we get a preprocessed image and labels:\n",
        "\n",
        "* `pixel_values` = preprocessed pixels (the preprocessed image).\n",
        "* `pixel_mask` = whether or not to mask the pixels (e.g. 0 = mask, 1 = no mask, in our case, all values will be `1` since we want the model to see all pixels).\n",
        "* `labels` = preprocessed labels (the preprocessed annotations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfWTxi_YKxW3"
      },
      "outputs": [],
      "source": [
        "# Inspect preprocessed image shape\n",
        "print(f\"[INFO] Preprocessed image shape: {random_sample_preprocessed['pixel_values'].shape} -> [batch_size, colour_channels, height, width]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDq3vHq3KxW5"
      },
      "outputs": [],
      "source": [
        "# Inspect the preprocessed labels (our boxes and other metadata)\n",
        "pprint(random_sample_preprocessed[\"labels\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbRPfRvRKxW6"
      },
      "source": [
        "Let's break this down:\n",
        "\n",
        "* `area` - An array/tensor of floats containing the area (`box_width * box_height`) of our boxes.\n",
        "* `boxes` - An array/tensor containing all of the bounding boxes for our image in normalized `CXCYWH` (`(center_x, center_y, width, height)`) format.\n",
        "* `class_labels` - An array/tensor of integer labels assosciated with each box (e.g. `tensor([5, 1, 0, 0, 4])` -> `['trash', 'hand', 'bin', 'bin', 'not_trash']`).\n",
        "* `image_id` - A unique integer identifier for our target image.\n",
        "* `is_crowd` - An array/tensor of a boolean value (0 or 1) for whether an annotation is a group or not.\n",
        "* `orig_size` - An array/tensor containing the original size in `(height, width)` format (this is important for drawing conversion factors when using originally sized images).\n",
        "* `size` - An array/tensor with the current size in `(height, width)` format of the processed image tensor contained within `random_sample_preprocessed[\"pixel_values\"]`.\n",
        "\n",
        "Woohoo!\n",
        "\n",
        "We've done it!\n",
        "\n",
        "We've officially preprocessed a single sample of our own data, both the image and its annotation pair.\n",
        "\n",
        "We'll write some code later on to scale this up to our whole dataset.\n",
        "\n",
        "For now, let's see what it looks like postprocessing a single output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RVFuL9FKxW7"
      },
      "source": [
        "# 10. Postprocessing a single output\n",
        "\n",
        "We've got our inputs processed and successfully passed them through our model.\n",
        "\n",
        "How about we postprocess the outputs of our model?\n",
        "\n",
        "Doing so will make our model's outputs far more usable.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11. Going end-to-end on a single sample\n",
        "\n",
        "When working on a new problem or with a custom dataset and an existing model, it's good practice to go end-to-end on a single sample.\n",
        "\n",
        "For example, preprocess one of your samples, pass it through the model and then postprocess it (just like we're in the middle of doing here).\n",
        "\n",
        "Being able to go end-to-end on a single sample will help you see the overall process and discover any bugs that may hinder you later on.\n",
        "\n",
        "To postprocess the outputs of our model we can use the [`transformers.ConditionalDetrImageProcessor.post_process_object_detection()`](https://huggingface.co/docs/transformers/en/model_doc/conditional_detr#transformers.ConditionalDetrImageProcessor.post_process_object_detection) method.\n",
        "\n",
        "Let's frist recompute the model's outputs for our preprocessed single sample."
      ],
      "metadata": {
        "id": "Rb-vGN99PNIy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NITCvTzKxW8"
      },
      "outputs": [],
      "source": [
        "# Recompute the random sample outputs with our preprocessed sample\n",
        "random_sample_outputs = model(\n",
        "    pixel_values=random_sample_preprocessed[\"pixel_values\"], # model expects input [batch_size, color_channels, height, width]\n",
        "    # pixel_mask=random_sample_preprocessed[\"pixel_mask\"],\n",
        ")\n",
        "\n",
        "# Inspect the output type\n",
        "type(random_sample_outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcVgVHgPKxW8"
      },
      "source": [
        "Wonderful!\n",
        "\n",
        "We get the exact output our `post_process_object_detection()` method is looking for.\n",
        "\n",
        "Now we can fill in the following parameters:\n",
        "\n",
        "* `outputs` - Raw outputs of the model (for us, this is `random_sample_outputs`).\n",
        "* `threshold` - A float score value to keep or discard boxes (e.g. `threshold=0.3` means all boxes under `0.3` will be discarded). This value can be adjusted as needed. A higher value means only the boxes the model is most confident on will be kept. A lower value means more boxes will be kept, however, these may be over lower quality. Best to be experimented with.\n",
        "* `target_sizes` - Size of target image in `(height, width)` format for bounding boxes. For example, if our image is 960 pixels wide by 1280 high, we could pass in `[1280, 960]`. Number of `target_sizes` must match number of `outputs`. For example, if pass in 1 set of outputs, only 1 `target_sizes` is needed. If we pass in a batch of 32 `outputs`, 32 `target_sizes` are required, else it will error. If `None`, postprocessed outputs won't be resized (this can be lead to poor looking boxes as the coordinates don't match your image).\n",
        "* `top_k` - Integer defining the number of boxes you'd like to prepare for postprocessing before thresholding. Defaults to `100`. For example, `top_k=100` and `threshold=0.3` means sample 100 boxes and then of those 100 boxes, only keep those with a score over 0.3.\n",
        "\n",
        "You can see what happens behind the scenes of `post_process_object_detection` in the [source code](https://github.com/huggingface/transformers/blob/a22a4378d97d06b7a1d9abad6e0086d30fdea199/src/transformers/models/conditional_detr/image_processing_conditional_detr.py#L1574).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eB7_XGnzKxW9"
      },
      "outputs": [],
      "source": [
        "# Set the score threshold for postprocessing\n",
        "THRESHOLD = 0.4 # adjust this where necessary to get a handful of outputs below (note: if it's too high, e.g. 0.5+, you might not see any outputs, try lowering to 0.3\n",
        "\n",
        "# Post process a single output from our model\n",
        "random_sample_outputs_post_processed = image_processor.post_process_object_detection(\n",
        "    outputs=random_sample_outputs,\n",
        "    threshold=THRESHOLD, # all boxes with scores under this value will be discarded (best to experiment with it)\n",
        "    target_sizes=random_sample_preprocessed[\"labels\"][0][\"orig_size\"].unsqueeze(0) # original input image size (or whichever target size you'd like), required to be same number of input items in a list\n",
        ")\n",
        "\n",
        "random_sample_outputs_post_processed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36UILRYWKxW_"
      },
      "source": [
        "Perfect!\n",
        "\n",
        "This looks like something we can use.\n",
        "\n",
        "Let's break down each of the keys in `random_sample_outputs_post_processed`.\n",
        "\n",
        "We get three equal length tensors:\n",
        "\n",
        "* `scores` - The prediction probabilities for each box, higher means the model is more confident in this prediction (though it doesn't mean the prediction is correct). Notice how all the values in this tensor are over our `threshold` value. This value is acquired by applying [`torch.sigmoid()`](https://pytorch.org/docs/main/generated/torch.sigmoid.html) to the models raw output logits.\n",
        "* `labels` - The predicted classification label values for each box. These will be random as our model hasn't been trained for our dataset. We can turn these into class names by mapping them to the `id2label` dictionary.\n",
        "* `boxes` - The predicted bounding boxes whose scores are above the `threshold` parameter. These are normalized and in the format `XYXY` or `(x_top_left, y_top_left, x_bottom_right, y_bottom_right)`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Jo1KyyZKxW_"
      },
      "source": [
        "## 11.1 Reproducing our postprocessed box scores by hand\n",
        "\n",
        "When a raw prediction output from our model goes through the `post_process_object_detection` method, a few steps happen.\n",
        "\n",
        "One of them is that the raw logits from our model get converted into prediction probabilities.\n",
        "\n",
        "This happens by:\n",
        "\n",
        "1. Applying the [`torch.sigmoid()`](https://pytorch.org/docs/main/generated/torch.sigmoid.html) function to the logits to turn them into prediction probabilities.\n",
        "2. Finding the max value for each prediction using [`torch.max()`](https://pytorch.org/docs/main/generated/torch.max.html) (e.g. the class index with the highest prediction probability).\n",
        "3. Removing the predictions with prediction probabilities which are not above the `threshold`.\n",
        "4. Sorting the top 100 (`post_process_object_detection` returns the top 100 values by default) predictions in descending order using [`torch.topk()`](https://pytorch.org/docs/main/generated/torch.topk.html) followed by [`torch.sort()`](https://pytorch.org/docs/stable/generated/torch.sort.html) (so the predictions with the highest prediction probability come first).\n",
        "\n",
        "`torch.max()` and `torch.sort()` will return both raw tensor values and the indicies of where they occur in a tuple `(values, indicies)`.\n",
        "\n",
        "These index values are the predicted label ID.\n",
        "\n",
        "To see this happen, let's reproduce the `\"scores\"` key in `random_sample_outputs_post_processed` by hand."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSUyr7DJKxXA"
      },
      "outputs": [],
      "source": [
        "# Get the output scores from our post processed single output\n",
        "output_scores = random_sample_outputs_post_processed[0][\"scores\"]\n",
        "len(output_scores), output_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHJb4sTc_3QI"
      },
      "outputs": [],
      "source": [
        "print(f\"[INFO] Original input logits shape: {random_sample_outputs.logits.shape}\\n\")\n",
        "\n",
        "# 1. Perform sigmoid on the logits to get prediction probabilities and take the max value for each prediction\n",
        "output_scores_manual = random_sample_outputs.logits.sigmoid().flatten(start_dim=1)\n",
        "print(f\"[INFO] Manual output scores shape: {output_scores_manual.shape}\")\n",
        "print(f\"[INFO] First 10 scores (these will be in random order):\\n{output_scores_manual[0][:10].detach().cpu()}\\n\")\n",
        "\n",
        "# 2. Get the top 100 scores (this is the default setting in post_process_object_detection)\n",
        "output_scores_manual_top_100, output_scores_manual_top_100_indices = torch.topk(input=output_scores_manual,\n",
        "                                                                                k=100,\n",
        "                                                                                dim=-1)\n",
        "print(f\"[INFO] Top 100 scores shape: {output_scores_manual_top_100.shape}\")\n",
        "print(f\"[INFO] First top 100 score:\\n{output_scores_manual_top_100[0][0].item():.4f}\\n\")\n",
        "\n",
        "# 3. Find the values above the threshold and create a mask\n",
        "output_scores_manual_mask = output_scores_manual_top_100 > THRESHOLD\n",
        "\n",
        "# 4. Sort the top 100 scores which are above the threshold and sort them in descending order and get the indices\n",
        "output_scores_manual_filtered, output_scores_manual_filtered_indices = torch.sort(input=output_scores_manual_top_100[output_scores_manual_mask],\n",
        "                                                                                  descending=True)\n",
        "\n",
        "print(f\"[INFO] Filtered scores shape: {output_scores_manual_filtered.shape}\")\n",
        "print(f\"[INFO] First filtered scores:\\n{output_scores_manual_filtered[0].detach().cpu():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdK-_6xNKxXF"
      },
      "outputs": [],
      "source": [
        "# Compare the original output scores to our own manual version\n",
        "torch.isclose(input=output_scores[:len(output_scores_manual_filtered)],\n",
        "              other=output_scores_manual_filtered,\n",
        "              atol=1e-2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tT4JRitKxXH"
      },
      "source": [
        "Nice!\n",
        "\n",
        "We managed to reproduce our postprocessed output scores values by hand.\n",
        "\n",
        "How about the labels?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7V--Vs8KxXI"
      },
      "source": [
        "## 11.2 Reproducing our postprocessed box labels by hand\n",
        "\n",
        "We've reproduce our postprocessed model prediction scores by hand.\n",
        "\n",
        "Now let's do the same with the labels.\n",
        "\n",
        "First, we'll get the output labels from our postprocessed object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvpfxA_jKxXJ"
      },
      "outputs": [],
      "source": [
        "# Get the model's predicted labels\n",
        "output_labels = random_sample_outputs_post_processed[0][\"labels\"]\n",
        "print(f\"[INFO] Output labels shape: {len(output_labels)}\")\n",
        "print(f\"[INFO] Output labels:\\n{output_labels}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjt_3zLIKxXJ"
      },
      "source": [
        "Wonderful!\n",
        "\n",
        "Now to reproduce these values, we can:\n",
        "\n",
        "1. Filter the `output_scores_manual_max_indices` for the top 100 indices which made it past the `threshold` using `output_scores_manual_mask`.\n",
        "2. Order the remaining label values from 1 in descending order according to `output_scores_manual_filtered_indices`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLNIg-abKxXK"
      },
      "outputs": [],
      "source": [
        "# 1. Get the number of classes\n",
        "num_classes = random_sample_outputs.logits.shape[2]\n",
        "print(f\"[INFO] Found total number of classes: {num_classes}\")\n",
        "\n",
        "# 2. Modulo the output_scores_manual_top_100_indices by the number of classes to get the predicted class (this is because we flattened our outputs above with .flatten(1))\n",
        "output_labels_manual = output_scores_manual_top_100_indices % num_classes\n",
        "\n",
        "# 3. Find the top labels which pass our score threshold\n",
        "output_labels_manual_filtered = output_labels_manual[0][output_scores_manual_filtered_indices]\n",
        "\n",
        "output_labels.shape, output_labels_manual_filtered.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iy8yaYWjKxXL"
      },
      "outputs": [],
      "source": [
        "output_labels[:len(output_labels_manual_filtered)] == output_labels_manual_filtered"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0vcXEwLKxXO"
      },
      "source": [
        "Perfect!\n",
        "\n",
        "How about we repeat the same for our model's postprocessed predicted boxes?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUWIQnGrKxXP"
      },
      "source": [
        "## 11.3 Reproducing our postprocessed box coordinates by hand\n",
        "\n",
        "We've reproduce our postprocessed model prediction scores by hand.\n",
        "\n",
        "Now let's do the same with the labels.\n",
        "\n",
        "First, we'll get the output labels from our postprocessed object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9hPsiK7KxXQ"
      },
      "outputs": [],
      "source": [
        "# These are in absolute XYXY (x_top_left, y_top_left, x_bottom_right, y_bottom_right) format\n",
        "output_boxes = random_sample_outputs_post_processed[0][\"boxes\"]\n",
        "print(f\"[INFO] Output boxes shape: {output_boxes.shape}\")\n",
        "print(f\"[INFO] Output boxes (absolute XYXY format), first 10:\\n{output_boxes[:10]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are the boxes we'd like to reproduce.\n",
        "\n",
        "Let's now get the raw predicted box coordinates from our model."
      ],
      "metadata": {
        "id": "L4LoMpynWtfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get model output raw boxes\n",
        "# These are in format: normalized CXCYWH (center_x, center_y, width, height) format\n",
        "output_boxes_manual_cxcywh = random_sample_outputs.pred_boxes[0]\n",
        "print(f\"[INFO] Output boxes manual shape: {output_boxes_manual_cxcywh.shape}\")\n",
        "print(f\"[INFO] Output boxes manual (normalized CXCYWH format), first 10:\\n{output_boxes_manual_cxcywh[:10]}\")"
      ],
      "metadata": {
        "id": "KCptxUWWWQRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qmrHZD0KxXU"
      },
      "source": [
        "Ok, so we've got 300 boxes here, let's filter these down to only the boxes which have a score above the target `threshold` using our `output_scores_manual_mask` tensor.\n",
        "\n",
        "If we want to go from raw boxes out of the model to the same format as our postprocessed boxes or from normalized `CXCYWH` to absolute `XYXY`, we'll have to:\n",
        "\n",
        "1. Normalize the `output_scores_manual_top_100_indices` by dividing them by the number of classes, for example, `top_100_index // num_classes = top_box_index` -> `763 // 7 = 763`.\n",
        "2. Filter for the top 100 scoring boxes coordinates which make it over the `threshold` using the `output_scores_manual_mask`.\n",
        "3. Convert the filtered box coordinates from normalized `CXCYWH` to normalized `XYXY` using [`torchvision.ops.box_convert`](https://pytorch.org/vision/main/generated/torchvision.ops.box_convert.html).\n",
        "4. Get the original input image size (required for box conversion).\n",
        "5. Convert the normalized `XYXY` coordinates to absolute `XYXY` coordinates by multiplying the `x` coordinates by the desired width and the `y` coordinates by the desired height. For example, if we want to plot our boxes on the original image, we'd use the original image dimensions of `(1280, 960)` (height, width).\n",
        "6. Sort the bounding box coordinates in the same order as the scores (descending).\n",
        "8. Check for equivalence between original postprocessed boxes and our manually processed boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s52QakoqKxXU"
      },
      "outputs": [],
      "source": [
        "# 1. Normalize the indices by dividing by the number of classes (this is because we flattened our logits tensor in a previous step)\n",
        "output_scores_manual_top_100_indicies_normalized = output_scores_manual_top_100_indices[0] // num_classes\n",
        "output_scores_manual_top_100_indicies_normalized"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Filter boxes for top 100 above the target threshold\n",
        "output_boxes_manual_above_threshold_cxcywh = output_boxes_manual_cxcywh[output_scores_manual_top_100_indicies_normalized]\n",
        "\n",
        "print(f\"[INFO] Output boxes manual above threshold shape: {output_boxes_manual_above_threshold_cxcywh.shape}\")\n",
        "print(f\"[INFO] Output boxes manual above threshold (normalized CXCYWH format), \\\n",
        "showing first 10:\\n{output_boxes_manual_above_threshold_cxcywh[:10, :]}\")"
      ],
      "metadata": {
        "id": "R9TwJbI7XFEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2N6EbMnKxXW"
      },
      "source": [
        "Nice!\n",
        "\n",
        "Okay, now let's convert the boxes from normalized `CXCYWH` to normalized `XYXY` format using `torchvision.ops.box_convert`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyvaZU2-KxXW"
      },
      "outputs": [],
      "source": [
        "from torchvision.ops import box_convert\n",
        "\n",
        "# 3. Convert the model's predicted boxes from CXCYWH to XYXY format\n",
        "output_boxes_manual_above_threshold_xyxy = box_convert(boxes=output_boxes_manual_above_threshold_cxcywh,\n",
        "                                                       in_fmt=\"cxcywh\",\n",
        "                                                       out_fmt=\"xyxy\")\n",
        "print(f\"[INFO] Output boxes manual above threshold (absolute XYXY format):\\n{output_boxes_manual_above_threshold_xyxy[:10]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nw_QaLoRKxXX"
      },
      "source": [
        "Excellent, we've got our box coordinates in normalized `XYXY` format.\n",
        "\n",
        "We could keep them here.\n",
        "\n",
        "But to fully replicate the outputs of our postprocessed boxes, we'll convert them to absolute format.\n",
        "\n",
        "Absolute format conversion will depend on the target size of image we'd like to use.\n",
        "\n",
        "For example, if we'd like to convert our boxes to the original dimensions of our input image so we can plot them on that image, we can use the image's original dimensions.\n",
        "\n",
        "To get the original dimensions of our image we can access the `orig_size` attribute of our preprocessed sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVnTAKmkKxXY"
      },
      "outputs": [],
      "source": [
        "# 4. Get the original input image size (required for box conversion)\n",
        "random_sample_image_original_size= random_sample_preprocessed[\"labels\"][0][\"orig_size\"]\n",
        "print(f\"[INFO] Image original size: {random_sample_image_original_size} (height, width)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQNhZczfKxXZ"
      },
      "source": [
        "Now to convert our normalized coordinates to absolute coordinates we can multiply `x` coordinates by the target width and `y` coordinates by the target height."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOpYZEtcKxXZ"
      },
      "outputs": [],
      "source": [
        "# 5. Convert normalized box coordinates to absolute pixel values\n",
        "\n",
        "# Get image original height and width\n",
        "original_height, original_width = random_sample_image_original_size\n",
        "\n",
        "# Create an XYXY tensor to multiply by\n",
        "original_dimensions = torch.tensor([original_width,   # x1\n",
        "                                    original_height,  # y1\n",
        "                                    original_width,   # x2\n",
        "                                    original_height]) # y2\n",
        "\n",
        "# Convert the boxes to absolute pixel values\n",
        "output_boxes_manual_above_threshold_xyxy_absolute = output_boxes_manual_above_threshold_xyxy * original_dimensions\n",
        "output_boxes_manual_above_threshold_xyxy_absolute[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7oUf6_LKxXa"
      },
      "source": [
        "Absolute `XYXY` coordinates acquired!\n",
        "\n",
        "Time to order them in the same order as our descending scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JT4F6Y6zKxXb"
      },
      "outputs": [],
      "source": [
        "# 6. Order boxes in same order as labels and scores (descending based on score)\n",
        "output_boxes_manual_sorted = output_boxes_manual_above_threshold_xyxy_absolute[output_scores_manual_filtered_indices]\n",
        "output_boxes_manual_sorted[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvRI8JvPKxXc"
      },
      "source": [
        "Finally, we can check to see if our manually postprocessed boxes are equivalent to original post processed boxes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n64Qo53kKxXd"
      },
      "outputs": [],
      "source": [
        "# 7. Check for equivalence between original postprocessed boxes and our manually processed boxes\n",
        "torch.all(input=output_boxes[:100] == output_boxes_manual_sorted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1bQIRFTKxXd"
      },
      "source": [
        "Excellent!\n",
        "\n",
        "We've now successfully converted our model's raw outputs to postprocessed usable outputs.\n",
        "\n",
        "Taking the time to do steps like this helps us understand the steps taken behind the scenes for in-built postprocessing methods.\n",
        "\n",
        "Knowing how to do these conversion steps can also help use troubleshoot errors we may come across in the future."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QW--4PODKxXe"
      },
      "source": [
        "## 11.4 Plotting our model's first box predictions on an image\n",
        "\n",
        "We've got some predictions, time to follow the data explorer's motto and *visualize, visualize, visualize!*\n",
        "\n",
        "To do so we'll:\n",
        "\n",
        "1. Extract the scores, labels and boxes from our `random_sample_outputs_post_processed`.\n",
        "2. Create a list of label names to plot by mapping label IDs to class names as well as a list of colours to colour our boxes with in accordance to our `colour_palette`.\n",
        "3. Draw boxes on the image with a combination of `torchvision`'s [`pil_to_tensor`](https://pytorch.org/vision/main/generated/torchvision.transforms.functional.pil_to_tensor.html), [`draw_bounding_boxes`](https://pytorch.org/vision/main/generated/torchvision.utils.draw_bounding_boxes.html) and [`to_pil_image`](https://pytorch.org/vision/main/generated/torchvision.transforms.functional.to_pil_image.html).\n",
        "\n",
        "We'll halve the image as well as the box coordinates using `half_image` and `half_boxes` to save space in our notebook (this is not 100% necessary, just for convenience)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S08wrThlKxXf"
      },
      "outputs": [],
      "source": [
        "# 1. Extract scores, labels and boxes\n",
        "random_sample_pred_scores = random_sample_outputs_post_processed[0][\"scores\"]\n",
        "random_sample_pred_labels = random_sample_outputs_post_processed[0][\"labels\"]\n",
        "random_sample_pred_boxes = half_boxes(random_sample_outputs_post_processed[0][\"boxes\"])\n",
        "\n",
        "# 2. Create a list of labels and colours to plot on the image/boxes\n",
        "random_sample_pred_labels_to_plot = [f\"Pred: {id2label[label_pred.item()]} ({round(score_pred.item(), 4)})\"\n",
        "                  for label_pred, score_pred in zip(random_sample_pred_labels, random_sample_pred_scores)]\n",
        "random_sample_pred_colours = [colour_palette[id2label[label_pred.item()]] for label_pred in random_sample_pred_labels]\n",
        "\n",
        "print(f\"[INFO] Labels with scores: {random_sample_pred_labels_to_plot[:3]}...\")\n",
        "\n",
        "# 3. Plot the random sample image with randomly predicted boxes\n",
        "# (these will be very poor since the model is not trained on our data yet)\n",
        "to_pil_image(\n",
        "    pic=draw_bounding_boxes(\n",
        "        image=pil_to_tensor(pic=half_image(random_sample[\"image\"])),\n",
        "        boxes=random_sample_pred_boxes, # boxes are in XYXY format, which is required for draw_bounding_boxes\n",
        "        labels=random_sample_pred_labels_to_plot,\n",
        "        colors=random_sample_pred_colours,\n",
        "        width=3\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCz9Nl47KxXg"
      },
      "source": [
        "Woah! Those boxes don't look good at all both the label and the coordinates look off.\n",
        "\n",
        "This should be expected though...\n",
        "\n",
        "While our model has been pretrained on the COCO dataset, it hasn't been trained on our specific data.\n",
        "\n",
        "The good news is we can hopefully (there are no guarantees in machine learning) improve our model's box predictions by fine-tuning it on our dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DS-iNaMKxXh"
      },
      "source": [
        "# 12. Aside: Bounding box formats in and out of our model\n",
        "\n",
        "We've done a fair bit of data transformation to get our data ready to go into our model and we've also taken a fair few steps to postprocess it into a usable format.\n",
        "\n",
        "This is often a standard practice in many machine learning workflows.\n",
        "\n",
        "Much of the work before ever training a model is preparing the data for the model.\n",
        "\n",
        "And much of the work after training a model is preparing the data for your use case.\n",
        "\n",
        "The following table highlights the different states our bounding boxes go in and out of.\n",
        "\n",
        "TK image - turn this into a nice image of the workflow\n",
        "\n",
        "| **Step** | **Box format** | **Scale** | **Goes into** |\n",
        "| ----- | ----- | ----- | ----- |\n",
        "| Starting data (default downloaded from Hugging Face) | `XYWH` or `[x1, y1, width, height]` | Absolute | `preprocess()` method |\n",
        "| Out of [`preprocess()`](https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrImageProcessor.preprocess) | `CXCYWH` or `[center_x, center_y, width, height]` | Normalized | `model.forward()` |\n",
        "| Out of [`model.forward()`](https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrForObjectDetection.forward) | `CXCYWH` or `[center_x, center_y, width, height]` | Normalized | `post_process_object_detection()`|\n",
        "| Out of [`post_process_object_detection()`](https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrImageProcessor.post_process_object_detection) | `XYXY` or `[x_top_left, y_top_left, x_bottom_right, y_bottom_right]` | Absolute (in relation to the `target_sizes` parameter). | Plotting or display function. |\n",
        "\n",
        "\n",
        "<figure style=\"text-align: center;\">\n",
        "    <img src=\"https://huggingface.co/datasets/mrdbourke/learn-hf-images/resolve/main/learn-hf-trashify-object-detection/08-bounding-box-formats.png\"\n",
        "     alt=\"A diagram illustrating a four-stage object detection pipeline: 'Original Data' shows an image of a hand, trash, and bin with bounding boxes, having 'Format: XYWH' and 'Scale: Absolute'; an arrow points to the '.preprocess()' stage, represented by a green 'RTDetrImageProcessor' block, with 'Format: CXCYWH' and 'Scale: Normalized'; another arrow leads to the '.forward()' stage, depicted by a neural network icon labeled 'RTDetrV2ForObjectDetection', with 'Format: CXCYWH' and 'Scale: Normalized'; a final arrow points to the '.post_process_object_detection()' stage, another green 'RTDetrImageProcessor' block, with 'Format: XYXY' and 'Scale: Absolute'.\"\n",
        "     style=\"width: 100%; max-width: 900px; height: auto;\"/>\n",
        "     <figcaption>Our bounding boxes go through a series of format changes from input to final output. Keeping track of what format our boduning boxes are in is important for both training models and visualizing boxes on images. If we use the wrong format for plotting boxes on images, we may falsely assume our model is performing better or worse than it actually is.</figcaption>\n",
        "</figure>\n",
        "\n",
        "Keeping track of these input and output formats is helpful for knowing the state of your data.\n",
        "\n",
        "But remember, just because our current workflow is like this, doesn't mean all future workflows you work on will have the same transformation steps.\n",
        "\n",
        "For more on different bounding box formats, see the [bounding box formats guide](https://www.learnml.io/posts/a-guide-to-bounding-box-formats/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTSw4jkrKxXh"
      },
      "source": [
        "# 13. Preparing data at scale\n",
        "\n",
        "We've performed preprocessing and postprocessing steps on a single data sample.\n",
        "\n",
        "However, in practice, we'll likely want to work with many more samples.\n",
        "\n",
        "Our model is hungry for more data.\n",
        "\n",
        "So let's step it up a notch and write some code that's capable of preprocessing *many* samples to pass to our model.\n",
        "\n",
        "We'll break it down into three subsections:\n",
        "\n",
        "1. **Splitting the data** into training, validation and test sets. We'll train our model on the training set and check its performance on the validation and test sets (our model won't see any of these samples during training). We perform these splits before preprocessing the samples in them in case we'd like to perform different preprocessing steps depending on the split. For example, we may want to use [**data augmentation**](https://pytorch.org/vision/main/transforms.html) on the training set and not use it on the testing set.\n",
        "2. **Preprocessing multiple samples at a time** by iterating over groups of samples. Rather than preprocess a single sample at a time, we'll write code capable of processing lists of examples simultaneously.\n",
        "3. **Collate samples into batches** so our model can view multiple samples simultaneously. Rather than performing a forward pass on a single sample at a time, we'll pass **batches** of data to the model. For example, we may pass 32 samples (image and label pairs) at a time to our model for it to try and learn the patterns between them. We use batches of data rather than the whole dataset as it's often much more memory efficient. If you have a *really* large dataset, all of your samples may not fit into memory at once, so in practice, you break it up into smaller batches of samples.\n",
        "\n",
        "Let's start by splitting the data into different sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFhXVWcLKxXi"
      },
      "source": [
        "## 13.1 Splitting the data into training and test sets\n",
        "\n",
        "Right now our data is all in one big group.\n",
        "\n",
        "However, it's best practice to split our data into two (or three) different sets:\n",
        "\n",
        "1. **Training set (~70-80% of data)** - This is the data the model will learn from, all samples in this set are seen by the model during training.\n",
        "2. **Validation set (~5-20% of data)** - This is the data we can fine-tune our model's hyperparameters on, all samples in this set are *not* seen by the model during training.\n",
        "3. **Test set (~5-20% of data)** - This is the data we will evaluate what our model has learned after going through the training set, all samples in this set are *not* seen by the model during training.\n",
        "\n",
        "Using the analogy of a student at univeristy, the **training set** would be the course materials throughout the semester, the **validation set** would be the practice exam and the **test set** would be the final exam.\n",
        "\n",
        "If a student doesn't perform well on the final exam, then we would usually say perhaps the course materials weren't of the highest quality.\n",
        "\n",
        "This is similar to our machine learning workflow.\n",
        "\n",
        "In an ideal world, the samples in the training set are sufficiently representative of those in the test set and in turn, sufficiently representative of samples in the wild.\n",
        "\n",
        "Before we split our dataset into different sets, let's remind ourselves of what it looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqm_9HRjKxXl"
      },
      "outputs": [],
      "source": [
        "# Original dataset (only a \"train\" split)\n",
        "dataset = load_dataset(path=\"mrdbourke/trashify_manual_labelled_images\")\n",
        "original_dataset_length = len(dataset[\"train\"])\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ue4uxDCLKxXm"
      },
      "source": [
        "Wonderful! Right now, we've only got one split, `\"train\"`.\n",
        "\n",
        "To make our required splits, we can call the [`train_test_split()`](https://huggingface.co/docs/datasets/en/package_reference/main_classes#datasets.Dataset.train_test_split) method on our dataset and pass in the size of the split we'd like via the `test_size` parameter.\n",
        "\n",
        "For example, `test_size=0.3` means 30% of the data will go to the test set and 70% will go to the training set.\n",
        "\n",
        "We'll make the following splits:\n",
        "\n",
        "* 70% of data to training set.\n",
        "* ~10% of data to validation set.\n",
        "* ~20% of data to testing set.\n",
        "\n",
        "To do so, we'll call `train_test_split()` twice with different amounts:\n",
        "\n",
        "1. First on `dataset[\"train\"]` with `test_size=0.3` to make the 70/30 training/test split, we'll save this split to the variable `dataset_split`.\n",
        "2. Next on `dataset_split[\"test\"]` with `test_size=0.66` to make the 66/33 test/validation split, we'll set this variable to `dataset_test_val_split`.\n",
        "\n",
        "<figure style=\"text-align: center;\">\n",
        "    <img src=\"https://huggingface.co/datasets/mrdbourke/learn-hf-images/resolve/main/learn-hf-trashify-object-detection/05-dataset-splits.png\"\n",
        "     alt=\"A diagram illustrating two methods of splitting a dataset, shown as three vertical bars: the leftmost pink bar represents the 'Whole Dataset'; an arrow points to the middle bar, which is split into a blue 'Testing Split (30%)' on top and a pink 'Training Split (70%)' on the bottom; a second arrow points to the rightmost bar, which is divided into a blue 'Testing Split (20%)' at the top, a yellow 'Validation Split (10%)' in the middle, and a pink 'Training Split (70%)' at the bottom.\"\n",
        "     style=\"width: 100%; max-width: 900px; height: auto;\"/>\n",
        "     <figcaption>An approximate breakdown of the different dataset splits we're going to create. We'll start with the whole dataset and then break it into training and test splits before breaking the subsequent test split into test and validation splits. Our model will train on the training data and be evaluated on the validation and testing data.</figcaption>\n",
        "</figure>\n",
        "\n",
        "Once we've done this, we'll reassign all of the splits back to our original `dataset`.\n",
        "\n",
        "We'll also set `seed=42` for reproducibility.\n",
        "\n",
        "Let's do it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gzp0raQQKxXn"
      },
      "outputs": [],
      "source": [
        "# Note: Be careful of running this cell multiple times, if you do, the dataset size will get smaller.\n",
        "# If this happens, just reload the whole `dataset` as above.\n",
        "\n",
        "# 1. Split the data into \"train\" and \"test\" splits\n",
        "dataset_split = dataset[\"train\"].train_test_split(test_size=0.3, seed=42) # split the dataset into 70/30 train/test\n",
        "\n",
        "# 2. Split the test split into \"test\" and \"validation\" splits\n",
        "dataset_test_val_split = dataset_split[\"test\"].train_test_split(test_size=0.66, seed=42) # split the test set into 40/60 validation/test\n",
        "\n",
        "# Create \"train\" split from 1.\n",
        "dataset[\"train\"] = dataset_split[\"train\"]\n",
        "\n",
        "# Create a \"validation\" and \"test\" split from 2.\n",
        "dataset[\"validation\"] = dataset_test_val_split[\"train\"]\n",
        "dataset[\"test\"] = dataset_test_val_split[\"test\"]\n",
        "\n",
        "# Ensure splits lengths add to equal original dataset length (otherwise there's a mistmatch somewhere)\n",
        "assert original_dataset_length == len(dataset[\"train\"]) + len(dataset[\"validation\"]) + len(dataset[\"test\"]), \"Total dataset split lengths don't equal original dataset length, is there a mismatch? Perhaps try reloading the original dataset and re-running this cell.\"\n",
        "\n",
        "# View the dataset (now with splits)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fDNxIplKxXo"
      },
      "source": [
        "Perfect!\n",
        "\n",
        "Now we've got three splits of our dataset to work with.\n",
        "\n",
        "We'll make sure our model never sees the `validation` and `test` splits during training, so when evaluate it we know that it's only seeing new samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBC9nNvpKxXp"
      },
      "source": [
        "## 13.2 Writing a function for preprocessing multiple samples at a time\n",
        "\n",
        "UPTOHERE - preprocessing multiple samples at a time\n",
        "\n",
        "We've preprocessed and passed one sample through our model, new let's do the same for multiple samples.\n",
        "\n",
        "We're going to work towards having a function that can go from a group or batch of samples (images and their annotations) and return them in preprocessed form (via [`transformers.ConditionalDetrImageProcessor.preprocess`](https://huggingface.co/docs/transformers/en/model_doc/conditional_detr#transformers.ConditionalDetrImageProcessor.preprocess)) ready to be used with our model.\n",
        "\n",
        "Let's first remind ourselves of what a single unprocessed sample looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GAeo-iRKxXp"
      },
      "outputs": [],
      "source": [
        "# Get one sample from the training dataset\n",
        "one_sample = dataset[\"train\"][42]\n",
        "one_sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3QtdZJ9KxXq"
      },
      "source": [
        "Awesome, we get an `image` in `PIL.Image.Image` form as well as a single dictionary of `annotations`.\n",
        "\n",
        "How about if we were to inspect a group of three samples?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YupeIU5yKxXr"
      },
      "outputs": [],
      "source": [
        "# Get three samples from the training set\n",
        "group_of_samples = dataset[\"train\"][0:3]\n",
        "\n",
        "# Uncomment for full output (commented for brevity)\n",
        "# group_of_samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZGAAbBtKxXs"
      },
      "source": [
        "<details>\n",
        "\n",
        "<summary>Output of random_samples</summary>\n",
        "\n",
        "```\n",
        "Signature:\n",
        "{'image': [<PIL.Image.Image image mode=RGB size=960x1280>,\n",
        "  <PIL.Image.Image image mode=RGB size=960x1280>,\n",
        "  <PIL.Image.Image image mode=RGB size=960x1280>],\n",
        " 'image_id': [69, 1027, 1092],\n",
        " 'annotations': [{'file_name': ['c56fee61-959c-44b8-ada2-807d2ff45f90.jpeg',\n",
        "    'c56fee61-959c-44b8-ada2-807d2ff45f90.jpeg',\n",
        "    'c56fee61-959c-44b8-ada2-807d2ff45f90.jpeg',\n",
        "    'c56fee61-959c-44b8-ada2-807d2ff45f90.jpeg',\n",
        "    'c56fee61-959c-44b8-ada2-807d2ff45f90.jpeg',\n",
        "    'c56fee61-959c-44b8-ada2-807d2ff45f90.jpeg',\n",
        "    'c56fee61-959c-44b8-ada2-807d2ff45f90.jpeg',\n",
        "    'c56fee61-959c-44b8-ada2-807d2ff45f90.jpeg'],\n",
        "   'image_id': [69, 69, 69, 69, 69, 69, 69, 69],\n",
        "   'category_id': [5, 0, 1, 4, 4, 4, 4, 4],\n",
        "   'bbox': [[360.20001220703125, 528.5, 177.1999969482422, 261.79998779296875],\n",
        "    [298.29998779296875,\n",
        "     495.1000061035156,\n",
        "     381.1000061035156,\n",
        "     505.70001220703125],\n",
        "    [81.5999984741211,\n",
        "     592.0999755859375,\n",
        "     358.79998779296875,\n",
        "     316.29998779296875],\n",
        "    [1.2999999523162842,\n",
        "     776.7000122070312,\n",
        "     193.8000030517578,\n",
        "     211.89999389648438],\n",
        "    [301.1000061035156, 60.79999923706055, 146.89999389648438, 115.0],\n",
        "    [501.0, 75.9000015258789, 24.200000762939453, 71.19999694824219],\n",
        "    [546.4000244140625,\n",
        "     54.70000076293945,\n",
        "     130.3000030517578,\n",
        "     115.0999984741211],\n",
        "    [862.9000244140625,\n",
        "     41.099998474121094,\n",
        "     75.69999694824219,\n",
        "     80.19999694824219]],\n",
        "   'iscrowd': [0, 0, 0, 0, 0, 0, 0, 0],\n",
        "   'area': [46390.9609375,\n",
        "    192722.265625,\n",
        "    113488.4375,\n",
        "    41066.21875,\n",
        "    16893.5,\n",
        "    1723.0400390625,\n",
        "    14997.5302734375,\n",
        "    6071.14013671875]},\n",
        "  {'file_name': ['b664785b-f8b6-4dd2-9ede-d89c07564812.jpeg',\n",
        "    'b664785b-f8b6-4dd2-9ede-d89c07564812.jpeg',\n",
        "    'b664785b-f8b6-4dd2-9ede-d89c07564812.jpeg',\n",
        "    'b664785b-f8b6-4dd2-9ede-d89c07564812.jpeg',\n",
        "    'b664785b-f8b6-4dd2-9ede-d89c07564812.jpeg'],\n",
        "   'image_id': [1027, 1027, 1027, 1027, 1027],\n",
        "   'category_id': [5, 4, 1, 0, 0],\n",
        "   'bbox': [[378.29998779296875, 657.5, 139.8000030517578, 165.10000610351562],\n",
        "    [463.29998779296875, 754.5, 39.400001525878906, 30.299999237060547],\n",
        "    [451.20001220703125,\n",
        "     734.7999877929688,\n",
        "     109.19999694824219,\n",
        "     163.8000030517578],\n",
        "    [140.39999389648438, 400.29998779296875, 460.8999938964844, 491.5],\n",
        "    [2.299999952316284,\n",
        "     322.29998779296875,\n",
        "     201.6999969482422,\n",
        "     429.20001220703125]],\n",
        "   'iscrowd': [0, 0, 0, 0, 0],\n",
        "   'area': [23080.98046875,\n",
        "    1193.8199462890625,\n",
        "    17886.9609375,\n",
        "    226532.34375,\n",
        "    86569.640625]},\n",
        "  {'file_name': ['d822c383-f53a-4a2e-b2f2-3eac55c0e515.jpeg',\n",
        "    'd822c383-f53a-4a2e-b2f2-3eac55c0e515.jpeg',\n",
        "    'd822c383-f53a-4a2e-b2f2-3eac55c0e515.jpeg',\n",
        "    'd822c383-f53a-4a2e-b2f2-3eac55c0e515.jpeg'],\n",
        "   'image_id': [1092, 1092, 1092, 1092],\n",
        "   'category_id': [2, 5, 1, 0],\n",
        "   'bbox': [[97.80000305175781, 93.30000305175781, 177.5, 101.5999984741211],\n",
        "    [342.20001220703125, 572.5999755859375, 350.0, 344.20001220703125],\n",
        "    [185.1999969482422, 803.0, 304.3999938964844, 371.6000061035156],\n",
        "    [219.39999389648438, 259.1000061035156, 598.7000122070312, 584.5]],\n",
        "   'iscrowd': [0, 0, 0, 0],\n",
        "   'area': [18034.0, 120470.0, 113115.0390625, 349940.15625]}],\n",
        " 'label_source': ['manual_prodigy_label',\n",
        "  'manual_prodigy_label',\n",
        "  'manual_prodigy_label'],\n",
        " 'image_source': ['manual_taken_photo',\n",
        "  'manual_taken_photo',\n",
        "  'manual_taken_photo']}\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "Okay, now we get a list of `image` objects as well as a list of `annotation` dictionaries and more in the format:\n",
        "\n",
        "```json\n",
        "{\n",
        "    \"image\": [<PIL.Image.Image>, <PIL.Image.Image>, ...],\n",
        "    \"image_id\": [int, int, ...],\n",
        "    \"annotations\": [\n",
        "        {\n",
        "            \"file_name\": [str, str, ...],\n",
        "            \"image_id\": [int, int, ...],\n",
        "            \"category_id\": [int, int, ...],\n",
        "            \"bbox\": [[float, float, float, float], ...],\n",
        "            \"iscrowd\": [int, int, ...],\n",
        "            \"area\": [float, float, ...]\n",
        "        },\n",
        "        {...},\n",
        "        {...}\n",
        "    ],\n",
        "    \"label_source\": [str, str, ...],\n",
        "    \"image_source\": [str, str, ...]\n",
        "}\n",
        "```\n",
        "\n",
        "Knowing this structure, we'll want to write a function capable of taking it as input and then preparing it for the `preprocess` method.\n",
        "\n",
        "The `preprocess` method expects a list of images as well as COCO formatted annotations as input.\n",
        "\n",
        "So to create our `preprocess_batch` function we'll:\n",
        "\n",
        "1. Take in a list of examples (these will be in the format above), an `image_processor` and optional `transforms` (we don't need to pass these in for now but it's good to have the option).\n",
        "2. Create empty lists of `images` and `coco_annotations` we'll fill throughout the rest of the function.\n",
        "3. Extract the `image`, `image_id` and `annotations_dict` from our list of input examples.\n",
        "4. Create lists of annotations attributes such as `bbox`, `category_id` and `area` (these are required for our `format_image_annotations_as_coco` function.\n",
        "5. Optionally perform transforms/augmentations on the image and related boxes (because in object detection if you transform an image, should transform the related boxes as well).\n",
        "6. Convert the annotations into COCO format using the `format_image_annotations_as_coco` helper function we created earlier.\n",
        "7. Append the images and COCO formatted annotations to the empty lists created in 2.\n",
        "8. Pass the list of images and COCO formatted annotations to the `image_processor.preprocess` method to get the preprocessed batch.\n",
        "9. Return the preprocessed batch.\n",
        "\n",
        "Let's do it!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DEf_EBMKxXt"
      },
      "outputs": [],
      "source": [
        "# 1. Take in a list of examples, image processor and optional transforms\n",
        "def preprocess_batch(examples,\n",
        "                     image_processor,\n",
        "                     transforms=None, # Note: Could optionally add transforms (e.g. data augmentation) here\n",
        "                     ):\n",
        "    \"\"\"\n",
        "    Preprocesses a batch of image data with annotations for object detection models.\n",
        "\n",
        "    This function takes a batch of examples in a custom dataset format, extracts images and\n",
        "    their corresponding annotations, and converts them into a format suitable for model training\n",
        "    or inference using the provided image processor.\n",
        "\n",
        "    Args:\n",
        "        examples (dict): A dictionary containing the batch data with the following structure:\n",
        "            - \"image\" (List[PIL.Image.Image]): List of PIL Image objects\n",
        "            - \"image_id\" (List[int]): List of unique image identifiers\n",
        "            - \"annotations\" (List[dict]): List of annotation dictionaries, where each contains:\n",
        "                - \"file_name\" (List[str]): List of image filenames\n",
        "                - \"image_id\" (List[int]): List of image identifiers\n",
        "                - \"category_id\" (List[int]): List of object category IDs\n",
        "                - \"bbox\" (List[List[float]]): List of bounding boxes as [x, y, width, height]\n",
        "                - \"iscrowd\" (List[int]): List of crowd indicators (0 or 1)\n",
        "                - \"area\" (List[float]): List of object areas\n",
        "            - \"label_source\" (List[str]): List of label sources\n",
        "            - \"image_source\" (List[str]): List of image sources\n",
        "\n",
        "        image_processor: An image processor object to preprocess images for model input.\n",
        "            For example, can be `transformers.RTDetrDetrImageProcessor`.\n",
        "\n",
        "        transforms (optional): Image and annotations transforms for data augmentation.\n",
        "            Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        dict: Preprocessed batch with images and annotations converted to tensors\n",
        "            in the format required for a `transformers.RTDetrV2ForObjectDetection` model.\n",
        "\n",
        "    Note:\n",
        "        The `format_image_annotations_as_coco` function converts the input annotation format to COCO\n",
        "        format before applying the image_processor. This is required as the image_processor is designed\n",
        "        to handle COCO format annotations.\n",
        "    \"\"\"\n",
        "    # 2. Create empty lists to store images and annotations\n",
        "    images = []\n",
        "    coco_annotations = []\n",
        "\n",
        "    # 3. Extract the image, image_id and annotations from the examples\n",
        "    for image, image_id, annotations_dict in zip(examples[\"image\"],\n",
        "                                                 examples[\"image_id\"],\n",
        "                                                 examples[\"annotations\"]):\n",
        "\n",
        "        # 4. Create lists of annotation attributes\n",
        "        bbox_list = annotations_dict[\"bbox\"]\n",
        "        category_list = annotations_dict[\"category_id\"]\n",
        "        area_list = annotations_dict[\"area\"]\n",
        "\n",
        "        ###\n",
        "        # 5. Note: Could optionally apply a transform/augmentation here.\n",
        "        if transforms:\n",
        "            # Perform transform on image/boxes\n",
        "            pass\n",
        "        ###\n",
        "\n",
        "        # 6. Format the annotations into COCO format\n",
        "        cooc_format_annotations = format_image_annotations_as_coco(image_id=image_id,\n",
        "                                                                   categories=category_list,\n",
        "                                                                   areas=area_list,\n",
        "                                                                   bboxes=bbox_list)\n",
        "\n",
        "        # 7. Add images/annotations to their respective lists\n",
        "        images.append(image) # Note: may need to open image if it is an image path rather than PIL.Image\n",
        "        coco_annotations.append(cooc_format_annotations)\n",
        "\n",
        "\n",
        "    # 8. Apply the image processor to lists of images and annotations\n",
        "    preprocessed_batch = image_processor.preprocess(images=images,\n",
        "                                                    annotations=coco_annotations,\n",
        "                                                    return_tensors=\"pt\")\n",
        "\n",
        "    # 9. Return the preprocessed batch\n",
        "    return preprocessed_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nX7Rkb2dKxXu"
      },
      "source": [
        "Nice!\n",
        "\n",
        "Now how about we test it out on our `group_of_samples`?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8b3HPlMDKxXv"
      },
      "outputs": [],
      "source": [
        "preprocessed_samples = preprocess_batch(examples=group_of_samples,\n",
        "                                        image_processor=image_processor)\n",
        "\n",
        "preprocessed_samples.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeGg9i1OKxXw"
      },
      "source": [
        "Perfect, we get the same `keys()` as with our single sample.\n",
        "\n",
        "Except this time, we've got multiple samples, let's check the shape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuPRboedKxXw"
      },
      "outputs": [],
      "source": [
        "# Check the shape of our preprocessed samples\n",
        "print(f\"[INFO] Shape of preprocessed samples: {preprocessed_samples['pixel_values'].shape} -> [batch_size, colour_channels, height, width]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6iCxw1tKxXx"
      },
      "source": [
        "Wonderful, our batch of three samples have been preprocessed and are ready for input to our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrledgFtKxXy"
      },
      "source": [
        "## 13.3 Applying our preprocessing function to each data split\n",
        "\n",
        "We've seen our `preprocess_batch` function in action on a small group of samples.\n",
        "\n",
        "Now let's apply it to our different data splits.\n",
        "\n",
        "To do so, we can call the [`with_transform()`](https://huggingface.co/docs/datasets/en/package_reference/main_classes#datasets.Dataset.with_transform) method on our target dataset split and pass it our desired `transform`.\n",
        "\n",
        "Using `with_transform()` means our transformations will be applied on-the-fly when we call on our split datasets.\n",
        "\n",
        "Because the `with_transform()` method expects a callable with a single argument (the input examples), we'll turn our `preprocess_batch` into a [Python partial function](https://docs.python.org/3/library/functools.html#functools.partial).\n",
        "\n",
        "Doing this will mean we can prefill the `image_processor` and optionally the `transforms` parameter of our `preprocess_batch` function meaning it will only take `examples` as input, this is inline with the `with_transform()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcIaW2ndKxXz"
      },
      "outputs": [],
      "source": [
        "# Create a partial function for preprocessing\n",
        "from functools import partial\n",
        "\n",
        "# Note: Could create separate preprocess functions with different inputs depending on the split\n",
        "# (e.g. use data augmentation on training but not on validation/test)\n",
        "preprocess_batch_partial = partial(preprocess_batch,\n",
        "                                   image_processor=image_processor,\n",
        "                                   transforms=None) # could use transforms here if wanted\n",
        "\n",
        "# Inspect the preprocess_batch_partial function\n",
        "# preprocess_batch_partial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2Sy8w_pKxX0"
      },
      "source": [
        "Beautiful, now let's pass the `preprocess_batch_partial` function to the `with_transform()` method on each of our data splits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdRU_TidKxX1"
      },
      "outputs": [],
      "source": [
        "# Create a copy of the original dataset\n",
        "# (we don't need to do this, this is just so we can inspect the original dataset later on)\n",
        "processed_dataset = dataset.copy()\n",
        "\n",
        "# Apply the preprocessing function to the datasets (the preprocessing will happen on the fly, e.g. when the dataset is called rather than in-place)\n",
        "processed_dataset[\"train\"] = dataset[\"train\"].with_transform(transform=preprocess_batch_partial)\n",
        "processed_dataset[\"validation\"] = dataset[\"validation\"].with_transform(transform=preprocess_batch_partial)\n",
        "processed_dataset[\"test\"] = dataset[\"test\"].with_transform(transform=preprocess_batch_partial)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDWXCEijKxX2"
      },
      "source": [
        "Now when we get (via `__getitem__`) one of our samples from a `processed_dataset` split, it will be preprocessed on the fly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RF1l3-eaKxX3"
      },
      "outputs": [],
      "source": [
        "# Get an item from the dataset (in will be preprocessed as we get it)\n",
        "processed_dataset[\"train\"][42]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqJWsz1CKxX3"
      },
      "source": [
        "And the same happens when we get multiple (a batch) samples!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqllrGNUKxX4"
      },
      "outputs": [],
      "source": [
        "# Now when we call one or more of our samples, the preprocessing will take place\n",
        "batch_size_to_get = 32\n",
        "print(f\"[INFO] Shape of preprocessed images: {processed_dataset['train'][:batch_size_to_get]['pixel_values'].shape} -> [batch_size, colour_channels, height, width]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sgpmEftKxX5"
      },
      "outputs": [],
      "source": [
        "# We can pass these straight to our model! (note: may take a while if it's on CPU)\n",
        "# model(processed_dataset[\"train\"][:batch_size_to_get][\"pixel_values\"]) # uncomment to view output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbmdAfGFKxX6"
      },
      "source": [
        "## 13.4 Creating a collation function\n",
        "\n",
        "We now preprocess multiple samples at once.\n",
        "\n",
        "Time to create a collation function which will tell our model trainer how to stack these samples together into batches.\n",
        "\n",
        "We do this because processing more samples at once (e.g. 32 samples in a batch) in a batch is generally more efficient than one sample at a time or trying to process all samples at once.\n",
        "\n",
        "Our collation function will be used for the [`data_collator` parameter](https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.Trainer.data_collator) in our [`transformers.Trainer`](https://huggingface.co/docs/transformers/en/main_classes/trainer) instance later on.\n",
        "\n",
        "The input to our data collation function will be the output of `image_processor.preprocess()` (a preprocessed sample).\n",
        "\n",
        "And the output will be passed as a batch (we'll define the batch size later on) to our model's [`forward()` method](https://huggingface.co/docs/transformers/en/model_doc/conditional_detr#transformers.ConditionalDetrModel.forward).\n",
        "\n",
        "::: {.callout-note}\n",
        "What batch size should I use?\n",
        "\n",
        "You should generally use the batch size which uses the maximum amount of GPU memory you have.\n",
        "\n",
        "For example, if you have 16GB of GPU memory and a batch size of 32 only uses 8GB of that memory, you should try doubling the batch size to 64.\n",
        "\n",
        "The ideal batch size for a given dataset/model/hardware is often discovered in an iterative process.\n",
        ":::"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T97QuTxVKxX6"
      },
      "outputs": [],
      "source": [
        "from typing import List, Dict, Any\n",
        "\n",
        "def data_collate_function(preprocessed_batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "    \"\"\"Stacks together groups of preprocessed samples into batches for our model.\n",
        "\n",
        "    Args:\n",
        "        preprocessed_batch: A list of dictionaries where each dictionary represnets a preprocessed sample.\n",
        "\n",
        "    Returns:\n",
        "        collated_data: A dictionary containing the batched data ready in the format our model\n",
        "            is expecting. The dictionary has the following keys:\n",
        "                - \"pixel_values\": A stacked tensor of preprocessed pixel values.\n",
        "                - \"labels\": A list of label dictionaries.\n",
        "                - \"pixel_mask\": (Optional) A stacked tensor of pixel masks (this will be present\n",
        "                    only if the input contains a \"pixel_mask\" key.\n",
        "    \"\"\"\n",
        "    # Create an empty dictionary (our model wants a dictionary input)\n",
        "    collated_data = {}\n",
        "\n",
        "    # Stack together a collection of pixel_values tensors\n",
        "    collated_data[\"pixel_values\"] = torch.stack([sample[\"pixel_values\"] for sample in preprocessed_batch])\n",
        "\n",
        "    # Get the labels (these are dictionaries so no need to use torch.stack)\n",
        "    collated_data[\"labels\"] = [sample[\"labels\"] for sample in preprocessed_batch]\n",
        "\n",
        "    # If there is a pixel_mask key, return the pixel_mask's as well\n",
        "    if \"pixel_mask\" in preprocessed_batch[0]:\n",
        "        collated_data[\"pixel_mask\"] = torch.stack([sample[\"pixel_mask\"] for sample in preprocessed_batch])\n",
        "\n",
        "    return collated_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yqno_OkIKxX7"
      },
      "source": [
        "Excellent! Now let's try out our data collation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgiBkWxjKxX8"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# Try data_collate_function\n",
        "example_collated_data_batch = data_collate_function(processed_dataset[\"train\"].select(range(32)))\n",
        "example_collated_data_batch.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tfe1LKsOKxX9"
      },
      "source": [
        "Perfect! Looks like it worked. We've now got a batch of preprocessed images and label pairs.\n",
        "\n",
        "Let's check the shapes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMDoPrexKxX9"
      },
      "outputs": [],
      "source": [
        "# Check shapes of batched preprocessed samples\n",
        "print(f\"[INFO] Batch of pixel value shapes: {example_collated_data_batch['pixel_values'].shape}\")\n",
        "print(f\"[INFO] Batch of labels: {example_collated_data_batch['labels']}\")\n",
        "if \"pixel_mask\" in example_collated_data_batch:\n",
        "    print(f\"[INFO] Batch of pixel masks: {example_collated_data_batch['pixel_mask'].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2i-J0cPuKxX-"
      },
      "source": [
        "Now let's try to pass the `\"pixel_values\"` through our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdXSPseIKxX_"
      },
      "outputs": [],
      "source": [
        "# %%time\n",
        "\n",
        "# # Try pass a batch through our model (note: this will be relatively slow if our model is on the CPU)\n",
        "# model = create_model()\n",
        "\n",
        "# # example_batch_outputs = model(example_collated_data_batch[\"pixel_values\"])\n",
        "# example_batch_outputs = model(example_collated_data_batch[\"pixel_values\"])\n",
        "# # example_batch_outputs # uncomment for full output\n",
        "# example_batch_outputs.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itEP4hEEKxYA"
      },
      "outputs": [],
      "source": [
        "# # We get 300 predictions per image in our batch, each with a logit value for each of the classes in our dataset\n",
        "# example_batch_outputs.logits.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8U6y0KhuKxYB"
      },
      "source": [
        "This is what will happen during training, our model will continually go over batches (the size of these batches will be defined by us) over data and try to match its own predictions with the ground truth labels.\n",
        "\n",
        "In summary, we've created two major steps:\n",
        "\n",
        "1. `preprocess_batch` - Preprocesses single or groups of samples into the specific format required by our model.\n",
        "2. `data_collate_function` - Stacks together groups/batches of samples to be passed to our model's `forward()` method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILbzT4SAKxYB"
      },
      "source": [
        "# 14. Setting up TrainingArguments and a Trainer instance to train our model\n",
        "\n",
        "Data ready and prepared, time to train a model!\n",
        "\n",
        "We'll use [`transformers.TrainingArguments`](https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments) to set various hyperparameters for our model (many of these will be set by default, however, we can tweak them to our liking).\n",
        "\n",
        "We'll also create an instance of [`transformers.Trainer`](https://huggingface.co/docs/transformers/en/main_classes/trainer) which we can pass our preprocessed datasets for it to train/evaluate on.\n",
        "\n",
        "To train a model, we'll go through the following steps:\n",
        "\n",
        "1. Create a fresh instance of our model using the `create_model()` function.\n",
        "2. Make a directory for saving our trained models to.\n",
        "3. Define our model's hyperparameters using `transformers.TrainingArguments`, we'll take many of these settings from the assosciated research papers that introduced the models.\n",
        "4. Create an instance of `transformers.Trainer` and pass it our training arguments from 2 as well as our preprocessed data.\n",
        "5. Call [`transformers.Trainer.train()`](https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.Trainer.train) to train the model from 1 on our own data.\n",
        "\n",
        "Let's do it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2t0I2ATzKxYC"
      },
      "outputs": [],
      "source": [
        "# 1. Create a model instance\n",
        "model = create_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5pEgkv-KxYD"
      },
      "source": [
        "Model ready, let's now create a folder where we can save our trained models to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uXWaqfEKxYD"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# 2. Make a models directory for saving models\n",
        "models_dir = Path(\"models\")\n",
        "models_dir.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TopDWVRLKxYE"
      },
      "source": [
        "Perfect! Time to setup our model's hyperparameters with `transformers.TrainingArguments`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQVO0xTbKxYE"
      },
      "source": [
        "## 14.1 Setting up our TrainingArguments\n",
        "\n",
        "The `transformers.TrainingArguments` class holds many of the hyperparameters/settings for training our model.\n",
        "\n",
        "Many of them are set by default in the [`transformers.ConditionalDetrConfig`](https://huggingface.co/docs/transformers/en/model_doc/conditional_detr#transformers.ConditionalDetrConfig) class.\n",
        "\n",
        "However, we can tweak any of them to our own liking.\n",
        "\n",
        "Where do we get the settings from?\n",
        "\n",
        "The original [*Conditional DETR for fast training convergence*](https://arxiv.org/abs/2108.06152) paper states that all hyperparameters are the same as the original DETR ([*End-to-End Object Detection with Transformers*](https://arxiv.org/abs/2005.12872)).\n",
        "\n",
        "We can even dig into related papers such as [*DETRs Beat YOLOs on Real-time Object Detection*](https://arxiv.org/abs/2304.08069) and find the hyperaparameter settings as well.\n",
        "\n",
        "The main hyperparameters we are going to set are:\n",
        "\n",
        "| **Hyperparameter** | **Value** | **What does it do?** |\n",
        "| ----- | ----- | ----- |\n",
        "| `per_device_train_batch_size`, `per_device_eval_batch_size` | `16`, `32` or larger (hardware dependent) | Defines the number of samples passed to our model at one time. For example, if batch size is 16, our model will see 16 samples at a time. It's usually best practice to set this value to the highest your hardware can handle. |\n",
        "| `learning_rate` | `0.0001` (as per the listed papers) | Defines the multiplier on the size of gradient updates during training. Too high and gradients will explode, too low and gradients won't update, both lead to poor training results. The papers mention two different learning rates for the backbone and the detection head, I tried these and got poor results (likely because of our smaller dataset), a single learning rate for the whole network turned out to be better. |\n",
        "| `weight_decay` | `0.0001` (as per the listed papers) | Prevents model weights from getting too large by applying a small decay penalty over time. This prevents a single weight providing too much information. In essence, the model is forced to learn smaller, simpler weights to represent the data. A form of regularization (overfitting prevention). See more at [paperswithcode.com/method/weight-decay](https://paperswithcode.com/method/weight-decay). |\n",
        "| `max_grad_norm` | `0.1` (as per the listed papers) | Prevents gradients from getting too large during training. This will help to ensure stable training. See more at [paperswithcode.com/method/gradient-clipping](https://paperswithcode.com/method/gradient-clipping). |\n",
        "| `num_train_epochs` | `25` (depends on training data and available time) | Defines how many laps of the data your model will do. For example, setting epochs to 25 means the model will do 25 laps of the training data to learn different patterns. In practice, I've found this value to be a good starting point for our dataset and also because we are fine-tuning rather than training from scratch. However, if you had more data you might want to do more epochs (when training from scratch, the papers did 300 epochs). |\n",
        "| `warmup_ratio` | `0.05` | Percentage of total training steps to take learning rate from `0` to to the set value (e.g. `0.0001`). Can help with training stability in the early training steps of the model by not doing too large updates when first starting out. The papers state `2000` warmup steps, however, in practice I found this to be too many for our smaller dataset. |\n",
        "| `dataloader_num_workers` | `4` (hardware dependent) | Number of workers to load data from the CPU to the GPU. Higher is generally better if it is available, however, it can often cap out. Experimentally I've found that `0.5 * os.cpu_count()` generally works well. |\n",
        "\n",
        "<figure style=\"text-align: center;\">\n",
        "    <img src=\"https://huggingface.co/datasets/mrdbourke/learn-hf-images/resolve/main/learn-hf-trashify-object-detection/07-hyperparameters-for-models.png\"\n",
        "     alt=\"An image displaying training hyperparameters for RT-DETR, with 'Table A. Main hyperparameters of RT-DETR.' on the left, listing items like 'optimizer' (AdamW), 'base learning rate' (1e-4), 'learning rate of backbone' (1e-5), and various cost and loss weights; on the right, section 'A.4 Training hyperparameters' describes the training process, mentioning the use of 'AdamW', 'gradient clipping', an 'ImageNet pretrained backbone ResNet-50 imported from Torchvision', fine-tuning the backbone with a learning rate of '10^-5', and training the transformer with a learning rate of '10^-4', with key phrases like 'gradient clipping', 'ResNet-50 is imported from Torchvision', 'fine-tune the backbone using learning rate of 10^-5', and 'train the transformer with a learning rate of 10^-4' underlined in green.\"\n",
        "     style=\"width: 100%; max-width: 900px; height: auto;\"/>\n",
        "     <figcaption>Different hyperparameter settings from the official papers for the <a href=\"https://arxiv.org/pdf/2304.08069\">RT-DETR model</a> (left) and the original <a href=\"https://arxiv.org/pdf/2005.12872\">DETR model</a> (right).</figcaption>\n",
        "</figure>\n",
        "\n",
        "\n",
        "It's important to note that all of these values can be experimented with.\n",
        "\n",
        "And just because a research paper mentions a specific value, doesn't mean you have to use.\n",
        "\n",
        "For example, all the mentioned research papers tend to focus on training a model from scratch on the COCO dataset (330k images, 80 classes).\n",
        "\n",
        "Which is a much larger dataset with more classes than our dataset (1k images, 7 classes) which we are trying to fine-tune an existing model on rather than train from scratch.\n",
        "\n",
        "There are many more possible arguments/settings we've left out in the above table but if you'd like to explore these, I'd encourage you to check out the documentation for [`transformers.TrainingArguments`](https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXFbgN4sKxYG"
      },
      "outputs": [],
      "source": [
        "# 3. Create an instance of TrainingArguments to pass to Trainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# Hardware dependent hyperparameters\n",
        "# Set the batch size according to the memory you have available on your GPU\n",
        "# e.g. on my NVIDIA RTX 4090 with 24GB of VRAM, I can use a batch size of 32\n",
        "# without running out of memory\n",
        "BATCH_SIZE = 16\n",
        "DATALOADER_NUM_WORKERS = 4 # note: if you're on Google Colab, you may have to lower this to os.cpu_count() or to 0\n",
        "\n",
        "# Set number of epochs to how many laps you'd like to do over the data\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "# Setup hyperameters for training from the DETR paper(s)\n",
        "LEARNING_RATE = 1e-4\n",
        "WEIGHT_DECAY = 1e-4\n",
        "MAX_GRAD_NORM = 0.1\n",
        "WARMUP_RATIO = 0.05 # learning rate warmup from 0 to learning_rate as a ratio of total steps (e.g. 0.05 = 5% of total steps)\n",
        "\n",
        "# Create directory to save models to\n",
        "OUTPUT_DIR = Path(models_dir, \"rt_detrv2_finetuned_trashify_box_detector_v1\")\n",
        "print(f\"[INFO] Saving model to: {OUTPUT_DIR}\")\n",
        "\n",
        "# Create TrainingArguments to pass to Trainer\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    max_grad_norm=MAX_GRAD_NORM,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    warmup_ratio=WARMUP_RATIO,\n",
        "    # warmup_steps=2000, # number of warmup steps from 0 to learning_rate (overrides warmup_ratio, found this to be too long for our dataset)\n",
        "    logging_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    remove_unused_columns=False,\n",
        "    fp16=True, # use mixed precision training\n",
        "    dataloader_num_workers=DATALOADER_NUM_WORKERS, # note: if you're on Google Colab, you may have to lower this to os.cpu_count() or to 0\n",
        "    eval_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False, # want to minimize eval_loss (e.g. lower is better)\n",
        "    report_to=\"none\", # don't save experiments to a third party service\n",
        "    push_to_hub=False,\n",
        "    eval_do_concat_batches=False, # this defaults to True but we'll set it to False for our evaluation function\n",
        "    # save_safetensors=False # turn this off to prevent potential checkpoint issues\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AgBk4-zKxYI"
      },
      "source": [
        "## 14.2 Optional: Setting up an optimizer for multiple learning rates\n",
        "\n",
        "In the papers that mentioned the DETR model we're using (see Table 1 in the [RT-DETRv2 paper](https://arxiv.org/abs/2407.17140)), they state that they used a different learning rate value for the backbone (`learning_rate=1e-5`) as well as the object detection head (`learning_rate=1e-4`).\n",
        "\n",
        "<figure style=\"text-align: center;\">\n",
        "    <img src=\"https://huggingface.co/datasets/mrdbourke/learn-hf-images/resolve/main/learn-hf-trashify-object-detection/06-table-1-rt-detrv2-paper.png\"\n",
        "     alt=\"A table, titled 'Table 1: The hyperparameters of RT-DETRv2,' lists four model variants: 'RT-DETRv2-S,' 'RT-DETRv2-M,' 'RT-DETRv2-L,' and 'RT-DETRv2-X,' along with their respective 'Backbone' architectures ('ResNet18,' 'ResNet34,' 'ResNet50,' and 'ResNet101'), and their learning rates for the backbone ('lr_backbone': '1e-4,' '5e-5,' '1e-5,' '1e-6') and the detector head ('lr_det': all '1e-4')\"\n",
        "     style=\"width: 100%; max-width: 600px; height: auto;\"/>\n",
        "     <figcaption>Different learning rates used for different sections of the model from the <a href=\"https://arxiv.org/pdf/2407.17140\">RT-DETRv2 paper</a>. The backbone uses a slightly lower learning rate than the detection head.</figcaption>\n",
        "</figure>\n",
        "\n",
        "To set this up ourselves, we can extract which parameters of our model belong to the `backbone` as well as which don't.\n",
        "\n",
        "To find the backbone parameters, we can loop through our model's `named_parameters()` method and filter for any which contain the string `\"backbone\"` in their name.\n",
        "\n",
        "We'll append these to a list called `backbone_parameters` and assume any that don't have `\"backbone\"` in their name are not part of the model's backbone.\n",
        "\n",
        "We can use these two lists of parameters to pass to [`torch.optim.AdamW`](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html) with different learning rate values for each.\n",
        "\n",
        "::: {.callout-note}\n",
        "In my experiments with our smaller dataset size (~1100 images), I found that setting two different learning rates for the backbone and the object detection head led to poorer performance than just setting a single learning rate for the whole model.\n",
        "\n",
        "The code below is an example of how to create a custom optimizer with different learning rates for different parts of the model.\n",
        "\n",
        "However, in our actual training code, we'll use a single learning rate for the whole model.\n",
        ":::\n",
        "\n",
        "We can then subclass [`transformers.Trainer`](https://huggingface.co/docs/transformers/en/main_classes/trainer) and update the method [`create_optimizer()`](https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.Trainer.create_optimizer) to use our custom optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHQ6biKjKxYI"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "# Create lists for different kinds of parameters\n",
        "backbone_parameters = []\n",
        "other_parameters = []\n",
        "\n",
        "# Can loop through model parameters and extract different model sections\n",
        "for name, param in model.model.named_parameters():\n",
        "    if \"backbone\" in name:\n",
        "        # print(f\"Backbone parameter: {name}\")\n",
        "        backbone_parameters.append(param)\n",
        "    else:\n",
        "        # print(f\"Other parameter: {name}\")\n",
        "        other_parameters.append(param)\n",
        "\n",
        "print(f\"[INFO] Number of backbone parameter modules: {len(backbone_parameters)}\")\n",
        "print(f\"[INFO] Number of other parameter modules: {len(other_parameters)}\")\n",
        "\n",
        "# Setup a custom subclass of Trainer to use different learning rates for different parts of the model\n",
        "class CustomTrainer(Trainer):\n",
        "    def create_optimizer(self):\n",
        "        self.optimizer = torch.optim.AdamW([\n",
        "            {\"params\": backbone_parameters, \"lr\": 1e-4},\n",
        "            {\"params\": other_parameters, \"lr\": 1e-4}\n",
        "        ], weight_decay=0.0001)\n",
        "        return self.optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAnuhI5BKxYJ"
      },
      "source": [
        "Awesome!\n",
        "\n",
        "Now if we wanted to use our custom optimizer, we could use `CustomTrainer` instead of `Trainer`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkVvLRXPJssR"
      },
      "source": [
        "### 14.3 Creating an evaluation function\n",
        "\n",
        "Evaluating a model's performance is just as important as training a model.\n",
        "\n",
        "After all, if you don't know how well your model is performing, how can you be confident in deploying it or using it in the real world?\n",
        "\n",
        "In this section, let's create an evaluation function we can pass to `transformers.Trainer`'s [`compute_metrics` parameter](https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.Trainer.compute_metrics).\n",
        "\n",
        "The main goal of an evaluation function is to compare the model's predictions to the ground truth labels.\n",
        "\n",
        "For example, how does a model's box predictions look like compared to the ground truth box predictions?\n",
        "\n",
        "Once we've got a trained model, we can inspect these visually by plotting them on images.\n",
        "\n",
        "However, during model training, we'll get our `Trainer` instance to output evaluation metrics so we can get a snapshot of performance along the way.\n",
        "\n",
        "Some things to note about the evaluation function we'll create:\n",
        "\n",
        "* Reading the documentation for the [`compute_metrics` parameter](https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.Trainer.compute_metrics), we can see our evaluation function will be required to take a [`transformers.EvalPrediction`](https://huggingface.co/docs/transformers/en/internal/trainer_utils#transformers.EvalPrediction) as input.   \n",
        "    * This contains our model's predictions and labels as `predictions` and `label_ids` attributes respectively.\n",
        "* We must also return a dictionary with string to metric values for it to be displayed during training. For example, `{\"metric_value\": 42, ...}`.\n",
        "* To evaluate our object detection model we're going to use the mAP metric (Mean Average Precision, a standard metric used amongst object detection models, see the [COCO evaluation section](https://cocodataset.org/#detection-eval) for more details). To do so, we'll use [`torchmetrics` package](https://lightning.ai/docs/torchmetrics/stable/), specifically [`torchmetrics.detection.mean_ap.MeanAveragePrecision`](https://lightning.ai/docs/torchmetrics/stable/detection/mean_average_precision.html).\n",
        "    * This method expects boxes in format XYXY absolute format by default.\n",
        "* Our evaluation function will be an adaptation of the code example in the object detection example [on the Hugging Face GitHub](https://github.com/huggingface/transformers/blob/336dc69d63d56f232a183a3e7f52790429b871ef/examples/pytorch/object-detection/run_object_detection.py#L160).\n",
        "* For an in-depth overview on object detection metrics, see the [Roboflow Guide to Object Detection Metrics](https://blog.roboflow.com/object-detection-metrics/).\n",
        "\n",
        "We'll start by making a small helper function to convert bounding boxes from CXCYWH normalized format to XYXY absolute format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVcoIIdWJssR"
      },
      "outputs": [],
      "source": [
        "def convert_bbox_cxcywh_to_xyxy_absolute(boxes,\n",
        "                                         image_size_target):\n",
        "    \"\"\"\n",
        "    Converts CXCYWH normalized boxes to XYXY absolute boxes.\n",
        "\n",
        "    The output of our preprocess method puts boxes in CXCYWH format.\n",
        "\n",
        "    But our evaluation metric torchmetrics.detection.mean_ap.MeanAveragePrecision expects\n",
        "        boxes in XYXY absolute format.\n",
        "\n",
        "    Args:\n",
        "        boxes (torch.Tensor): A tensor of shape (N, 4) where N is the number of boxes and each box is in CXCYWH format.\n",
        "        image_size_target (tuple): A tuple containing the target image size as (height, width).\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: A tensor of shape (N, 4) where each box is converted to XYXY absolute format.\n",
        "    \"\"\"\n",
        "    # Convert normalized CXCYWH (output of model) -> absolute XYXY format (required for evaluation)\n",
        "    boxes = box_convert(boxes=boxes, in_fmt=\"cxcywh\", out_fmt=\"xyxy\")\n",
        "\n",
        "    # Convert normalized box coordinates to absolute pixel values based on the target size\n",
        "    image_size_target_height = image_size_target[0]\n",
        "    image_size_target_width = image_size_target[1]\n",
        "    boxes = boxes * torch.tensor([image_size_target_width,\n",
        "                                  image_size_target_height,\n",
        "                                  image_size_target_width,\n",
        "                                  image_size_target_height]) # Multiply X coordinates by the width and Y coordinates by the height\n",
        "\n",
        "    return boxes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GruqLSU4JssS"
      },
      "source": [
        "Perfect!\n",
        "\n",
        "Time to craft our `compute_metrics` function.\n",
        "\n",
        "The main goal of the function will be to take a `transformers.EvalPrediction` output from our model and return a dictionary mapping metric names to values, for example, `{\"metric_name\": 42.0 ...}`.\n",
        "\n",
        "To do so, we'll go through the following steps:\n",
        "\n",
        "1. Create a Python `dataclass` to hold our model's outputs. We could use a dictionary but this will give our code a bit more structure.\n",
        "2. Create a `compute_metrics` function which takes in an [`EvalPrediction`](https://huggingface.co/docs/transformers/en/internal/trainer_utils#transformers.EvalPrediction) object as well as other required evaluation parameters such as `image_processor` (for post processing boxes), `id2label` (for mapping metrics to class names) and `threshold` (for assigning a prediction probability threshold to boxes).\n",
        "3. Extract predictions and targets from `EvalPrediction` via `EvalPrediction.predictions` and `EvalPrediction.label_ids` respectively.\n",
        "4. Create empty lists of `image_sizes` (for post processing boxes), `post_processed_predictions` and `post_processed_targets` (we'll compare the latter two to each other).\n",
        "5. Collect target samples in format required for [`torchmetrics.detection.mean_ap.MeanAveragePrecision`](https://lightning.ai/docs/torchmetrics/stable/detection/mean_average_precision.html), for example, `[{\"boxes\": [...], \"labels\": [...]}]`.\n",
        "6. Collect predictions in the required formart for `MeanAveragePrecision`, our model produces boxes in CXCYWH format, then we use `image_processor.post_process_object_detection` to convert the predictions to XYXY format, and append them to `post_processed_predictions` in form `[{\"boxes\": [...], \"labels\": [...], \"scores\": [...]}]`.\n",
        "7. Initialize an instance of `torchmetrics.detection.mean_ap.MeanAveragePrecision` (see documentation for output of `MeanAveragePrecision`) and pass it predictions and labels to compute on.\n",
        "8. Extract lists of target metrics from the output of `MeanAveragePrecision`, for example, with `metrics.pop(\"target_item\")`.\n",
        "9. Prepare metrics for output in the form of a dict with metric names -> values, for example, `{\"metric_name\": 42.0, ...}`.\n",
        "10. Round metric values in output dictionary for visual display during training.\n",
        "11. Create a partial function we can pass to `transformers.Trainer`'s `compute_metrics` parameter to run as a callable with appropriate parameter inputs to our `compute_metrics` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fb8LYco8JssT"
      },
      "outputs": [],
      "source": [
        "# Create an evaluation function to test our model's performance\n",
        "import numpy as np\n",
        "\n",
        "from typing import Optional, Mapping\n",
        "\n",
        "from transformers import EvalPrediction\n",
        "\n",
        "from torchvision.ops import box_convert\n",
        "\n",
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
        "\n",
        "# 1. Create a dataclass to hold our model's outputs\n",
        "@dataclass\n",
        "class ModelOutput:\n",
        "    logits: torch.Tensor\n",
        "    pred_boxes: torch.Tensor\n",
        "\n",
        "# 2. Create a compute_metrics function which takes in EvalPrediction and other required parameters\n",
        "@torch.no_grad()\n",
        "def compute_metrics(\n",
        "    evaluation_results: EvalPrediction, # these come out of the Trainer.evaluate method, see: https://huggingface.co/docs/transformers/en/internal/trainer_utils#transformers.EvalPrediction\n",
        "    image_processor: AutoImageProcessor,\n",
        "    threshold: float = 0.0,\n",
        "    id2label: Optional[Mapping[int, str]] = None,\n",
        ") -> Mapping[str, float]:\n",
        "    \"\"\"\n",
        "    Compute mean average mAP, mAR and their variants for the object detection task.\n",
        "\n",
        "    Args:\n",
        "        evaluation_results (EvalPrediction): Predictions and targets from evaluation.\n",
        "        threshold (float, optional): Threshold to filter predicted boxes by confidence. Defaults to 0.0.\n",
        "        id2label (Optional[dict], optional): Mapping from class id to class name. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        Mapping[str, float]: Metrics in a form of dictionary {<metric_name>: <metric_value>}\n",
        "    \"\"\"\n",
        "\n",
        "    # 3. Extract predictions and targets from EvalPrediction\n",
        "    predictions, targets = evaluation_results.predictions, evaluation_results.label_ids\n",
        "\n",
        "    # For metric computation we need to provide to MeanAveragePrecision\n",
        "    #  - 'targets' in a form of list of dictionaries with keys \"boxes\", \"labels\"\n",
        "    #  - 'predictions' in a form of list of dictionaries with keys \"boxes\", \"scores\", \"labels\"\n",
        "\n",
        "    # 4. Get a list of image sizes, processed targets and processed predictions\n",
        "    image_sizes = []\n",
        "    post_processed_targets = []\n",
        "    post_processed_predictions = []\n",
        "\n",
        "    ### Target collection ###\n",
        "\n",
        "    # 5. Collect target attributes in the required format for metric computation\n",
        "    for batch in targets:\n",
        "        # Collect ground truth image sizes, we will need them for predictions post processing\n",
        "        batch_image_sizes = torch.tensor(np.array([x[\"orig_size\"] for x in batch])) # turn into a list of numpy arrays first, then tensors\n",
        "        image_sizes.append(batch_image_sizes)\n",
        "\n",
        "        # Collect targets in the required format for metric computation\n",
        "        # boxes were converted to YOLO format needed for model training\n",
        "        # here we will convert them to Pascal VOC format (x_min, y_min, x_max, y_max)\n",
        "        # or XYXY format. We do this because the boxes out of preprocess() are in\n",
        "        # CXCYWH normalized format.\n",
        "        for image_target in batch:\n",
        "\n",
        "            # Get boxes and convert from CXCYWH to XYXY\n",
        "            boxes = torch.tensor(image_target[\"boxes\"])\n",
        "            boxes = convert_bbox_cxcywh_to_xyxy_absolute(boxes=boxes,\n",
        "                                                         image_size_target=image_target[\"orig_size\"])\n",
        "\n",
        "            # Get labels\n",
        "            labels = torch.tensor(image_target[\"class_labels\"])\n",
        "\n",
        "            # Append box and label pairs in format requried for MeanAveragePrecision class\n",
        "            post_processed_targets.append({\"boxes\": boxes,\n",
        "                                           \"labels\": labels})\n",
        "\n",
        "    ### Prediction collection ###\n",
        "\n",
        "    # 6. Collect predictions in the required format for metric computation,\n",
        "    # model produce boxes in YOLO format (CXCYWH), then image_processor.post_process_object_detection to\n",
        "    # convert them to Pascal VOC format (XYXY).\n",
        "    for batch, target_sizes in zip(predictions, image_sizes):\n",
        "        batch_logits, batch_boxes = batch[1], batch[2]\n",
        "        output = ModelOutput(logits=torch.tensor(batch_logits),\n",
        "                             pred_boxes=torch.tensor(batch_boxes))\n",
        "\n",
        "        # Post process the model outputs\n",
        "        post_processed_output = image_processor.post_process_object_detection(\n",
        "                                                    outputs=output,\n",
        "                                                    threshold=threshold,\n",
        "                                                    target_sizes=target_sizes) # target sizes required to shape boxes in correct ratio of original image\n",
        "\n",
        "        # Append post_processed_output in form `[{\"boxes\": [...], \"labels\": [...], \"scores\": [...]}]`\n",
        "        post_processed_predictions.extend(post_processed_output)\n",
        "\n",
        "    # 7. Compute mAP\n",
        "    max_detection_thresholds = [1, 10, 100] # 1 = mar@1, mar@10, mar@100 (100 = default max total boxes for post processed predictions out of object detection model)\n",
        "    metric = MeanAveragePrecision(box_format=\"xyxy\",\n",
        "                                  class_metrics=True,\n",
        "                                  max_detection_thresholds=max_detection_thresholds)\n",
        "    metric.warn_on_many_detections = False # don't output a warning when large amount of detections come out (the sorting handles this anyway)\n",
        "    metric.update(post_processed_predictions,\n",
        "                  post_processed_targets)\n",
        "    metrics = metric.compute()\n",
        "\n",
        "    # Optional: print metrics dict for troubleshooting\n",
        "    # print(metrics)\n",
        "\n",
        "    # 8. Extract list of per class metrics with separate metric for each class\n",
        "    classes = metrics.pop(\"classes\")\n",
        "    map_per_class = metrics.pop(\"map_per_class\")\n",
        "\n",
        "    # Optional: mAR@N per class (mAR = Mean Average Recall)\n",
        "    mar_per_class = metrics.pop(\"mar_100_per_class\")\n",
        "\n",
        "    # 9. Prepare metrics per class in the form of a dict with metric names -> values, e.g. {\"metric_name\": 42.0, ...}\n",
        "    # for class_id, class_map in zip(classes, map_per_class):\n",
        "    for class_id, class_map, class_mar in zip(classes, map_per_class, mar_per_class):\n",
        "        class_name = id2label[class_id.item()] if id2label is not None else class_id.item()\n",
        "        metrics[f\"map_{class_name}\"] = class_map\n",
        "\n",
        "        # Optional: mAR@100 per class\n",
        "        metrics[f\"mar_100_{class_name}\"] = class_mar\n",
        "\n",
        "    # 10. Round metrics for suitable visual output\n",
        "    metrics = {k: round(v.item(), 4) for k, v in metrics.items()}\n",
        "\n",
        "    # Optional: print metrics dict for troubleshooting\n",
        "    # print(metrics)\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# 11. Create a partial function for our compute_metrics function (we'll pass this to compute_metrics in Trainer)\n",
        "eval_compute_metrics_fn = partial(\n",
        "        compute_metrics,\n",
        "        image_processor=image_processor,\n",
        "        threshold=0.0,\n",
        "        id2label=id2label,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKx1IIP1KxYK"
      },
      "source": [
        "## 14.4 Training our model with Trainer\n",
        "\n",
        "We've now got all the ingredients needed to train our model!\n",
        "\n",
        "The good news is since we've put so much effort into preparing our dataset, creating an evaluation function and setting up our training arguments, we can train our model in a few lines of code.\n",
        "\n",
        "To train our model, we'll set up an instance of [`transformers.Trainer`](https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.Trainer) and then we'll pass it the following arguments:\n",
        "\n",
        "* `model` - The `model` we'd like to train. In our case it will be the fresh insteand of `model` we created using our `create_model()` function.\n",
        "* `args` - An instance of [`transformers.TrainingArguments`](https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments) (or `training_args` in our case) containing various hyperparameter settings to use for our model.\n",
        "* `data_collator` - The function to use which will turn a list of samples from `train_dataset` into a batch of samples.\n",
        "* `train_dataset` - The dataset we'd like our model to train on, in our case this will be `processed_dataset[\"train\"]`, the dataset we've already preprocessed.\n",
        "* `eval_dataset` - The dataset we'd like our model to be evaluated on, in our case this will be `processed_dataset[\"validation\"]`, our model will never see these samples during training, it will only test itself on these.\n",
        "* `compute_metrics` - A `Callable` which takes in [`EvalPrediction`] and is able to return a string to metric (`{\"metric_name\": value}`) dictionary, these will displayed during training.\n",
        "\n",
        "After we've done all that, we can start to train our model with by calling [`transformers.Trainer.train()`](https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.Trainer.train)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMy14QQLKxYK"
      },
      "outputs": [],
      "source": [
        "# Note: Depending on the size/speed of your GPU, this may take a while\n",
        "from transformers import Trainer\n",
        "\n",
        "# 5. Setup instance of Trainer\n",
        "model_v1_trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collate_function,\n",
        "    train_dataset=processed_dataset[\"train\"], # pass in the already preprocessed data\n",
        "    eval_dataset=processed_dataset[\"validation\"],\n",
        "    compute_metrics=eval_compute_metrics_fn,\n",
        ")\n",
        "\n",
        "# 6. Train the model\n",
        "model_v1_results = model_v1_trainer.train(\n",
        "    # resume_from_checkpoint=False # you can continue training a model here by passing in the path to a previous checkpoint\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aNK0uQXKxYM"
      },
      "source": [
        "UPTOHERE:\n",
        "\n",
        "- going through training steps and making sure we end with a model training\n",
        "- Plot the loss curves + mAP curves (make sure the loss going down, mAP going up)\n",
        "- TK - if your loss values aren't the exact same, this is because of the randomness of machine learning, what's important is that the direction is similar (e.g. loss going down)\n",
        "- notes on training:\n",
        "    - 25 epochs gets good baseline results\n",
        "    - 50 epochs (longer training) gets better results but takes 2x the time (this is a good extension for someone to try)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIEWA5-MKxYN"
      },
      "outputs": [],
      "source": [
        "# # 25 epochs\n",
        "# # Epoch\tTraining Loss\tValidation Loss\tMap\tMap 50\tMap 75\tMap Small\tMap Medium\tMap Large\tMar 1\tMar 10\tMar 100\tMar Small\tMar Medium\tMar Large\tMap Bin\tMap Hand\tMap Not Bin\tMap Not Hand\tMap Not Trash\tMap Trash\tMap Trash Arm\n",
        "# # 1\t12.257200\t4.579205\t0.000700\t0.003100\t0.000300\t0.000000\t0.001100\t0.000700\t0.005600\t0.011000\t0.044300\t0.000000\t0.014800\t0.043800\t0.003100\t0.000000\t0.000000\t-1.000000\t0.000000\t0.001000\t0.000100\n",
        "# # 2\t3.214800\t2.093854\t0.012600\t0.027700\t0.009400\t0.000000\t0.002300\t0.012900\t0.027500\t0.091400\t0.136600\t0.000000\t0.056800\t0.142000\t0.066100\t0.001500\t0.000000\t-1.000000\t0.001600\t0.006700\t0.000000\n",
        "# # 3\t2.031300\t1.615478\t0.022200\t0.054700\t0.017600\t0.000000\t0.005100\t0.022500\t0.051500\t0.157800\t0.202400\t0.000000\t0.052300\t0.204500\t0.107500\t0.007300\t0.000000\t-1.000000\t0.000300\t0.018000\t0.000000\n",
        "# # 4\t1.750000\t1.561830\t0.060300\t0.118400\t0.057600\t0.000000\t0.007500\t0.063100\t0.097500\t0.253500\t0.302300\t0.000000\t0.237500\t0.306400\t0.228100\t0.055700\t0.000000\t-1.000000\t0.001500\t0.076500\t0.000000\n",
        "# # 5\t1.617800\t1.382881\t0.064500\t0.121800\t0.061400\t0.000000\t0.006800\t0.066900\t0.130900\t0.292900\t0.348300\t0.000000\t0.105700\t0.360300\t0.209900\t0.087600\t0.000000\tNo Log\t0.002600\t0.086600\t0.000000\n",
        "# # 6\t1.510300\t1.307172\t0.072900\t0.132500\t0.068500\t0.000000\t0.009700\t0.075700\t0.151900\t0.299400\t0.352800\t0.000000\t0.112500\t0.361600\t0.199800\t0.117100\t0.000000\t-1.000000\t0.002400\t0.118200\t0.000000\n",
        "# # 7\t1.405600\t1.251348\t0.079900\t0.130500\t0.078700\t0.000000\t0.017500\t0.083700\t0.175700\t0.310100\t0.365800\t0.000000\t0.056800\t0.383100\t0.223800\t0.048100\t0.000000\tNo Log\t0.002600\t0.204600\t0.000000\n",
        "# # 8\t1.331100\t1.159470\t0.113300\t0.180400\t0.120800\t0.000000\t0.028400\t0.116000\t0.210500\t0.345200\t0.392100\t0.000000\t0.130700\t0.403300\t0.313500\t0.154200\t0.000000\tNo Log\t0.006600\t0.205600\t0.000000\n",
        "# # 9\t1.221300\t1.101122\t0.144700\t0.227600\t0.156400\t0.025200\t0.019500\t0.148800\t0.232200\t0.385000\t0.416300\t0.025000\t0.134100\t0.433900\t0.377000\t0.264400\t0.000000\tNo Log\t0.012300\t0.214600\t0.000000\n",
        "# # 10\t1.179900\t1.075704\t0.163600\t0.247600\t0.181500\t0.000000\t0.038200\t0.168100\t0.231600\t0.387100\t0.434300\t0.000000\t0.164800\t0.455400\t0.457000\t0.255800\t0.000000\t-1.000000\t0.017300\t0.251400\t0.000000\n",
        "# # 11\t1.145600\t1.023341\t0.204600\t0.295000\t0.230600\t0.050500\t0.039500\t0.211200\t0.257700\t0.425600\t0.457900\t0.050000\t0.142000\t0.487100\t0.557100\t0.334300\t0.000600\t-1.000000\t0.026500\t0.309000\t0.000000\n",
        "# # 12\t1.136500\t1.016237\t0.217600\t0.319900\t0.244700\t0.000000\t0.035900\t0.225000\t0.253500\t0.431300\t0.457300\t0.000000\t0.202300\t0.487200\t0.585900\t0.347900\t0.000700\t-1.000000\t0.027600\t0.343800\t0.000000\n",
        "# # 13\t1.117300\t0.968450\t0.238400\t0.348400\t0.265800\t0.000000\t0.046200\t0.247500\t0.274400\t0.443500\t0.471100\t0.000000\t0.294300\t0.504300\t0.583500\t0.394100\t0.000400\t-1.000000\t0.036700\t0.415800\t0.000000\n",
        "# # 14\t1.022700\t0.964383\t0.259700\t0.384800\t0.289100\t0.000000\t0.067500\t0.269300\t0.274300\t0.475400\t0.498700\t0.000000\t0.267000\t0.544700\t0.637500\t0.454800\t0.002400\t-1.000000\t0.039700\t0.424200\t0.000000\n",
        "# # 15\t1.039000\t0.985401\t0.260200\t0.394400\t0.283300\t0.000000\t0.049100\t0.269700\t0.286400\t0.515200\t0.536000\t0.000000\t0.185200\t0.590000\t0.621600\t0.450200\t0.005300\t-1.000000\t0.042100\t0.441800\t0.000500\n",
        "# # 16\t1.012600\t0.940276\t0.269900\t0.411300\t0.311300\t0.000000\t0.073900\t0.279600\t0.289500\t0.504500\t0.526500\t0.000000\t0.272700\t0.576900\t0.634600\t0.460400\t0.004900\t-1.000000\t0.053900\t0.465300\t0.000000\n",
        "# # 17\t0.928400\t0.934266\t0.276900\t0.413600\t0.309300\t0.000000\t0.070100\t0.290400\t0.297700\t0.590600\t0.609800\t0.000000\t0.308500\t0.659700\t0.661100\t0.472700\t0.006000\t-1.000000\t0.066200\t0.452900\t0.002500\n",
        "# # 18\t0.939700\t0.919036\t0.277800\t0.417400\t0.313000\t0.000000\t0.082100\t0.288300\t0.290600\t0.634400\t0.652700\t0.000000\t0.318700\t0.697400\t0.638400\t0.485200\t0.005000\t-1.000000\t0.061900\t0.470600\t0.006000\n",
        "# # 19\t0.860800\t0.921640\t0.279500\t0.424300\t0.308700\t0.000000\t0.068900\t0.291900\t0.306400\t0.627100\t0.648800\t0.000000\t0.263100\t0.700300\t0.646300\t0.490200\t0.007300\t-1.000000\t0.069300\t0.459000\t0.004700\n",
        "# # 20\t0.888200\t0.908211\t0.290500\t0.435600\t0.329600\t0.025200\t0.066200\t0.303200\t0.309100\t0.641300\t0.660500\t0.025000\t0.282400\t0.711400\t0.660700\t0.519400\t0.006700\t-1.000000\t0.079200\t0.471500\t0.005800\n",
        "# # 21\t0.865500\t0.907514\t0.289400\t0.429000\t0.333000\t0.000000\t0.080400\t0.302600\t0.303400\t0.650900\t0.669600\t0.000000\t0.304000\t0.720300\t0.667100\t0.504400\t0.006100\t-1.000000\t0.071600\t0.480000\t0.007100\n",
        "# # 22\t0.841000\t0.897333\t0.296700\t0.437500\t0.335400\t0.000000\t0.087800\t0.311200\t0.307200\t0.651100\t0.664800\t0.000000\t0.281800\t0.718600\t0.671300\t0.534000\t0.005100\t-1.000000\t0.073900\t0.488800\t0.006800\n",
        "# # 23\t0.821500\t0.895820\t0.299100\t0.441800\t0.339100\t0.000000\t0.087100\t0.313200\t0.314600\t0.651800\t0.670400\t0.000000\t0.336900\t0.718500\t0.670900\t0.548300\t0.005400\t-1.000000\t0.074900\t0.488800\t0.006400\n",
        "# # 24\t0.798200\t0.892997\t0.296100\t0.441700\t0.335200\t0.000000\t0.075000\t0.310400\t0.310500\t0.638100\t0.656000\t0.000000\t0.306800\t0.705000\t0.670700\t0.534100\t0.005300\t-1.000000\t0.076800\t0.483200\t0.006700\n",
        "# # 25\t0.786300\t0.894873\t0.296400\t0.442700\t0.335400\t0.000000\t0.075000\t0.311800\t0.309700\t0.635700\t0.655900\t0.000000\t0.293700\t0.707600\t0.671900\t0.535700\t0.005100\t-1.000000\t0.076700\t0.482500\t0.006400\n",
        "\n",
        "# # 50 epochs\n",
        "# Epoch\tTraining Loss\tValidation Loss\tMap\tMap 50\tMap 75\tMap Small\tMap Medium\tMap Large\tMar 1\tMar 10\tMar 100\tMar Small\tMar Medium\tMar Large\tMap Bin\tMap Hand\tMap Not Bin\tMap Not Hand\tMap Not Trash\tMap Trash\tMap Trash Arm\n",
        "# 1\t61.998300\t29.889622\t0.000300\t0.001000\t0.000100\t0.000000\t0.000000\t0.000500\t0.002800\t0.011600\t0.029300\t0.000000\t0.000000\t0.030400\t0.000500\t0.000400\t0.000000\t-1.000000\t0.000000\t0.000600\t0.000000\n",
        "# 2\t11.920100\t3.913074\t0.000000\t0.000100\t0.000000\t0.000000\t0.000000\t0.000000\t0.000000\t0.000700\t0.000700\t0.000000\t0.000000\t0.000800\t0.000000\t0.000000\t0.000000\tNo Log\t0.000000\t0.000100\t0.000000\n",
        "# 3\t2.872600\t1.999650\t0.017200\t0.042100\t0.012400\t0.000000\t0.004700\t0.017500\t0.029800\t0.148200\t0.201100\t0.000000\t0.036400\t0.206600\t0.081900\t0.005500\t0.000000\t-1.000000\t0.000300\t0.015400\t0.000000\n",
        "# 4\t2.044500\t1.702604\t0.019800\t0.050300\t0.013200\t0.000000\t0.004700\t0.020800\t0.051700\t0.157200\t0.222200\t0.000000\t0.033000\t0.228900\t0.038300\t0.044200\t0.000000\t-1.000000\t0.000100\t0.036300\t0.000000\n",
        "# 5\t1.773500\t1.489281\t0.033800\t0.075200\t0.028200\t0.000000\t0.010100\t0.035100\t0.081800\t0.210300\t0.265600\t0.000000\t0.064800\t0.272200\t0.106600\t0.047500\t0.000000\tNo Log\t0.000700\t0.047800\t0.000000\n",
        "# 6\t1.651000\t1.427438\t0.055900\t0.104200\t0.057300\t0.000000\t0.012600\t0.056700\t0.117200\t0.257700\t0.323200\t0.000000\t0.072700\t0.330300\t0.178000\t0.098700\t0.000000\tNo Log\t0.000000\t0.058800\t0.000000\n",
        "# 7\t1.555300\t1.418692\t0.062700\t0.115600\t0.059800\t0.000000\t0.018700\t0.063600\t0.124000\t0.273100\t0.320400\t0.000000\t0.044300\t0.328100\t0.158000\t0.141500\t0.000000\t-1.000000\t0.000100\t0.076300\t0.000000\n",
        "# 8\t1.488100\t1.246883\t0.067600\t0.117500\t0.065600\t0.000000\t0.021800\t0.068800\t0.155800\t0.306300\t0.359700\t0.000000\t0.088600\t0.375000\t0.198000\t0.094800\t0.000000\tNo Log\t0.001500\t0.111000\t0.000000\n",
        "# 9\t1.410900\t1.337418\t0.068200\t0.128300\t0.065800\t0.000000\t0.019200\t0.068700\t0.161000\t0.282400\t0.334600\t0.000000\t0.108000\t0.340400\t0.197300\t0.139100\t0.000000\tNo Log\t0.000700\t0.072200\t0.000000\n",
        "# 10\t1.405900\t1.198891\t0.097000\t0.160900\t0.100100\t0.000000\t0.020400\t0.099000\t0.201600\t0.327300\t0.374400\t0.000000\t0.127300\t0.384000\t0.243500\t0.221500\t0.000000\tNo Log\t0.001900\t0.115300\t0.000000\n",
        "# 11\t1.339600\t1.184751\t0.093200\t0.147700\t0.100400\t0.000000\t0.023500\t0.095200\t0.199100\t0.342100\t0.393800\t0.000000\t0.089800\t0.412500\t0.233400\t0.193300\t0.000000\tNo Log\t0.005000\t0.127800\t0.000000\n",
        "# 12\t1.322600\t1.155094\t0.129100\t0.203800\t0.138700\t0.000000\t0.013500\t0.131800\t0.229300\t0.355500\t0.403500\t0.000000\t0.109100\t0.422200\t0.378100\t0.266500\t0.000000\t-1.000000\t0.006700\t0.123400\t0.000000\n",
        "# 13\t1.328200\t1.137684\t0.124100\t0.190000\t0.129300\t0.000000\t0.013200\t0.127800\t0.224100\t0.372900\t0.419500\t0.000000\t0.106800\t0.442500\t0.330900\t0.229900\t0.000000\tNo Log\t0.010100\t0.173800\t0.000000\n",
        "# 14\t1.207300\t1.071396\t0.163700\t0.243600\t0.174900\t0.000000\t0.036600\t0.167900\t0.241100\t0.390200\t0.431400\t0.000000\t0.125000\t0.457400\t0.425600\t0.336300\t0.000000\tNo Log\t0.013100\t0.207000\t0.000000\n",
        "# 15\t1.230200\t1.066224\t0.176400\t0.271200\t0.190900\t0.000000\t0.044600\t0.180000\t0.242400\t0.379000\t0.424400\t0.000000\t0.119300\t0.446900\t0.477700\t0.363600\t0.000000\tNo Log\t0.010100\t0.207200\t0.000000\n",
        "# 16\t1.216300\t1.033326\t0.212500\t0.331700\t0.217800\t0.000000\t0.050400\t0.217400\t0.256500\t0.406400\t0.438000\t0.000000\t0.331800\t0.461300\t0.539800\t0.450800\t0.000000\t-1.000000\t0.018300\t0.265900\t0.000000\n",
        "# 17\t1.102900\t0.971974\t0.242100\t0.366900\t0.271200\t0.000000\t0.061100\t0.249300\t0.283700\t0.445900\t0.472900\t0.000000\t0.286400\t0.505500\t0.601400\t0.480600\t0.000800\t-1.000000\t0.046000\t0.324100\t0.000000\n",
        "# 18\t1.118400\t0.970455\t0.259500\t0.387200\t0.285700\t0.000000\t0.092200\t0.266500\t0.292000\t0.451300\t0.478300\t0.000000\t0.301100\t0.511800\t0.607900\t0.504600\t0.000700\t-1.000000\t0.043900\t0.399800\t0.000000\n",
        "# 19\t1.042000\t0.967601\t0.259800\t0.395300\t0.279900\t0.000000\t0.076400\t0.267100\t0.287500\t0.507200\t0.530000\t0.000000\t0.220500\t0.569900\t0.596500\t0.509700\t0.002400\t-1.000000\t0.048600\t0.400700\t0.001000\n",
        "# 20\t1.064600\t0.944465\t0.273800\t0.412100\t0.298800\t0.025200\t0.087900\t0.282700\t0.291900\t0.532800\t0.558100\t0.025000\t0.266500\t0.595900\t0.630500\t0.485800\t0.002400\t-1.000000\t0.078400\t0.443900\t0.002100\n",
        "# 21\t1.028700\t0.944142\t0.281000\t0.419900\t0.313000\t0.000000\t0.085900\t0.291500\t0.334500\t0.611100\t0.634500\t0.000000\t0.323900\t0.672900\t0.650000\t0.486900\t0.004900\t-1.000000\t0.064900\t0.473900\t0.005300\n",
        "# 22\t0.997400\t0.932104\t0.284800\t0.432100\t0.309500\t0.075700\t0.082800\t0.294500\t0.307500\t0.623800\t0.642300\t0.075000\t0.280100\t0.680400\t0.662000\t0.490000\t0.005500\t-1.000000\t0.070900\t0.474300\t0.006200\n",
        "# 23\t0.976200\t0.916920\t0.296600\t0.447800\t0.331300\t0.050500\t0.106500\t0.307100\t0.440900\t0.608600\t0.625300\t0.050000\t0.296000\t0.661600\t0.652400\t0.522800\t0.005000\t-1.000000\t0.086600\t0.506100\t0.006700\n",
        "# 24\t0.910200\t0.926796\t0.302600\t0.454600\t0.333800\t0.000000\t0.100200\t0.312900\t0.394900\t0.614900\t0.632800\t0.000000\t0.296600\t0.673000\t0.676300\t0.529700\t0.004300\t-1.000000\t0.093100\t0.507500\t0.005100\n",
        "# 25\t0.856500\t0.927848\t0.298400\t0.452700\t0.333100\t0.025200\t0.102300\t0.309900\t0.305100\t0.596100\t0.628000\t0.025000\t0.294900\t0.665000\t0.659500\t0.518100\t0.004400\t-1.000000\t0.094800\t0.508600\t0.005200\n",
        "# 26\t0.775300\t0.911072\t0.299500\t0.458800\t0.331300\t0.025200\t0.104800\t0.311200\t0.309600\t0.596300\t0.625900\t0.025000\t0.329000\t0.663900\t0.669500\t0.537500\t0.004000\t-1.000000\t0.077300\t0.503100\t0.005500\n",
        "# 27\t0.750800\t0.904915\t0.304500\t0.449700\t0.336000\t0.041700\t0.116300\t0.314500\t0.365000\t0.611300\t0.642400\t0.050000\t0.311900\t0.677100\t0.681200\t0.524300\t0.004000\t-1.000000\t0.077200\t0.532100\t0.007900\n",
        "# 28\t0.719000\t0.896321\t0.302000\t0.445200\t0.328900\t0.050500\t0.105400\t0.313600\t0.402800\t0.601200\t0.632200\t0.050000\t0.341500\t0.670200\t0.690700\t0.518200\t0.005000\t-1.000000\t0.089500\t0.500900\t0.007500\n",
        "# 29\t0.696800\t0.876628\t0.312700\t0.465100\t0.339100\t0.025200\t0.110800\t0.325400\t0.324300\t0.636000\t0.651200\t0.025000\t0.314200\t0.687500\t0.702100\t0.528600\t0.004600\t-1.000000\t0.107600\t0.524600\t0.008700\n",
        "# 30\t0.673600\t0.893076\t0.306200\t0.459600\t0.340800\t0.000000\t0.105400\t0.319800\t0.397700\t0.622300\t0.637500\t0.000000\t0.248300\t0.686800\t0.684300\t0.527800\t0.005400\t-1.000000\t0.104100\t0.507100\t0.008400\n",
        "# 31\t0.656000\t0.880665\t0.317400\t0.466100\t0.355800\t0.101000\t0.127100\t0.331400\t0.407000\t0.640700\t0.653900\t0.100000\t0.283000\t0.699900\t0.715900\t0.525300\t0.007600\t-1.000000\t0.107800\t0.540300\t0.007500\n",
        "# 32\t0.641600\t0.881308\t0.319100\t0.469200\t0.351600\t0.050500\t0.116600\t0.335300\t0.419000\t0.631700\t0.646500\t0.050000\t0.302300\t0.687800\t0.694800\t0.541000\t0.008700\t-1.000000\t0.115100\t0.538300\t0.016900\n",
        "# 33\t0.630900\t0.872384\t0.321800\t0.483500\t0.348600\t0.025200\t0.118400\t0.337300\t0.375800\t0.631200\t0.642800\t0.025000\t0.321000\t0.686500\t0.713900\t0.543200\t0.006600\t-1.000000\t0.125400\t0.534800\t0.006800\n",
        "# 34\t0.601500\t0.867039\t0.322600\t0.474200\t0.365000\t0.025200\t0.122200\t0.338900\t0.374300\t0.636800\t0.654000\t0.025000\t0.390900\t0.689400\t0.714800\t0.531500\t0.006300\t-1.000000\t0.127600\t0.548800\t0.006700\n",
        "# 35\t0.589100\t0.865702\t0.325600\t0.477700\t0.359800\t0.012600\t0.124500\t0.340300\t0.419100\t0.631200\t0.640800\t0.025000\t0.342000\t0.679900\t0.713000\t0.555900\t0.008400\t-1.000000\t0.127000\t0.537900\t0.011500\n",
        "# 36\t0.570200\t0.863400\t0.329900\t0.487600\t0.367800\t0.000000\t0.120900\t0.344800\t0.413700\t0.626100\t0.635300\t0.000000\t0.330700\t0.676700\t0.732300\t0.562100\t0.008200\t-1.000000\t0.135800\t0.531900\t0.009000\n",
        "# 37\t0.572800\t0.867819\t0.334600\t0.490000\t0.375500\t0.000000\t0.124800\t0.350600\t0.406900\t0.620100\t0.634700\t0.000000\t0.353400\t0.679100\t0.724500\t0.571400\t0.011300\t-1.000000\t0.144700\t0.547700\t0.007600\n",
        "# 38\t0.551600\t0.878277\t0.328400\t0.489500\t0.356200\t0.025200\t0.115700\t0.344700\t0.419600\t0.624900\t0.637700\t0.025000\t0.373900\t0.678300\t0.715600\t0.558400\t0.006800\t-1.000000\t0.154000\t0.527100\t0.008400\n",
        "# 39\t0.544900\t0.875394\t0.327500\t0.482600\t0.367100\t0.075700\t0.130200\t0.344400\t0.379400\t0.629200\t0.641100\t0.075000\t0.383000\t0.676000\t0.723600\t0.541600\t0.008400\t-1.000000\t0.146200\t0.536900\t0.008500\n",
        "# 40\t0.526600\t0.868821\t0.330400\t0.486800\t0.360500\t0.050000\t0.143700\t0.346700\t0.380400\t0.628400\t0.638700\t0.050000\t0.355700\t0.676300\t0.723500\t0.542100\t0.011400\t-1.000000\t0.160700\t0.534000\t0.010600\n",
        "# 41\t0.517000\t0.870999\t0.335300\t0.491700\t0.364400\t0.025200\t0.114200\t0.352700\t0.433600\t0.626100\t0.634300\t0.025000\t0.331800\t0.669300\t0.731000\t0.554100\t0.012400\t-1.000000\t0.160300\t0.541300\t0.012600\n",
        "# 42\t0.495300\t0.876202\t0.334900\t0.492200\t0.367800\t0.012600\t0.123700\t0.352400\t0.455700\t0.621900\t0.637400\t0.025000\t0.359100\t0.677800\t0.728100\t0.557000\t0.018100\t-1.000000\t0.155300\t0.536000\t0.015200\n",
        "# 43\t0.494200\t0.860925\t0.337400\t0.497000\t0.368300\t0.050000\t0.122900\t0.354100\t0.426900\t0.619600\t0.634000\t0.050000\t0.375600\t0.667600\t0.736100\t0.546500\t0.015700\t-1.000000\t0.163500\t0.551300\t0.011600\n",
        "# 44\t0.490000\t0.880336\t0.336900\t0.498100\t0.373100\t0.000000\t0.118500\t0.355300\t0.449600\t0.625400\t0.633400\t0.000000\t0.312500\t0.673300\t0.728800\t0.546800\t0.027900\t-1.000000\t0.160900\t0.539500\t0.017500\n",
        "# 45\t0.476800\t0.873949\t0.338300\t0.497100\t0.370800\t0.000000\t0.122000\t0.357300\t0.448100\t0.621400\t0.632500\t0.000000\t0.343200\t0.672700\t0.724400\t0.557100\t0.022300\t-1.000000\t0.168200\t0.541900\t0.015900\n",
        "# 46\t0.476600\t0.881447\t0.335800\t0.494500\t0.368600\t0.000000\t0.119000\t0.353900\t0.455400\t0.621800\t0.637500\t0.000000\t0.305700\t0.681400\t0.729700\t0.550700\t0.020600\t-1.000000\t0.163800\t0.533800\t0.016400\n",
        "# 47\t0.468400\t0.881990\t0.334900\t0.498200\t0.366200\t0.000000\t0.119200\t0.354700\t0.449900\t0.616500\t0.630500\t0.000000\t0.325600\t0.674500\t0.723700\t0.545300\t0.022100\t-1.000000\t0.173100\t0.530000\t0.015300\n",
        "# 48\t0.463700\t0.880277\t0.337800\t0.497900\t0.371900\t0.000000\t0.122900\t0.356800\t0.460000\t0.625600\t0.633100\t0.000000\t0.322200\t0.674500\t0.724000\t0.555300\t0.021900\t-1.000000\t0.175600\t0.533600\t0.016500\n",
        "# 49\t0.458200\t0.878279\t0.338000\t0.495400\t0.369900\t0.000000\t0.123000\t0.356500\t0.461000\t0.629700\t0.637300\t0.000000\t0.336400\t0.677500\t0.727100\t0.553400\t0.018100\t-1.000000\t0.174100\t0.538700\t0.016400\n",
        "# 50\t0.453200\t0.873663\t0.340100\t0.499100\t0.372300\t0.000000\t0.122000\t0.359300\t0.462100\t0.630300\t0.639900\t0.000000\t0.373300\t0.678000\t0.729100\t0.553300\t0.028000\t-1.000000\t0.173400\t0.538800\t0.017700"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33OcJPo7KxYP"
      },
      "source": [
        "- TK - Plot loss curves\n",
        "- TK - Note: May get an error at the beginning where a box is predicted a negative output. This will break training as boxes are expected to be positive floats."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjlihLxvKxYP"
      },
      "source": [
        "# 15. Making predictions on the test dataset\n",
        "\n",
        "Notes:\n",
        "- Predicting on the whole test dataset only returns a single batch\n",
        "- e.g. `test_dataset_preds = model_v1_trainer.predict(test_dataset=processed_dataset[\"test\"])`\n",
        "- Can just predict on samples individually?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTsoBrRPKxYQ"
      },
      "outputs": [],
      "source": [
        "processed_dataset[\"test\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgDuTVtlKxYR"
      },
      "outputs": [],
      "source": [
        "# Make predictions with trainer containing trained model\n",
        "test_dataset_preds = model_v1_trainer.predict(test_dataset=processed_dataset[\"test\"])\n",
        "# test_dataset_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zr9Fb2_BKxYS"
      },
      "outputs": [],
      "source": [
        "processed_dataset[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENznTlMKKxYT"
      },
      "outputs": [],
      "source": [
        "test_dataset_preds.predictions[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsZ5YmWUKxYU"
      },
      "outputs": [],
      "source": [
        "# Get the logits\n",
        "test_pred_logits = test_dataset_preds.predictions[0][1]\n",
        "\n",
        "# Get the boxes\n",
        "test_pred_boxes = test_dataset_preds.predictions[0][2]\n",
        "\n",
        "# Get the label IDs\n",
        "test_pred_label_ids = test_dataset_preds.label_ids\n",
        "\n",
        "# Check shapes\n",
        "test_pred_logits.shape, test_pred_boxes.shape, len(test_pred_label_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJ26sHJpKxYV"
      },
      "outputs": [],
      "source": [
        "len(processed_dataset[\"test\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywncr1NRKxYW"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# Get a random sample from the test preds\n",
        "random_test_pred_index = random.randint(0, len(processed_dataset[\"test\"]))\n",
        "print(f\"[INFO] Making predictions on test item with index: {random_test_pred_index}\")\n",
        "\n",
        "random_test_sample = processed_dataset[\"test\"][random_test_pred_index]\n",
        "\n",
        "# Do a single forward pass with the model\n",
        "random_test_sample_outputs = model(pixel_values=random_test_sample[\"pixel_values\"].unsqueeze(0).to(\"cuda\"), # model expects input [batch_size, color_channels, height, width]\n",
        "                                   pixel_mask=None)\n",
        "# random_test_sample_outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbHWk9M6KxYX"
      },
      "outputs": [],
      "source": [
        "# image_processor.preprocess?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yC9ZbLRvKxYY"
      },
      "source": [
        "TK - if your predictions aren't the exact same, this is because of the randomness of machine learning, what's important is that the direction is similar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kdTLlc5KxYZ"
      },
      "outputs": [],
      "source": [
        "# Get a random sample from the test preds\n",
        "random_test_pred_index = random.randint(0, len(processed_dataset[\"test\"]))\n",
        "print(f\"[INFO] Making predictions on test item with index: {random_test_pred_index}\")\n",
        "\n",
        "random_test_sample = processed_dataset[\"test\"][random_test_pred_index]\n",
        "\n",
        "# # Do a single forward pass with the model\n",
        "random_test_sample_outputs = model(pixel_values=random_test_sample[\"pixel_values\"].unsqueeze(0).to(\"cuda\"), # model expects input [batch_size, color_channels, height, width]\n",
        "                                   pixel_mask=None)\n",
        "\n",
        "# Post process a random item from test preds\n",
        "random_test_sample_outputs_post_processed = image_processor.post_process_object_detection(\n",
        "    outputs=random_test_sample_outputs,\n",
        "    threshold=0.2, # prediction probability threshold for boxes (note: boxes from an untrained model will likely be bad)\n",
        "    target_sizes=[random_test_sample[\"labels\"][\"orig_size\"]] # original input image size (or whichever target size you'd like), required to be same number of input items in a list\n",
        ")\n",
        "\n",
        "# Plot the random sample test preds\n",
        "# Extract scores, labels and boxes\n",
        "random_test_sample_pred_scores = random_test_sample_outputs_post_processed[0][\"scores\"]\n",
        "random_test_sample_pred_labels = random_test_sample_outputs_post_processed[0][\"labels\"]\n",
        "random_test_sample_pred_boxes = half_boxes(random_test_sample_outputs_post_processed[0][\"boxes\"])\n",
        "\n",
        "# Create a list of labels to plot on the boxes\n",
        "# TK - Update the colours here\n",
        "random_test_sample_labels_to_plot = [f\"Pred: {id2label[label_pred.item()]} ({round(score_pred.item(), 4)})\"\n",
        "                  for label_pred, score_pred in zip(random_test_sample_pred_labels, random_test_sample_pred_scores)]\n",
        "\n",
        "print(f\"[INFO] Labels with scores:\")\n",
        "for label in random_test_sample_labels_to_plot:\n",
        "    print(label)\n",
        "\n",
        "# Plot the predicted boxes on the random test image\n",
        "to_pil_image(\n",
        "    pic=draw_bounding_boxes(\n",
        "        image=pil_to_tensor(pic=half_image(dataset[\"test\"][random_test_pred_index][\"image\"])),\n",
        "        boxes=random_test_sample_pred_boxes,\n",
        "        labels=random_test_sample_labels_to_plot,\n",
        "        width=3\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXYmgfTWKxYa"
      },
      "source": [
        "* TK - nice!!! these boxes look far better than our randomly predicted boxes with an untrained model...\n",
        "* TK - plot the boxes versus the ground truth (e.g. ground truth = green, predictions = red)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-itthRKKxYa"
      },
      "source": [
        "## 15.1 Predict on image from filepath"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUNes6eVKxYb"
      },
      "outputs": [],
      "source": [
        "# Pred on image from pathname\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "path_to_test_image_folder = Path(\"data/trashify_test_images\")\n",
        "test_image_filepaths = list(path_to_test_image_folder.rglob(\"*.jp*g\"))\n",
        "test_image_targ_filepath = random.choice(test_image_filepaths)\n",
        "# test_image_targ_filepath = \"data/trashify_test_images/IMG_6692.jpeg\"\n",
        "test_image_pil = Image.open(test_image_targ_filepath)\n",
        "test_image_preprocessed = image_processor.preprocess(images=test_image_pil,\n",
        "                                                     return_tensors=\"pt\")\n",
        "\n",
        "def get_image_dimensions_from_pil(image: Image.Image) -> torch.tensor:\n",
        "    \"\"\"\n",
        "    Convert the dimensions of a PIL image to a PyTorch tensor in the order (height, width).\n",
        "\n",
        "    Args:\n",
        "        image (Image.Image): The input PIL image.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: A tensor containing the height and width of the image.\n",
        "    \"\"\"\n",
        "    # Get (width, height) of image (PIL.Image.size returns width, height)\n",
        "    width, height = image.size\n",
        "\n",
        "    # Convert to a tensor in the order (height, width)\n",
        "    image_dimensions_tensor = torch.tensor([height, width])\n",
        "\n",
        "    return image_dimensions_tensor\n",
        "\n",
        "# Get image original size\n",
        "test_image_size = get_image_dimensions_from_pil(image=test_image_pil)\n",
        "\n",
        "# Make predictions on the preprocessed image\n",
        "random_test_sample_outputs = model(pixel_values=test_image_preprocessed[\"pixel_values\"].to(\"cuda\"), # model expects input [batch_size, color_channels, height, width]\n",
        "                                   pixel_mask=None)\n",
        "\n",
        "THRESHOLD = 0.2\n",
        "\n",
        "# Post process the predictions\n",
        "random_test_sample_outputs_post_processed = image_processor.post_process_object_detection(\n",
        "    outputs=random_test_sample_outputs,\n",
        "    threshold=THRESHOLD,\n",
        "    target_sizes=[test_image_size] # needs to be same length as batch dimension of the logits (e.g. [[height, width]])\n",
        ")\n",
        "\n",
        "# Extract scores, labels and boxes\n",
        "random_test_sample_pred_scores = random_test_sample_outputs_post_processed[0][\"scores\"]\n",
        "random_test_sample_pred_labels = random_test_sample_outputs_post_processed[0][\"labels\"]\n",
        "random_test_sample_pred_boxes = random_test_sample_outputs_post_processed[0][\"boxes\"]\n",
        "\n",
        "# Create a lsit of labels to plot on the boxes\n",
        "random_test_sample_labels_to_plot = [f\"Pred: {id2label[label_pred.item()]} ({round(score_pred.item(), 4)})\"\n",
        "                                     for label_pred, score_pred in zip(random_test_sample_pred_labels, random_test_sample_pred_scores)]\n",
        "\n",
        "print(\"[INFO] Labels with scores:\")\n",
        "for item in random_test_sample_labels_to_plot:\n",
        "    print(item)\n",
        "\n",
        "# Plot the predicted boxes on the random test image\n",
        "to_pil_image(\n",
        "    pic=draw_bounding_boxes(\n",
        "        image=pil_to_tensor(pic=test_image_pil),\n",
        "        boxes=random_test_sample_pred_boxes,\n",
        "        labels=random_test_sample_labels_to_plot,\n",
        "        width=3\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66qM7LDgKxYc"
      },
      "source": [
        "# 16. Upload our trained model to Hugging Face Hub\n",
        "\n",
        "TK - Let's make our model available for others to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCYCj_owKxYd"
      },
      "outputs": [],
      "source": [
        "# TK\n",
        "# Make extensions to make the model better... (e.g. data augmentation = harder training set = better overall validation loss)\n",
        "# Model with data augmentation\n",
        "# Model with longer training (e.g. 100 epochs)\n",
        "# Research eval_do_concat_batches=False/True & see what the results do..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJheAwEcKxYd"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "from datetime import datetime\n",
        "\n",
        "# TODO: update this save path so we know when the model was saved and what its parameters were\n",
        "training_epochs_ = training_args.num_train_epochs\n",
        "learning_rate_ = \"{:.0e}\".format(training_args.learning_rate)\n",
        "\n",
        "model_save_path = f\"models/learn_hf_microsoft_detr_finetuned_trashify_box_dataset_only_manual_data_no_aug_{training_epochs_}_epochs_lr_{learning_rate_}\"\n",
        "print(f\"[INFO] Saving model to: {model_save_path}\")\n",
        "model_v1_trainer.save_model(model_save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AemHqPRaKxYe"
      },
      "outputs": [],
      "source": [
        "# Push the model to the hub\n",
        "# Note: this will require you to have your Hugging Face account setup\n",
        "model_v1_trainer.push_to_hub(commit_message=\"upload trashify object detection model\",\n",
        "                    # token=None # Optional to add a token manually\n",
        "                    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yRVRCGNKxYf"
      },
      "source": [
        "# 17. Creating a demo of our model with Gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEe1MFVaKxYg"
      },
      "outputs": [],
      "source": [
        "%%writefile demos/trashify_object_detector/README.md\n",
        "---\n",
        "title: Trashify Demo V1 ðŸš®\n",
        "emoji: ðŸ—‘ï¸\n",
        "colorFrom: purple\n",
        "colorTo: blue\n",
        "sdk: gradio\n",
        "sdk_version: 4.40.0\n",
        "app_file: app.py\n",
        "pinned: false\n",
        "license: apache-2.0\n",
        "---\n",
        "\n",
        "# ðŸš® Trashify Object Detector V1\n",
        "\n",
        "Object detection demo to detect `trash`, `bin`, `hand`, `trash_arm`, `not_trash`, `not_bin`, `not_hand`.\n",
        "\n",
        "Used as example for encouraging people to cleanup their local area.\n",
        "\n",
        "If `trash`, `hand`, `bin` all detected = +1 point.\n",
        "\n",
        "## Dataset\n",
        "\n",
        "All Trashify models are trained on a custom hand-labelled dataset of people picking up trash and placing it in a bin.\n",
        "\n",
        "The dataset can be found on Hugging Face as [`mrdbourke/trashify_manual_labelled_images`](https://huggingface.co/datasets/mrdbourke/trashify_manual_labelled_images).\n",
        "\n",
        "## Demos\n",
        "\n",
        "* [V1](https://huggingface.co/spaces/mrdbourke/trashify_demo_v1) = Fine-tuned DETR model trained *without* data augmentation.\n",
        "* [V2](https://huggingface.co/spaces/mrdbourke/trashify_demo_v2) = Fine-tuned DETR model trained *with* data augmentation.\n",
        "* [V3](https://huggingface.co/spaces/mrdbourke/trashify_demo_v3) = Fine-tuned DETR model trained *with* data augmentation (same as V2) with an NMS (Non Maximum Suppression) post-processing step.\n",
        "\n",
        "TK - add links to resources to learn more"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-U6XTiKCKxYh"
      },
      "outputs": [],
      "source": [
        "%%writefile demos/trashify_object_detector/requirements.txt\n",
        "timm\n",
        "gradio\n",
        "torch\n",
        "transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1gfYzvhKxYi"
      },
      "outputs": [],
      "source": [
        "%%writefile demos/trashify_object_detector/app.py\n",
        "import gradio as gr\n",
        "import torch\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "from transformers import AutoImageProcessor\n",
        "from transformers import AutoModelForObjectDetection\n",
        "\n",
        "# Note: Can load from Hugging Face or can load from local\n",
        "model_save_path = \"mrdbourke/detr_finetuned_trashify_box_detector\"\n",
        "\n",
        "# Load the model and preprocessor\n",
        "image_processor = AutoImageProcessor.from_pretrained(model_save_path)\n",
        "model = AutoModelForObjectDetection.from_pretrained(model_save_path)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "\n",
        "# Get the id2label dictionary from the model\n",
        "id2label = model.config.id2label\n",
        "\n",
        "# Set up a colour dictionary for plotting boxes with different colours\n",
        "color_dict = {\n",
        "    \"bin\": \"green\",\n",
        "    \"trash\": \"blue\",\n",
        "    \"hand\": \"purple\",\n",
        "    \"trash_arm\": \"yellow\",\n",
        "    \"not_trash\": \"red\",\n",
        "    \"not_bin\": \"red\",\n",
        "    \"not_hand\": \"red\",\n",
        "}\n",
        "\n",
        "# Create helper functions for seeing if items from one list are in another\n",
        "def any_in_list(list_a, list_b):\n",
        "    \"Returns True if any item from list_a is in list_b, otherwise False.\"\n",
        "    return any(item in list_b for item in list_a)\n",
        "\n",
        "def all_in_list(list_a, list_b):\n",
        "    \"Returns True if all items from list_a are in list_b, otherwise False.\"\n",
        "    return all(item in list_b for item in list_a)\n",
        "\n",
        "def predict_on_image(image, conf_threshold):\n",
        "    with torch.no_grad():\n",
        "        inputs = image_processor(images=[image], return_tensors=\"pt\")\n",
        "        outputs = model(**inputs.to(device))\n",
        "\n",
        "        target_sizes = torch.tensor([[image.size[1], image.size[0]]]) # height, width\n",
        "\n",
        "        results = image_processor.post_process_object_detection(outputs,\n",
        "                                                                threshold=conf_threshold,\n",
        "                                                                target_sizes=target_sizes)[0]\n",
        "    # Return all items in results to CPU\n",
        "    for key, value in results.items():\n",
        "        try:\n",
        "            results[key] = value.item().cpu() # can't get scalar as .item() so add try/except block\n",
        "        except:\n",
        "            results[key] = value.cpu()\n",
        "\n",
        "    # Can return results as plotted on a PIL image (then display the image)\n",
        "    draw = ImageDraw.Draw(image)\n",
        "\n",
        "    # Get a font from ImageFont\n",
        "    font = ImageFont.load_default(size=20)\n",
        "\n",
        "    # Get class names as text for print out\n",
        "    class_name_text_labels = []\n",
        "\n",
        "    for box, score, label in zip(results[\"boxes\"], results[\"scores\"], results[\"labels\"]):\n",
        "        # Create coordinates\n",
        "        x, y, x2, y2 = tuple(box.tolist())\n",
        "\n",
        "        # Get label_name\n",
        "        label_name = id2label[label.item()]\n",
        "        targ_color = color_dict[label_name]\n",
        "        class_name_text_labels.append(label_name)\n",
        "\n",
        "        # Draw the rectangle\n",
        "        draw.rectangle(xy=(x, y, x2, y2),\n",
        "                       outline=targ_color,\n",
        "                       width=3)\n",
        "\n",
        "        # Create a text string to display\n",
        "        text_string_to_show = f\"{label_name} ({round(score.item(), 3)})\"\n",
        "\n",
        "        # Draw the text on the image\n",
        "        draw.text(xy=(x, y),\n",
        "                  text=text_string_to_show,\n",
        "                  fill=\"white\",\n",
        "                  font=font)\n",
        "\n",
        "    # Remove the draw each time\n",
        "    del draw\n",
        "\n",
        "    # Setup blank string to print out\n",
        "    return_string = \"\"\n",
        "\n",
        "    # Setup list of target items to discover\n",
        "    target_items = [\"trash\", \"bin\", \"hand\"]\n",
        "\n",
        "    # If no items detected or trash, bin, hand not in list, return notification\n",
        "    if (len(class_name_text_labels) == 0) or not (any_in_list(list_a=target_items, list_b=class_name_text_labels)):\n",
        "        return_string = f\"No trash, bin or hand detected at confidence threshold {conf_threshold}. Try another image or lowering the confidence threshold.\"\n",
        "        return image, return_string\n",
        "\n",
        "    # If there are some missing, print the ones which are missing\n",
        "    elif not all_in_list(list_a=target_items, list_b=class_name_text_labels):\n",
        "        missing_items = []\n",
        "        for item in target_items:\n",
        "            if item not in class_name_text_labels:\n",
        "                missing_items.append(item)\n",
        "        return_string = f\"Detected the following items: {class_name_text_labels}. But missing the following in order to get +1: {missing_items}. If this is an error, try another image or altering the confidence threshold. Otherwise, the model may need to be updated with better data.\"\n",
        "\n",
        "    # If all 3 trash, bin, hand occur = + 1\n",
        "    if all_in_list(list_a=target_items, list_b=class_name_text_labels):\n",
        "        return_string = f\"+1! Found the following items: {class_name_text_labels}, thank you for cleaning up the area!\"\n",
        "\n",
        "    print(return_string)\n",
        "\n",
        "    return image, return_string\n",
        "\n",
        "# Create the interface\n",
        "demo = gr.Interface(\n",
        "    fn=predict_on_image,\n",
        "    inputs=[\n",
        "        gr.Image(type=\"pil\", label=\"Target Image\"),\n",
        "        gr.Slider(minimum=0, maximum=1, value=0.25, label=\"Confidence Threshold\")\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Image(type=\"pil\", label=\"Image Output\"),\n",
        "        gr.Text(label=\"Text Output\")\n",
        "    ],\n",
        "    title=\"ðŸš® Trashify Object Detection Demo V1\",\n",
        "    description=\"Help clean up your local area! Upload an image and get +1 if there is all of the following items detected: trash, bin, hand.\",\n",
        "    # Examples come in the form of a list of lists, where each inner list contains elements to prefill the `inputs` parameter with\n",
        "    examples=[\n",
        "        [\"examples/trashify_example_1.jpeg\", 0.25],\n",
        "        [\"examples/trashify_example_2.jpeg\", 0.25],\n",
        "        [\"examples/trashify_example_3.jpeg\", 0.25],\n",
        "    ],\n",
        "    cache_examples=True\n",
        ")\n",
        "\n",
        "# Launch the demo\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EscdfriKxYj"
      },
      "source": [
        "## 17.1 Upload demo to Hugging Face Spaces to get it live"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCmU5jS5KxYk"
      },
      "outputs": [],
      "source": [
        "# 1. Import the required methods for uploading to the Hugging Face Hub\n",
        "from huggingface_hub import (\n",
        "    create_repo,\n",
        "    get_full_repo_name,\n",
        "    upload_file, # for uploading a single file (if necessary)\n",
        "    upload_folder # for uploading multiple files (in a folder)\n",
        ")\n",
        "\n",
        "# 2. Define the parameters we'd like to use for the upload\n",
        "LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD = \"demos/trashify_object_detector\" # TK - update this path\n",
        "HF_TARGET_SPACE_NAME = \"trashify_demo_v1\"\n",
        "HF_REPO_TYPE = \"space\" # we're creating a Hugging Face Space\n",
        "HF_SPACE_SDK = \"gradio\"\n",
        "HF_TOKEN = \"\" # optional: set to your Hugging Face token (but I'd advise storing this as an environment variable as previously discussed)\n",
        "\n",
        "# 3. Create a Space repository on Hugging Face Hub\n",
        "print(f\"[INFO] Creating repo on Hugging Face Hub with name: {HF_TARGET_SPACE_NAME}\")\n",
        "create_repo(\n",
        "    repo_id=HF_TARGET_SPACE_NAME,\n",
        "    # token=HF_TOKEN, # optional: set token manually (though it will be automatically recognized if it's available as an environment variable)\n",
        "    repo_type=HF_REPO_TYPE,\n",
        "    private=False, # set to True if you don't want your Space to be accessible to others\n",
        "    space_sdk=HF_SPACE_SDK,\n",
        "    exist_ok=True, # set to False if you want an error to raise if the repo_id already exists\n",
        ")\n",
        "\n",
        "# 4. Get the full repository name (e.g. {username}/{model_id} or {username}/{space_name})\n",
        "full_hf_repo_name = get_full_repo_name(model_id=HF_TARGET_SPACE_NAME)\n",
        "print(f\"[INFO] Full Hugging Face Hub repo name: {full_hf_repo_name}\")\n",
        "\n",
        "# 5. Upload our demo folder\n",
        "print(f\"[INFO] Uploading {LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD} to repo: {full_hf_repo_name}\")\n",
        "folder_upload_url = upload_folder(\n",
        "    repo_id=full_hf_repo_name,\n",
        "    folder_path=LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD,\n",
        "    path_in_repo=\".\", # upload our folder to the root directory (\".\" means \"base\" or \"root\", this is the default)\n",
        "    # token=HF_TOKEN, # optional: set token manually\n",
        "    repo_type=HF_REPO_TYPE,\n",
        "    commit_message=\"Uploading Trashify box detection model app.py\"\n",
        ")\n",
        "print(f\"[INFO] Demo folder successfully uploaded with commit URL: {folder_upload_url}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTUWWW1qKxYl"
      },
      "source": [
        "TK - see the demo here: https://huggingface.co/spaces/mrdbourke/trashify_demo_v1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DRSFPlpKxYm"
      },
      "source": [
        "## 17.2 Testing the hosted demo\n",
        "\n",
        "> Add blockquote\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuDAfjqTKxYm"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "\n",
        "# You can get embeddable HTML code for your demo by clicking the \"Embed\" button on the demo page\n",
        "HTML(data='''\n",
        "<iframe\n",
        "    src=\"https://mrdbourke-trashify-demo-v1.hf.space\"\n",
        "    frameborder=\"0\"\n",
        "    width=\"850\"\n",
        "    height=\"1000\"\n",
        "></iframe>\n",
        "''')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE1p6lDdKxYo"
      },
      "source": [
        "# 18. Improve our model with data augmentation\n",
        "\n",
        "UPTOHERE\n",
        "- Read for object detection augmentation (keep it simple)\n",
        "- Check out the papers for detection augmentation\n",
        "- Train a model with data augmentation\n",
        "    - Compare the model's metrics between data augmentation and no data augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVWooIxVKxYp"
      },
      "source": [
        "## 18.1 Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U92DF1h2KxYq"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# load_dataset?\n",
        "dataset = load_dataset(path=\"mrdbourke/trashify_manual_labelled_images\")\n",
        "\n",
        "print(f\"[INFO] Length of original dataset: {len(dataset['train'])}\")\n",
        "\n",
        "# Split the data\n",
        "dataset_split = dataset[\"train\"].train_test_split(test_size=0.3, seed=42) # split the dataset into 70/30 train/test\n",
        "dataset_test_val_split = dataset_split[\"test\"].train_test_split(test_size=0.6, seed=42) # split the test set into 40/60 validation/test\n",
        "\n",
        "# Create splits\n",
        "dataset[\"train\"] = dataset_split[\"train\"]\n",
        "dataset[\"validation\"] = dataset_test_val_split[\"train\"]\n",
        "dataset[\"test\"] = dataset_test_val_split[\"test\"]\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3Iki-9NKxYr"
      },
      "outputs": [],
      "source": [
        "# Get the categories from the dataset\n",
        "# Note: this requires the dataset to have been uploaded with this feature setup\n",
        "categories = dataset[\"train\"].features[\"annotations\"].feature[\"category_id\"]\n",
        "\n",
        "# Get the names attribute\n",
        "categories.names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gj2cvPJdKxYs"
      },
      "outputs": [],
      "source": [
        "id2label = {i: class_name for i, class_name in enumerate(categories.names)}\n",
        "label2id = {value: key for key, value in id2label.items()}\n",
        "\n",
        "id2label, label2id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okfB91jSKxYs"
      },
      "outputs": [],
      "source": [
        "# View a random sample\n",
        "import random\n",
        "random_idx = random.randint(0, len(dataset[\"train\"]))\n",
        "random_sample = dataset[\"train\"][random_idx]\n",
        "random_sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AetEIEneKxYt"
      },
      "source": [
        "## 18.2 Setup model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Nymb0VyKxYu"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForObjectDetection, AutoImageProcessor\n",
        "\n",
        "# Model config - https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrConfig\n",
        "# Model docs - https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrModel\n",
        "MODEL_NAME = \"microsoft/conditional-detr-resnet-50\"\n",
        "\n",
        "# Set image size\n",
        "IMAGE_SIZE = 640 # other common image sizes include: 300x300, 480x480, 512x512, 640x640, 800x800 (best to experiment and see which works best)\n",
        "\n",
        "# Get the image processor (this is required for prepraring images)\n",
        "# See docs: https://huggingface.co/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrImageProcessor.preprocess\n",
        "image_processor = AutoImageProcessor.from_pretrained(\n",
        "    pretrained_model_name_or_path=MODEL_NAME,\n",
        "    format=\"coco_detection\", # this is the default\n",
        "    do_convert_annotations=True, # defaults to True, converts boxes to (center_x, center_y, width, height)\n",
        "    size={\"shortest_edge\": IMAGE_SIZE, \"longest_edge\": IMAGE_SIZE},\n",
        "    max_size=None # Note: this parameter is deprecated and will produce a warning if used during processing.\n",
        ")\n",
        "\n",
        "# Check out the image processor\n",
        "image_processor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOnGB3QPKxYv"
      },
      "outputs": [],
      "source": [
        "# First create a couple of dataclasses to store our data format\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import List, Tuple\n",
        "\n",
        "@dataclass\n",
        "class SingleCOCOAnnotation:\n",
        "    \"An instance of a single COCO annotation. See COCO format: https://cocodataset.org/#format-data\"\n",
        "    image_id: int\n",
        "    category_id: int\n",
        "    bbox: List[float] # bboxes in format [x_top_left, y_top_left, width, height]\n",
        "    area: float = 0.0\n",
        "    iscrowd: int = 0\n",
        "\n",
        "@dataclass\n",
        "class ImageCOCOAnnotations:\n",
        "    \"A collection of COCO annotations for a given image_id.\"\n",
        "    image_id: int\n",
        "    annotations: List[SingleCOCOAnnotation]\n",
        "\n",
        "def format_image_annotations_as_coco(\n",
        "        image_id: int,\n",
        "        categories: List[int],\n",
        "        areas: List[float],\n",
        "        bboxes: List[Tuple[float, float, float, float]] # bboxes in format\n",
        ") -> dict:\n",
        "    # Turn input lists into a list of dicts\n",
        "    coco_format_annotations = [\n",
        "        asdict(SingleCOCOAnnotation(\n",
        "            image_id=image_id,\n",
        "            category_id=category,\n",
        "            bbox=list(bbox),\n",
        "            area=area,\n",
        "        ))\n",
        "        for category, area, bbox in zip(categories, areas, bboxes)\n",
        "    ]\n",
        "\n",
        "    # Return dictionary of annotations with format {\"image_id\": ..., \"annotations\": ...}\n",
        "    return asdict(ImageCOCOAnnotations(image_id=image_id,\n",
        "                                       annotations=coco_format_annotations))\n",
        "\n",
        "# Let's try it out\n",
        "image_id = 0\n",
        "random_idx = random.randint(0, len(dataset[\"train\"]))\n",
        "random_sample = dataset[\"train\"][random_idx]\n",
        "random_sample_categories = random_sample[\"annotations\"][\"category_id\"]\n",
        "random_sample_areas = random_sample[\"annotations\"][\"area\"]\n",
        "random_sample_bboxes = random_sample[\"annotations\"][\"bbox\"]\n",
        "\n",
        "random_sample_coco_annotations = format_image_annotations_as_coco(image_id=image_id,\n",
        "                                                                  categories=random_sample_categories,\n",
        "                                                                  areas=random_sample_areas,\n",
        "                                                                  bboxes=random_sample_bboxes)\n",
        "random_sample_coco_annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkYMYbTCKxYw"
      },
      "outputs": [],
      "source": [
        "# Setup the model\n",
        "# TODO: Can functionize this to create a base model (e.g. a model with all the base settings/untrained weights)\n",
        "def create_model():\n",
        "    model = AutoModelForObjectDetection.from_pretrained(\n",
        "                pretrained_model_name_or_path=MODEL_NAME,\n",
        "                label2id=label2id,\n",
        "                id2label=id2label,\n",
        "                ignore_mismatched_sizes=True,\n",
        "                backbone=\"resnet50\")\n",
        "    return model\n",
        "\n",
        "model_aug = create_model()\n",
        "model_aug"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7edepYpKxYx"
      },
      "source": [
        "## 18.3 Setup and visualize transforms (augmentations)\n",
        "\n",
        "* TK - explain simple augmentations:\n",
        "    * RandomHorizontalFlip\n",
        "    * ColorJitter\n",
        "        * That's it...\n",
        "        * Tailor the data augmentations to your own dataset/problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgoyQ57PKxYy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torchvision.transforms import v2\n",
        "from torchvision.transforms.v2.functional import to_pil_image, pil_to_tensor, pad\n",
        "from torchvision.utils import draw_bounding_boxes\n",
        "\n",
        "# Optional transform from here: https://arxiv.org/pdf/2012.07177\n",
        "# Scale jitter -> pad -> resize\n",
        "\n",
        "train_transforms = v2.Compose([\n",
        "    v2.ToImage(),\n",
        "    # v2.RandomResizedCrop(size=(640, 640), antialias=True),\n",
        "    # v2.Resize(size=(640, 640)),\n",
        "    # v2.RandomShortestSize(min_size=480, max_size=640),\n",
        "    # v2.ScaleJitter(target_size=(640, 640)),\n",
        "    # PadToSize(target_height=640, target_width=640),\n",
        "    v2.RandomHorizontalFlip(p=0.5),\n",
        "    # v2.RandomPhotometricDistort(p=0.75),\n",
        "    # v2.RandomShortestSize(min_size=480, max_size=640),\n",
        "    # v2.Resize(size=(640, 640)),\n",
        "    v2.ColorJitter(brightness=0.75, # randomly adjust the brightness\n",
        "                   contrast=0.75), # randomly alter the contrast\n",
        "    # v2.RandomPerspective(distortion_scale=0.3,\n",
        "    #                      p=0.3,\n",
        "    #                      fill=(123, 117, 104)), # fill with average colour\n",
        "    # v2.RandomZoomOut(side_range=(1.0, 1.5),\n",
        "    #                  fill=(123, 117, 104)),\n",
        "    v2.ToDtype(dtype=torch.float32, scale=True),\n",
        "\n",
        "    # v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    # sanitize boxes, recommended to be called at least once at the end of the transform pipeline\n",
        "    # https://pytorch.org/vision/stable/generated/torchvision.transforms.v2.SanitizeBoundingBoxes.html#torchvision.transforms.v2.SanitizeBoundingBoxes\n",
        "    v2.SanitizeBoundingBoxes(labels_getter=None)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NCdW0jZKxYz"
      },
      "source": [
        "## 18.4 Visualize transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5H6EdxvKxY0"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "random_idx = random.randint(0, len(dataset[\"train\"]))\n",
        "random_sample = dataset[\"train\"][random_idx]\n",
        "\n",
        "# Perform transform on image\n",
        "random_sample_image = random_sample[\"image\"]\n",
        "random_sample_image_width, random_sample_image_height = random_sample[\"image\"].size\n",
        "random_sample_boxes_xywh = random_sample[\"annotations\"][\"bbox\"] # these are in XYWH format\n",
        "random_sample_boxes_xyxy = torchvision.ops.box_convert(boxes=torch.tensor(random_sample_boxes_xywh),\n",
        "                                                       in_fmt=\"xywh\",\n",
        "                                                       out_fmt=\"xyxy\")\n",
        "\n",
        "# Format boxes to be xyxy for transforms\n",
        "random_sample_boxes_xyxy = torchvision.tv_tensors.BoundingBoxes(\n",
        "    data=random_sample_boxes_xyxy,\n",
        "    format=\"XYXY\",\n",
        "    canvas_size=(random_sample_image_height, random_sample_image_width) # comes in the form height, width\n",
        ")\n",
        "\n",
        "random_sample_image_transformed, random_sample_boxes_transformed = train_transforms(random_sample_image,\n",
        "                                                                                    random_sample_boxes_xyxy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-HQqTkZKxY0"
      },
      "outputs": [],
      "source": [
        "random_sample_original_image_with_boxes = to_pil_image(pic=draw_bounding_boxes(\n",
        "                                                       image=pil_to_tensor(pic=random_sample_image),\n",
        "                                                       boxes=random_sample_boxes_xyxy,\n",
        "                                                       labels=None,\n",
        "                                                       width=3))\n",
        "random_sample_original_image_with_boxes_size = (random_sample_original_image_with_boxes.size[1], random_sample_original_image_with_boxes.size[0])\n",
        "\n",
        "# Plot the predicted boxes on the random test image\n",
        "random_sample_transformed_image_with_boxes = to_pil_image(pic=draw_bounding_boxes(\n",
        "                                                          image=random_sample_image_transformed,\n",
        "                                                          boxes=random_sample_boxes_transformed,\n",
        "                                                          labels=None,\n",
        "                                                          width=3))\n",
        "random_sample_transformed_image_with_boxes_size = (random_sample_transformed_image_with_boxes.size[1], random_sample_transformed_image_with_boxes.size[0])\n",
        "\n",
        "# Visualize the transformed image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a figure with two subplots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "# Display image 1\n",
        "axes[0].imshow(random_sample_original_image_with_boxes)\n",
        "axes[0].axis(\"off\")  # Hide axes\n",
        "axes[0].set_title(f\"Original Image | Size: {random_sample_original_image_with_boxes_size} (hxw)\")\n",
        "\n",
        "# Display image 2\n",
        "axes[1].imshow(random_sample_transformed_image_with_boxes)\n",
        "axes[1].axis(\"off\")  # Hide axes\n",
        "axes[1].set_title(f\"Transformed Image | Size: {random_sample_transformed_image_with_boxes_size} (hxw)\")\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWg9AGULKxY1"
      },
      "source": [
        "## 18.5 Create function to preprocess and transform batch of examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJrxjDo-KxY2"
      },
      "outputs": [],
      "source": [
        "from torchvision import tv_tensors\n",
        "\n",
        "def preprocess_and_transform_batch(examples,\n",
        "                                   image_processor,\n",
        "                                   transforms=None # Note: Could optionally add transforms (e.g. data augmentation) here\n",
        "                                   ):\n",
        "    \"\"\"\n",
        "    Function to preprocess batches of data.\n",
        "\n",
        "    Can optionally apply a transform later on.\n",
        "    \"\"\"\n",
        "    images = []\n",
        "\n",
        "    coco_annotations = []\n",
        "\n",
        "    for image, image_id, annotations_dict in zip(examples[\"image\"], examples[\"image_id\"], examples[\"annotations\"]):\n",
        "        # Note: may need to open image if it is an image path rather than PIL.Image\n",
        "        bbox_list = annotations_dict[\"bbox\"]\n",
        "        category_list = annotations_dict[\"category_id\"]\n",
        "        area_list = annotations_dict[\"area\"]\n",
        "\n",
        "        # Note: Could optionally apply a transform here.\n",
        "        if transforms:\n",
        "            width, height = image.size[0], image.size[1]\n",
        "            bbox_list = tv_tensors.BoundingBoxes(data=torch.tensor(bbox_list),\n",
        "                                                 format=\"XYWH\",\n",
        "                                                 canvas_size=(height, width)) # canvas_size = height, width\n",
        "            image, bbox_list = transforms(image,\n",
        "                                          bbox_list)\n",
        "\n",
        "        # Format the annotations into COCO format\n",
        "        cooc_format_annotations = format_image_annotations_as_coco(image_id=image_id,\n",
        "                                                                   categories=category_list,\n",
        "                                                                   areas=area_list,\n",
        "                                                                   bboxes=bbox_list)\n",
        "\n",
        "        # Add images/annotations to their respective lists\n",
        "        images.append(image)\n",
        "        coco_annotations.append(cooc_format_annotations)\n",
        "\n",
        "\n",
        "    # Apply the image processor to lists of images and annotations\n",
        "    preprocessed_batch = image_processor.preprocess(images=images,\n",
        "                                                    annotations=coco_annotations,\n",
        "                                                    return_tensors=\"pt\",\n",
        "                                                    do_rescale=False if transforms else True,\n",
        "                                                    do_resize=True,\n",
        "                                                    do_pad=True)\n",
        "\n",
        "    return preprocessed_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2EpmOywKxY3"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "\n",
        "# Make a transform for different splits\n",
        "train_transform_batch = partial(\n",
        "    preprocess_and_transform_batch,\n",
        "    transforms=train_transforms,\n",
        "    image_processor=image_processor\n",
        ")\n",
        "\n",
        "validation_transform_batch = partial(\n",
        "    preprocess_and_transform_batch,\n",
        "    transforms=None,\n",
        "    image_processor=image_processor\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQchXB8nKxY4"
      },
      "outputs": [],
      "source": [
        "processed_dataset = dataset.copy()\n",
        "processed_dataset[\"train\"] = dataset[\"train\"].with_transform(train_transform_batch)\n",
        "processed_dataset[\"validation\"] = dataset[\"validation\"].with_transform(validation_transform_batch)\n",
        "processed_dataset[\"test\"] = dataset[\"test\"].with_transform(validation_transform_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcqLh7CzKxY5"
      },
      "outputs": [],
      "source": [
        "# Create data_collate_function to collect samples into batches\n",
        "# TK - want to get a dictionary of {\"pixel_mask\": [batch_of_samples], \"labels\": [batch_of_samples], \"pixel_mask\": [batch_of_samples]}\n",
        "def data_collate_function(batch):\n",
        "    collated_data = {}\n",
        "\n",
        "    # Stack together a collection of pixel_values tensors\n",
        "    collated_data[\"pixel_values\"] = torch.stack([sample[\"pixel_values\"] for sample in batch])\n",
        "\n",
        "    # Get the labels (these are dictionaries so no need to use torch.stack)\n",
        "    collated_data[\"labels\"] = [sample[\"labels\"] for sample in batch]\n",
        "\n",
        "    # If there is a pixel_mask key, return the pixel_mask's as well\n",
        "    if \"pixel_mask\" in batch[0]:\n",
        "        collated_data[\"pixel_mask\"] = torch.stack([sample[\"pixel_mask\"] for sample in batch])\n",
        "\n",
        "    return collated_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UctPzqz9KxY6"
      },
      "outputs": [],
      "source": [
        "model_aug = create_model()\n",
        "model_aug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFJJOa7DKxY7"
      },
      "outputs": [],
      "source": [
        "# Note: Depending on the size/speed of your GPU, this may take a while\n",
        "\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# Set the batch size according to the memory you have available on your GPU\n",
        "# e.g. on my NVIDIA RTX 4090 with 24GB of VRAM, I can use a batch size of 32 without running out of memory\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "# Disable warnings about `max_size` parameter being deprecated (this is okay)\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\"The `max_size` parameter is deprecated*\")\n",
        "\n",
        "# Note: AdamW Optimizer is used by default\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"detr_finetuned_trashify_box_detector_with_data_aug\", # Tk - make sure this is suitable for data aug model\n",
        "    num_train_epochs=25,\n",
        "    fp16=True,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    learning_rate=1e-4,\n",
        "    lr_scheduler_type=\"linear\", # default = \"linear\", can try others such as \"cosine\", \"constant\" etc\n",
        "    weight_decay=1e-4,\n",
        "    max_grad_norm=0.01,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    remove_unused_columns=False,\n",
        "    report_to=\"none\", # don't save experiments to a third party service\n",
        "    dataloader_num_workers=4,\n",
        "    warmup_ratio=0.05,\n",
        "    push_to_hub=False,\n",
        "    eval_do_concat_batches=False\n",
        ")\n",
        "\n",
        "model_v2_trainer = Trainer(\n",
        "    model=model_aug,\n",
        "    args=training_args,\n",
        "    train_dataset=processed_dataset[\"train\"],\n",
        "    eval_dataset=processed_dataset[\"validation\"],\n",
        "    tokenizer=image_processor,\n",
        "    data_collator=data_collate_function,\n",
        "    # compute_metrics=None # TODO: add a metrics function, just see if model trains first\n",
        ")\n",
        "\n",
        "model_v2_results = model_v2_trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeLkW0uZKxY8"
      },
      "source": [
        "TK - Note: You might get the following issue (negative bounding box coordinate predictions), can try again for more stable predictions (predictions are inherently random to begin with) or use a learning rate warmup to help stabilize predictions:\n",
        "\n",
        "> ValueError: boxes1 must be in [x0, y0, x1, y1] (corner) format, but got tensor([[ 0.5796,  0.5566,  0.9956,  0.9492],\n",
        "        [ 0.5718,  0.0610,  0.7202,  0.1738],\n",
        "        [ 0.8218,  0.5107,  0.9878,  0.6289],\n",
        "        ...,\n",
        "        [ 0.1379,  0.1403,  0.6709,  0.6138],\n",
        "        [ 0.7471,  0.4319,  1.0088,  0.5864],\n",
        "        [-0.0660,  0.2052,  0.2067,  0.5107]], device='cuda:0',\n",
        "       dtype=torch.float16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gp-flYIkKxY8"
      },
      "source": [
        "## 18.6 Save the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jb5lAvoyKxY9"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "from datetime import datetime\n",
        "\n",
        "# TODO: update this save path so we know when the model was saved and what its parameters were\n",
        "training_epochs_ = training_args.num_train_epochs\n",
        "learning_rate_ = \"{:.0e}\".format(training_args.learning_rate)\n",
        "\n",
        "model_v2_save_path = f\"models/learn_hf_microsoft_detr_finetuned_trashify_box_dataset_only_manual_data_with_aug_{training_epochs_}_epochs_lr_{learning_rate_}\"\n",
        "print(f\"[INFO] Saving model to: {model_v2_save_path}\")\n",
        "model_v2_trainer.save_model(model_v2_save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhJraslKKxY-"
      },
      "source": [
        "# 19. Upload Augmentation Model to Hugging Face Hub\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTbC1fbJKxY_"
      },
      "outputs": [],
      "source": [
        "# Push the model to the Hugging Face Hub\n",
        "# TK Note: This will require you to have your Hugging Face account setup (e.g. see the setup guide, tk - link to setup guide)\n",
        "# TK - this will push to the parameter `output_dir=\"detr_finetuned_trashify_box_detector_with_data_aug\"`\n",
        "model_v2_trainer.push_to_hub(commit_message=\"upload trashify object detection model with data augmentation\"\n",
        "                             # token=None, # Optional to add token manually\n",
        "                            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjvzQF8YKxY_"
      },
      "source": [
        "# 20. Compare results of different models\n",
        "\n",
        "UPTOHERE\n",
        "- Showcase model 2 doing better because of augmentation (harder to learn)\n",
        "\n",
        "* TK - Compare v1 model to v2\n",
        "    * TK - Get model_v1 results into a variable and save it for later\n",
        "    * Compare both of these as plots against each other, e.g. have the training curves for aug/no_aug on one plot and the curves for validation data for aug/no_aug on another plot\n",
        "* TK - offer extensions to improve the model\n",
        "    * TK - training model for longer, potentially using synthetic data...?\n",
        "        * TK - could I use 1000 high quality synthetic data samples to improve our model?\n",
        "    * TK - try use a different learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXc7XbD8KxZA"
      },
      "outputs": [],
      "source": [
        "# TK - Turn this workflow into a function e.g. def get_history_from_trainer() -> df/dict of history\n",
        "def get_history_metrics_from_trainer(trainer):\n",
        "    trainer_history = trainer.state.log_history\n",
        "    trainer_history_metrics = trainer_history[:-1] # get everything except the training time metrics (we've seen these already)\n",
        "    trainer_history_training_time = trainer_history[-1]\n",
        "\n",
        "    model_train_loss = [item[\"loss\"] for item in trainer_history_metrics if \"loss\" in item.keys()]\n",
        "    model_eval_loss = [item[\"eval_loss\"] for item in trainer_history_metrics if \"eval_loss\" in item.keys()]\n",
        "    model_learning_rate = [item[\"learning_rate\"] for item in trainer_history_metrics if \"learning_rate\" in item.keys()]\n",
        "\n",
        "    return model_train_loss, model_eval_loss, model_learning_rate, trainer_history_training_time\n",
        "\n",
        "model_v1_train_loss, model_v1_eval_loss, model_v1_learning_rate, _ = get_history_metrics_from_trainer(trainer=model_v1_trainer)\n",
        "model_v2_train_loss, model_v2_eval_loss, model_v2_learning_rate, _ = get_history_metrics_from_trainer(trainer=model_v2_trainer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfJe9u-LKxZB"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot model loss curves against each other for same model\n",
        "# Note: Start from index 1 onwards to remove large loss spike at beginning of training\n",
        "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 6))\n",
        "ax[0].plot(model_v1_train_loss[1:], label=\"Model V1 Train Loss\")\n",
        "ax[0].plot(model_v1_eval_loss[1:], label=\"Model V1 Eval Loss\")\n",
        "ax[0].set_title(\"Model V1 Loss Curves\")\n",
        "ax[0].set_ylabel(\"Loss\")\n",
        "ax[0].set_xlabel(\"Epoch\")\n",
        "ax[0].legend()\n",
        "\n",
        "ax[1].plot(model_v2_train_loss[1:], label=\"Model V2 Train Loss\")\n",
        "ax[1].plot(model_v2_eval_loss[1:], label=\"Model V2 Eval Loss\")\n",
        "ax[1].set_title(\"Model V2 Loss Curves\")\n",
        "ax[1].set_ylabel(\"Loss\")\n",
        "ax[1].set_xlabel(\"Epoch\")\n",
        "ax[1].legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Z6wdYxiKxZC"
      },
      "source": [
        "tk - notice the overfitting begin to happen with model v1 (no data augmentation) but model v2 has less overfitting and achieves a lower validation loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wt2Dku1uKxZD"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(model_v1_learning_rate, label=\"Model V1\")\n",
        "plt.plot(model_v2_learning_rate, label=\"Model V2\")\n",
        "plt.title(\"Model Learning Rate vs. Epoch\")\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwO4n8w5KxZE"
      },
      "outputs": [],
      "source": [
        "# Plot loss values against each other\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 6))\n",
        "num_epochs = range(0, len(model_v1_train_loss))\n",
        "ax[0].plot(model_v1_train_loss[1:], label=\"Model 1 Training Loss\")\n",
        "ax[0].plot(model_v2_train_loss[1:], label=\"Model 2 Training Loss\")\n",
        "ax[0].set_title(\"Model Training Loss Curves\")\n",
        "ax[0].set_ylabel(\"Training Loss\")\n",
        "ax[0].set_xlabel(\"Epochs\")\n",
        "ax[0].legend()\n",
        "\n",
        "ax[1].plot(model_v1_eval_loss[1:], label=\"Model 1 Eval Loss\")\n",
        "ax[1].plot(model_v2_eval_loss[1:], label=\"Model 2 Eval Loss\")\n",
        "ax[1].set_title(\"Model Eval Loss Curves\")\n",
        "ax[1].set_ylabel(\"Eval Loss\")\n",
        "ax[1].set_xlabel(\"Epochs\")\n",
        "ax[1].legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTEjGxrmKxZF"
      },
      "source": [
        "tk - describe the loss curves here, model 2 curves may be higher for training loss but they really start to accelerate on the evaluation set towards the end"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kktmadCeKxZF"
      },
      "source": [
        "# 21. Create demo with Augmentation Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bF8cq74vKxZG"
      },
      "outputs": [],
      "source": [
        "# Make directory for demo\n",
        "from pathlib import Path\n",
        "\n",
        "trashify_data_aug_model_dir = Path(\"demos/trashify_object_detector_data_aug_model/\")\n",
        "trashify_data_aug_model_dir.mkdir(exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A10On5B4KxZH"
      },
      "outputs": [],
      "source": [
        "%%writefile demos/trashify_object_detector_data_aug_model/README.md\n",
        "---\n",
        "title: Trashify Demo V2 ðŸš®\n",
        "emoji: ðŸ—‘ï¸\n",
        "colorFrom: purple\n",
        "colorTo: blue\n",
        "sdk: gradio\n",
        "sdk_version: 4.40.0\n",
        "app_file: app.py\n",
        "pinned: false\n",
        "license: apache-2.0\n",
        "---\n",
        "\n",
        "# ðŸš® Trashify Object Detector Demo V2\n",
        "\n",
        "Object detection demo to detect `trash`, `bin`, `hand`, `trash_arm`, `not_trash`, `not_bin`, `not_hand`.\n",
        "\n",
        "Used as example for encouraging people to cleanup their local area.\n",
        "\n",
        "If `trash`, `hand`, `bin` all detected = +1 point.\n",
        "\n",
        "## Dataset\n",
        "\n",
        "All Trashify models are trained on a custom hand-labelled dataset of people picking up trash and placing it in a bin.\n",
        "\n",
        "The dataset can be found on Hugging Face as [`mrdbourke/trashify_manual_labelled_images`](https://huggingface.co/datasets/mrdbourke/trashify_manual_labelled_images).\n",
        "\n",
        "## Demos\n",
        "\n",
        "* [V1](https://huggingface.co/spaces/mrdbourke/trashify_demo_v1) = Fine-tuned DETR model trained *without* data augmentation.\n",
        "* [V2](https://huggingface.co/spaces/mrdbourke/trashify_demo_v2) = Fine-tuned DETR model trained *with* data augmentation.\n",
        "* [V3](https://huggingface.co/spaces/mrdbourke/trashify_demo_v3) = Fine-tuned DETR model trained *with* data augmentation (same as V2) with an NMS (Non Maximum Suppression) post-processing step.\n",
        "\n",
        "TK - finish the README.md + update with links to materials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPeykjPnKxZJ"
      },
      "outputs": [],
      "source": [
        "%%writefile demos/trashify_object_detector_data_aug_model/requirements.txt\n",
        "timm\n",
        "gradio\n",
        "torch\n",
        "transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJAbs4RhKxZK"
      },
      "outputs": [],
      "source": [
        "%%writefile demos/trashify_object_detector_data_aug_model/app.py\n",
        "import gradio as gr\n",
        "import torch\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "from transformers import AutoImageProcessor\n",
        "from transformers import AutoModelForObjectDetection\n",
        "\n",
        "# Note: Can load from Hugging Face or can load from local.\n",
        "# You will have to replace {mrdbourke} for your own username if the model is on your Hugging Face account.\n",
        "model_save_path = \"HimanshuGoyal2004/detr_finetuned_trashify_box_detector_with_data_aug\"\n",
        "\n",
        "# Load the model and preprocessor\n",
        "image_processor = AutoImageProcessor.from_pretrained(model_save_path)\n",
        "model = AutoModelForObjectDetection.from_pretrained(model_save_path)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "\n",
        "# Get the id2label dictionary from the model\n",
        "id2label = model.config.id2label\n",
        "\n",
        "# Set up a colour dictionary for plotting boxes with different colours\n",
        "color_dict = {\n",
        "    \"bin\": \"green\",\n",
        "    \"trash\": \"blue\",\n",
        "    \"hand\": \"purple\",\n",
        "    \"trash_arm\": \"yellow\",\n",
        "    \"not_trash\": \"red\",\n",
        "    \"not_bin\": \"red\",\n",
        "    \"not_hand\": \"red\",\n",
        "}\n",
        "\n",
        "# Create helper functions for seeing if items from one list are in another\n",
        "def any_in_list(list_a, list_b):\n",
        "    \"Returns True if any item from list_a is in list_b, otherwise False.\"\n",
        "    return any(item in list_b for item in list_a)\n",
        "\n",
        "def all_in_list(list_a, list_b):\n",
        "    \"Returns True if all items from list_a are in list_b, otherwise False.\"\n",
        "    return all(item in list_b for item in list_a)\n",
        "\n",
        "def predict_on_image(image, conf_threshold):\n",
        "    with torch.no_grad():\n",
        "        inputs = image_processor(images=[image], return_tensors=\"pt\")\n",
        "        outputs = model(**inputs.to(device))\n",
        "\n",
        "        target_sizes = torch.tensor([[image.size[1], image.size[0]]]) # height, width\n",
        "\n",
        "        results = image_processor.post_process_object_detection(outputs,\n",
        "                                                                threshold=conf_threshold,\n",
        "                                                                target_sizes=target_sizes)[0]\n",
        "    # Return all items in results to CPU\n",
        "    for key, value in results.items():\n",
        "        try:\n",
        "            results[key] = value.item().cpu() # can't get scalar as .item() so add try/except block\n",
        "        except:\n",
        "            results[key] = value.cpu()\n",
        "\n",
        "    # Can return results as plotted on a PIL image (then display the image)\n",
        "    draw = ImageDraw.Draw(image)\n",
        "\n",
        "    # Get a font from ImageFont\n",
        "    font = ImageFont.load_default(size=20)\n",
        "\n",
        "    # Get class names as text for print out\n",
        "    class_name_text_labels = []\n",
        "\n",
        "    for box, score, label in zip(results[\"boxes\"], results[\"scores\"], results[\"labels\"]):\n",
        "        # Create coordinates\n",
        "        x, y, x2, y2 = tuple(box.tolist())\n",
        "\n",
        "        # Get label_name\n",
        "        label_name = id2label[label.item()]\n",
        "        targ_color = color_dict[label_name]\n",
        "        class_name_text_labels.append(label_name)\n",
        "\n",
        "        # Draw the rectangle\n",
        "        draw.rectangle(xy=(x, y, x2, y2),\n",
        "                       outline=targ_color,\n",
        "                       width=3)\n",
        "\n",
        "        # Create a text string to display\n",
        "        text_string_to_show = f\"{label_name} ({round(score.item(), 3)})\"\n",
        "\n",
        "        # Draw the text on the image\n",
        "        draw.text(xy=(x, y),\n",
        "                  text=text_string_to_show,\n",
        "                  fill=\"white\",\n",
        "                  font=font)\n",
        "\n",
        "    # Remove the draw each time\n",
        "    del draw\n",
        "\n",
        "    # Setup blank string to print out\n",
        "    return_string = \"\"\n",
        "\n",
        "    # Setup list of target items to discover\n",
        "    target_items = [\"trash\", \"bin\", \"hand\"]\n",
        "\n",
        "    # If no items detected or trash, bin, hand not in list, return notification\n",
        "    if (len(class_name_text_labels) == 0) or not (any_in_list(list_a=target_items, list_b=class_name_text_labels)):\n",
        "        return_string = f\"No trash, bin or hand detected at confidence threshold {conf_threshold}. Try another image or lowering the confidence threshold.\"\n",
        "        return image, return_string\n",
        "\n",
        "    # If there are some missing, print the ones which are missing\n",
        "    elif not all_in_list(list_a=target_items, list_b=class_name_text_labels):\n",
        "        missing_items = []\n",
        "        for item in target_items:\n",
        "            if item not in class_name_text_labels:\n",
        "                missing_items.append(item)\n",
        "        return_string = f\"Detected the following items: {class_name_text_labels}. But missing the following in order to get +1: {missing_items}. If this is an error, try another image or altering the confidence threshold. Otherwise, the model may need to be updated with better data.\"\n",
        "\n",
        "    # If all 3 trash, bin, hand occur = + 1\n",
        "    if all_in_list(list_a=target_items, list_b=class_name_text_labels):\n",
        "        return_string = f\"+1! Found the following items: {class_name_text_labels}, thank you for cleaning up the area!\"\n",
        "\n",
        "    print(return_string)\n",
        "\n",
        "    return image, return_string\n",
        "\n",
        "# Create the interface\n",
        "demo = gr.Interface(\n",
        "    fn=predict_on_image,\n",
        "    inputs=[\n",
        "        gr.Image(type=\"pil\", label=\"Target Image\"),\n",
        "        gr.Slider(minimum=0, maximum=1, value=0.25, label=\"Confidence Threshold\")\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Image(type=\"pil\", label=\"Image Output\"),\n",
        "        gr.Text(label=\"Text Output\")\n",
        "    ],\n",
        "    title=\"ðŸš® Trashify Object Detection Demo V2\",\n",
        "    description=\"\"\"Help clean up your local area! Upload an image and get +1 if there is all of the following items detected: trash, bin, hand.\n",
        "\n",
        "    The [model](https://huggingface.co/mrdbourke/detr_finetuned_trashify_box_detector_with_data_aug) in V2 has been trained with data augmentation preprocessing (color jitter, horizontal flipping) to improve robustness.\n",
        "    \"\"\",\n",
        "    # Examples come in the form of a list of lists, where each inner list contains elements to prefill the `inputs` parameter with\n",
        "    examples=[\n",
        "        [\"examples/trashify_example_1.jpeg\", 0.25],\n",
        "        [\"examples/trashify_example_2.jpeg\", 0.25],\n",
        "        [\"examples/trashify_example_3.jpeg\", 0.25]\n",
        "    ],\n",
        "    cache_examples=True\n",
        ")\n",
        "\n",
        "# Launch the demo\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U931MGanKxZM"
      },
      "outputs": [],
      "source": [
        "# 1. Import the required methods for uploading to the Hugging Face Hub\n",
        "from huggingface_hub import (\n",
        "    create_repo,\n",
        "    get_full_repo_name,\n",
        "    upload_file, # for uploading a single file (if necessary)\n",
        "    upload_folder # for uploading multiple files (in a folder)\n",
        ")\n",
        "\n",
        "# 2. Define the parameters we'd like to use for the upload\n",
        "LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD = \"demos/trashify_object_detector_data_aug_model\" # TK - update this path\n",
        "HF_TARGET_SPACE_NAME = \"trashify_demo_v2\"\n",
        "HF_REPO_TYPE = \"space\" # we're creating a Hugging Face Space\n",
        "HF_SPACE_SDK = \"gradio\"\n",
        "HF_TOKEN = \"\" # optional: set to your Hugging Face token (but I'd advise storing this as an environment variable as previously discussed)\n",
        "\n",
        "# 3. Create a Space repository on Hugging Face Hub\n",
        "print(f\"[INFO] Creating repo on Hugging Face Hub with name: {HF_TARGET_SPACE_NAME}\")\n",
        "create_repo(\n",
        "    repo_id=HF_TARGET_SPACE_NAME,\n",
        "    # token=HF_TOKEN, # optional: set token manually (though it will be automatically recognized if it's available as an environment variable)\n",
        "    repo_type=HF_REPO_TYPE,\n",
        "    private=False, # set to True if you don't want your Space to be accessible to others\n",
        "    space_sdk=HF_SPACE_SDK,\n",
        "    exist_ok=True, # set to False if you want an error to raise if the repo_id already exists\n",
        ")\n",
        "\n",
        "# 4. Get the full repository name (e.g. {username}/{model_id} or {username}/{space_name})\n",
        "full_hf_repo_name = get_full_repo_name(model_id=HF_TARGET_SPACE_NAME)\n",
        "print(f\"[INFO] Full Hugging Face Hub repo name: {full_hf_repo_name}\")\n",
        "\n",
        "# 5. Upload our demo folder\n",
        "print(f\"[INFO] Uploading {LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD} to repo: {full_hf_repo_name}\")\n",
        "folder_upload_url = upload_folder(\n",
        "    repo_id=full_hf_repo_name,\n",
        "    folder_path=LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD,\n",
        "    path_in_repo=\".\", # upload our folder to the root directory (\".\" means \"base\" or \"root\", this is the default)\n",
        "    # token=HF_TOKEN, # optional: set token manually\n",
        "    repo_type=HF_REPO_TYPE,\n",
        "    commit_message=\"Uploading Trashify V2 box detection model (with data augmentation) app.py\"\n",
        ")\n",
        "print(f\"[INFO] Demo folder successfully uploaded with commit URL: {folder_upload_url}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxUx-u_lKxZN"
      },
      "outputs": [],
      "source": [
        "# Next:\n",
        "# Upload augmentation model to Hugging Face Hub âœ…\n",
        "# Create demo for augmentation model âœ…\n",
        "# Compare results from augmentation model to non-augmentation model âœ…"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPK07IeJKxZO"
      },
      "source": [
        "\n",
        "## 21.1 Make a prediction on a random test sample with model using data aug model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0v_uCH_KxZP"
      },
      "outputs": [],
      "source": [
        "# Get a random sample from the test preds\n",
        "random_test_pred_index = random.randint(0, len(processed_dataset[\"test\"]))\n",
        "print(f\"[INFO] Making predictions on test item with index: {random_test_pred_index}\")\n",
        "\n",
        "random_test_sample = processed_dataset[\"test\"][random_test_pred_index]\n",
        "\n",
        "# # Do a single forward pass with the model\n",
        "random_test_sample_outputs = model_aug(pixel_values=random_test_sample[\"pixel_values\"].unsqueeze(0).to(\"cuda\"), # model expects input [batch_size, color_channels, height, width]\n",
        "                                       pixel_mask=None)\n",
        "\n",
        "# Post process a random item from test preds\n",
        "random_test_sample_outputs_post_processed = image_processor.post_process_object_detection(\n",
        "    outputs=random_test_sample_outputs,\n",
        "    threshold=0.25, # prediction probability threshold for boxes (note: boxes from an untrained model will likely be bad)\n",
        "    target_sizes=[random_test_sample[\"labels\"][\"orig_size\"]] # original input image size (or whichever target size you'd like), required to be same number of input items in a list\n",
        ")\n",
        "\n",
        "# Plot the random sample test preds\n",
        "# Extract scores, labels and boxes\n",
        "random_test_sample_pred_scores = random_test_sample_outputs_post_processed[0][\"scores\"]\n",
        "random_test_sample_pred_labels = random_test_sample_outputs_post_processed[0][\"labels\"]\n",
        "random_test_sample_pred_boxes = random_test_sample_outputs_post_processed[0][\"boxes\"]\n",
        "\n",
        "# Create a list of labels to plot on the boxes\n",
        "random_test_sample_labels_to_plot = [f\"Pred: {id2label[label_pred.item()]} ({round(score_pred.item(), 4)})\"\n",
        "                  for label_pred, score_pred in zip(random_test_sample_pred_labels, random_test_sample_pred_scores)]\n",
        "\n",
        "print(f\"[INFO] Labels with scores: {random_test_sample_labels_to_plot}\")\n",
        "print(f\"[INFO] Boxes:\")\n",
        "for item in random_test_sample_pred_boxes:\n",
        "    print(item.detach().cpu())\n",
        "print(f\"[INFO] Total preds: {len(random_test_sample_labels_to_plot)}\")\n",
        "\n",
        "# Plot the predicted boxes on the random test image\n",
        "to_pil_image(\n",
        "    pic=draw_bounding_boxes(\n",
        "        image=pil_to_tensor(pic=dataset[\"test\"][random_test_pred_index][\"image\"]),\n",
        "        boxes=random_test_sample_pred_boxes,\n",
        "        labels=random_test_sample_labels_to_plot,\n",
        "        width=3\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lncfi34JKxZf"
      },
      "source": [
        "# 22. Model V3 - Cleaning up predictions with NMS (Non-max Suppression)\n",
        "\n",
        "UPTOHERE\n",
        "* Take preds from model v2 and perform NMS on them to see what happens\n",
        "* Need to calculate:\n",
        "    * IoU (intersection over union)\n",
        "* Can write about these in a blog post as extension material\n",
        "* Test image index good to practice on:\n",
        "    * 163, 108\n",
        "* Create a demo which compares NMS-free boxes to boxes *with* NMS\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 22.1 NMS filtering logic to do\n",
        "\n",
        "TK - create a table of different items here\n",
        "\n",
        "1. Simplest filtering: keep only 1x class label with the highest score per image (e.g. if there are two \"hand\" predictions, keep only the highest scoring one) âœ…\n",
        "    * TK - problem with simple filtering is that it might take out a box that would've been helpful, it also assumes that there's little false positives (e.g. each box is predicting the class that it should predict)\n",
        "2. Greedy IoU filtering: Filter boxes which have IoU > 0.9 (big overlap) and keep the box with the higher score âœ…\n",
        "    * TK - problem here is that it may filter heavily overlapping classes (e.g. if there are many boxes of different classes clustered together because your objects overlap, such as on a plate of food, items may overlap)\n",
        "3. Class-aware IoU filtering: Filter boxes which have the same label and have IoU > 0.5 and keep the box with the higher score\n",
        "\n",
        "Other potential NMS options:\n",
        "    * Greedy NMS (good for distinct boxes, just take the highest scoring box per class)\n",
        "    * Soft-NMS with linear penalty (good for boxes which may have overlap, e.g. smaller boxes in clusters)\n",
        "    * Class-aware NMS (only perform NMS on same class of boxes)\n",
        "\n",
        "* See this video here: https://youtu.be/VAo84c1hQX8?si=dYftsYADb9Kq-bul  "
      ],
      "metadata": {
        "id": "axSw4ShQRReA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzbv87veKxZg"
      },
      "source": [
        "* TK - show prediction with more boxes than ideal, then introduce NMS as a technique to fix the predictions (e.g. on the same sample)\n",
        "    * TK - NMS doesn't need an extra model, just a way to\n",
        "* TK - test index 163 is a good example with many boxes that could be shortened to a few"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyYl8IpHKxZh"
      },
      "source": [
        "## 22.2 Simple NMS - Keep only highest scoring class per prediction\n",
        "\n",
        "TK - This is the simplest method and simply iterates through the boxes and keep the highest scoring box per class (e.g. if there are two \"hand\" prediction boxes, only keep the higher scoring one)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ud3WjNkKxZh"
      },
      "outputs": [],
      "source": [
        "def filter_highest_scoring_box_per_class(boxes, labels, scores):\n",
        "    \"\"\"\n",
        "    Perform NMS (Non-max Supression) to only keep the top scoring box per class.\n",
        "\n",
        "    Args:\n",
        "        boxes: tensor of shape (N, 4)\n",
        "        labels: tensor of shape (N,)\n",
        "        scores: tensor of shape (N,)\n",
        "    Returns:\n",
        "        boxes: tensor of shape (N, 4) filtered for max scoring item per class\n",
        "        labels: tensor of shape (N,) filtered for max scoring item per class\n",
        "        scores: tensor of shape (N,) filtered for max scoring item per class\n",
        "    \"\"\"\n",
        "    # Start with a blank keep mask (e.g. all False and then update the boxes to keep with True)\n",
        "    keep_mask = torch.zeros(len(boxes), dtype=torch.bool)\n",
        "\n",
        "    # For each unique class\n",
        "    for class_id in labels.unique():\n",
        "        # Get the indicies for the target class\n",
        "        class_mask = labels == class_id\n",
        "\n",
        "        # If any of the labels match the current class_id\n",
        "        if class_mask.any():\n",
        "            # Find the index of highest scoring box for this specific class\n",
        "            class_scores = scores[class_mask]\n",
        "            highest_score_idx = class_scores.argmax()\n",
        "\n",
        "            # Convert back to the original index\n",
        "            original_idx = torch.where(class_mask)[0][highest_score_idx]\n",
        "\n",
        "            # Update the index in the keep mask to keep the highest scoring box\n",
        "            keep_mask[original_idx] = True\n",
        "\n",
        "    return boxes[keep_mask], labels[keep_mask], scores[keep_mask]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HDjGXBHKxZj"
      },
      "outputs": [],
      "source": [
        "# Mask with simple NMS keep mask\n",
        "keep_boxes, keep_labels, keep_scores = filter_highest_scoring_box_per_class(boxes=random_test_sample_pred_boxes,\n",
        "                                                                            labels=random_test_sample_pred_labels,\n",
        "                                                                            scores=random_test_sample_pred_scores)\n",
        "\n",
        "print(len(random_test_sample_pred_boxes), len(random_test_sample_pred_labels), len(random_test_sample_pred_scores))\n",
        "print(len(keep_scores), len(keep_labels), len(keep_boxes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1_oeDlTKxZk"
      },
      "outputs": [],
      "source": [
        "keep_boxes, keep_labels, keep_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPE1Nk99KxZl"
      },
      "outputs": [],
      "source": [
        "# Create a list of labels to plot on the boxes\n",
        "random_test_sample_labels_to_plot = [f\"Pred: {id2label[label_pred.item()]} ({round(score_pred.item(), 4)})\"\n",
        "                  for label_pred, score_pred in zip(random_test_sample_pred_labels, random_test_sample_pred_scores)]\n",
        "\n",
        "print(f\"[INFO] Labels with scores: {random_test_sample_labels_to_plot}\")\n",
        "\n",
        "# Plot the predicted boxes on the random test image\n",
        "test_image_with_preds_original = to_pil_image(\n",
        "    pic=draw_bounding_boxes(\n",
        "        image=pil_to_tensor(pic=dataset[\"test\"][random_test_pred_index][\"image\"]),\n",
        "        boxes=random_test_sample_pred_boxes,\n",
        "        labels=random_test_sample_labels_to_plot,\n",
        "        width=3\n",
        "    )\n",
        ")\n",
        "\n",
        "### Create image with filtered boxes\n",
        "\n",
        "# Create a list of labels to plot on the boxes\n",
        "random_test_sample_labels_to_plot_filtered = [f\"Pred: {id2label[label_pred.item()]} ({round(score_pred.item(), 4)})\"\n",
        "                  for label_pred, score_pred in zip(keep_labels, keep_scores)]\n",
        "\n",
        "print(f\"[INFO] Labels with scores: {random_test_sample_labels_to_plot_filtered}\")\n",
        "\n",
        "# Plot the predicted boxes on the random test image\n",
        "test_image_with_preds_filtered = to_pil_image(\n",
        "    pic=draw_bounding_boxes(\n",
        "        image=pil_to_tensor(pic=dataset[\"test\"][random_test_pred_index][\"image\"]),\n",
        "        boxes=keep_boxes,\n",
        "        labels=random_test_sample_labels_to_plot_filtered,\n",
        "        width=3\n",
        "    )\n",
        ")\n",
        "\n",
        "# Visualize the transformed image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a figure with two subplots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n",
        "\n",
        "# Display image 1\n",
        "axes[0].imshow(test_image_with_preds_original)\n",
        "axes[0].axis(\"off\")  # Hide axes\n",
        "axes[0].set_title(f\"Original Image Preds (total: {len(random_test_sample_pred_boxes)})\")\n",
        "\n",
        "# Display image 2\n",
        "axes[1].imshow(test_image_with_preds_filtered)\n",
        "axes[1].axis(\"off\")  # Hide axes\n",
        "axes[1].set_title(f\"Filtered Image Preds (total: {len(keep_boxes)})\")\n",
        "\n",
        "# Show the plot\n",
        "plt.suptitle(\"Simple NMS - Only keep the highest scoring box per prediction\")\n",
        "plt.tight_layout()\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LchkWYRPKxZm"
      },
      "source": [
        "TK - problem with simple filtering is that it might take out a box that would've been helpful, it also assumes that there's little false positives (e.g. each box is predicting the class that it should predict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7J1BCUZKxZn"
      },
      "source": [
        "## 22.3 Greedy IoU Filtering - Intersection over Union - If a pair of boxes have an IoU over a certain threshold, keep the box with the higher score\n",
        "\n",
        "* IoU in torchmetrics - https://lightning.ai/docs/torchmetrics/stable/detection/intersection_over_union.html\n",
        "\n",
        "To calculate the **Intersection over Union (IoU)** between two bounding boxes:\n",
        "\n",
        "1. **Coordinates of the intersection rectangle**:\n",
        "   $$\n",
        "   x_{\\text{left}} = \\max(x_{1A}, x_{1B})\n",
        "   $$\n",
        "   $$\n",
        "   y_{\\text{top}} = \\max(y_{1A}, y_{1B})\n",
        "   $$\n",
        "   $$\n",
        "   x_{\\text{right}} = \\min(x_{2A}, x_{2B})\n",
        "   $$\n",
        "   $$\n",
        "   y_{\\text{bottom}} = \\min(y_{2A}, y_{2B})\n",
        "   $$\n",
        "\n",
        "Where:\n",
        "\n",
        "$$\n",
        "   \\text{A} = \\text{Box 1}\n",
        "$$\n",
        "$$\n",
        "   \\text{B} = \\text{Box 2}\n",
        "$$\n",
        "\n",
        "2. **Width and height of the intersection**:\n",
        "   $$\n",
        "   \\text{intersection\\_width} = \\max(0, x_{\\text{right}} - x_{\\text{left}})\n",
        "   $$\n",
        "   $$\n",
        "   \\text{intersection\\_height} = \\max(0, y_{\\text{bottom}} - y_{\\text{top}})\n",
        "   $$\n",
        "\n",
        "3. **Area of Overlap**:\n",
        "   $$\n",
        "   \\text{Area of Overlap} = \\text{intersection\\_width} \\times \\text{intersection\\_height}\n",
        "   $$\n",
        "\n",
        "4. **Area of Union**:\n",
        "   $$\n",
        "   \\text{Area of Union} = \\text{Area of Box 1} + \\text{Area of Box 2} - \\text{Area of Overlap}\n",
        "   $$\n",
        "\n",
        "5. **Intersection over Union (IoU)**:\n",
        "   $$\n",
        "   \\text{IoU} = \\text{Area of Overlap} / \\text{Area of Union}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-R8hTuBKxZo"
      },
      "outputs": [],
      "source": [
        "# IoU = Intersection / Union\n",
        "# Inserction =\n",
        "    # x_left = max(x1_A, x1_B)\n",
        "    # y_top = max(y1_A, y1_B)\n",
        "    # x_right = min(x2_A, x2_B)\n",
        "    # y_bottom = min(y2_A, x2_B)\n",
        "    #\n",
        "    # Where:\n",
        "        # A = Box 1\n",
        "        # B = Box 2\n",
        "    # intersection_width = max(0, x_right - x_left)\n",
        "    # interesection_height = max(0, y_bottom - y_top)\n",
        "    # area_intersection = intersection_width * intersection_height\n",
        "# Union = area_box_1 + area_box_2 - intersection\n",
        "\n",
        "def intersection_over_union_score(box_1, box_2):\n",
        "    \"\"\"Calculates Intersection over Union (IoU) score for two given boxes in XYXY format.\"\"\"\n",
        "    assert len(box_1) == 4, f\"Box 1 should have four elements in the format [x_1, y_1, x_2, y_2] but has: {len(box_1)}, see: {box_1}\"\n",
        "    assert len(box_2) == 4, f\"Box 2 should have four elements in the format [x_1, y_1, x_2, y_2] but has: {len(box_2)}, see: {box_2}\"\n",
        "\n",
        "    x1_box_1, y1_box_1, x2_box_1, y2_box_1 = box_1[0], box_1[1], box_1[2], box_1[3]\n",
        "    x1_box_2, y1_box_2, x2_box_2, y2_box_2 = box_2[0], box_2[1], box_2[2], box_2[3]\n",
        "\n",
        "    # Get coordinates of overlapping box (note: there may not be any overlapping box)\n",
        "    x_left = torch.max(x1_box_1, x1_box_2)\n",
        "    y_top = torch.max(y1_box_1, y1_box_2)\n",
        "    x_right = torch.min(x2_box_1, x2_box_2)\n",
        "    y_bottom = torch.min(y2_box_1, y2_box_2)\n",
        "\n",
        "    # Calculate the intersection width and height (we take the max of 0 and the value to find non-overlapping boxes)\n",
        "    intersection_width = max(0, x_right - x_left)\n",
        "    intersection_height = max(0, y_bottom - y_top)\n",
        "\n",
        "    # Calculate the area of intersection (note: this will 0 if either width or height are 0)\n",
        "    area_of_intersection = intersection_height * intersection_width\n",
        "\n",
        "    # Calculate individual box areas\n",
        "    box_1_area = (x2_box_1 - x1_box_1) * (y2_box_1 - y1_box_1) # width * height\n",
        "    box_2_area = (x2_box_2 - x1_box_2) * (y2_box_2 - y1_box_2)\n",
        "\n",
        "    # Calcuate area of union (sum of box areas minus the intersection area)\n",
        "    area_of_union = box_1_area + box_2_area - area_of_intersection\n",
        "\n",
        "    # Calculate the IoU score\n",
        "    iou_score = area_of_intersection / area_of_union\n",
        "\n",
        "    return iou_score\n",
        "\n",
        "\n",
        "iou_score_test_pred_boxes = intersection_over_union_score(box_1=random_test_sample_pred_boxes[4],\n",
        "                                                          box_2=random_test_sample_pred_boxes[3])\n",
        "\n",
        "print(f\"[INFO] IoU Score: {iou_score_test_pred_boxes}\")\n",
        "\n",
        "\n",
        "random_test_sample_pred_boxes[0], random_test_sample_pred_boxes[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ge_0mF5_KxZp"
      },
      "outputs": [],
      "source": [
        "# TK - for visualization purposes, write code to highlight the intersecting points on a box and print the IoU score in the middle of the box\n",
        "\n",
        "# IoU logic\n",
        "    # 1. General IoU threshold (removing boxes at a global level, regardless of label)\n",
        "        # -> for box pairs with IoU > 0.9, keep the higher scoring box\n",
        "    # 2. Label specific IoU threshold (only concern is comparing boxes with the same label)\n",
        "        # -> for box pairs with same label and IoU > 0.5, keep the higher scoring box"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHc9doW0KxZq"
      },
      "outputs": [],
      "source": [
        "keep_boxes = []\n",
        "keep_scores = []\n",
        "keep_labels = []\n",
        "\n",
        "random_test_sample_pred_scores = random_test_sample_outputs_post_processed[0][\"scores\"]\n",
        "random_test_sample_pred_labels = random_test_sample_outputs_post_processed[0][\"labels\"]\n",
        "random_test_sample_pred_boxes = random_test_sample_outputs_post_processed[0][\"boxes\"]\n",
        "\n",
        "keep_indexes = torch.ones(len(random_test_sample_pred_boxes), dtype=torch.bool)\n",
        "\n",
        "iou_general_threshold = 0.9 # general threshold = remove the lower scoring box in box pairs with over iou_general_threshold regardless of the label\n",
        "iou_class_level_threshold = 0.5 # remove overlapping similar classes\n",
        "\n",
        "# TODO: Add a clause here to include if class labels are the same, then filter based on the class-specifc IoU threshold\n",
        "filter_global = True\n",
        "filter_same_label = True\n",
        "\n",
        "# Count the total loops\n",
        "total_loops = 0\n",
        "\n",
        "for i, box_A in enumerate(random_test_sample_pred_boxes):\n",
        "    if not keep_indexes[i]: # insert clause to prevent calculating on already filtered labels\n",
        "        continue\n",
        "\n",
        "    for j, box_B in enumerate(random_test_sample_pred_boxes):\n",
        "        if not keep_indexes[i]:\n",
        "            continue\n",
        "\n",
        "        # Only calculate IoU score if indexes aren't the same (saves comparing the same index boxes for unwanted calculations)\n",
        "        if (i != j):\n",
        "            iou_score = intersection_over_union_score(box_1=box_A, box_2=box_B)\n",
        "            print(f\"[INFO] IoU Score for box {(i, j)}: {iou_score}\")\n",
        "\n",
        "            if filter_global:\n",
        "                if iou_score > iou_general_threshold:\n",
        "                    score_A, score_B = random_test_sample_pred_scores[i], random_test_sample_pred_scores[j]\n",
        "                    if score_A > score_B:\n",
        "                        print(f\"[INFO] Box to keep index: {i} -> {box_A}\")\n",
        "                        keep_indexes[j] = False\n",
        "                    else:\n",
        "                        print(f\"[INFO] Box to keep index: {j} -> {box_B}\")\n",
        "                        keep_indexes[i] = False\n",
        "\n",
        "            if filter_same_label:\n",
        "                if iou_score > iou_class_level_threshold:\n",
        "                    i_label = random_test_sample_pred_labels[i]\n",
        "                    j_label = random_test_sample_pred_labels[j]\n",
        "                    if i_label == j_label:\n",
        "                        print(f\"Labels are equal: {i_label, j_label}\")\n",
        "                        score_A, score_B = random_test_sample_pred_scores[i], random_test_sample_pred_scores[j]\n",
        "                        if score_A > score_B:\n",
        "                            print(f\"[INFO] Box to keep index: {i} -> {box_A}\")\n",
        "                            keep_indexes[j] = False\n",
        "                        else:\n",
        "                            print(f\"[INFO] Box to keep index: {j} -> {box_B}\")\n",
        "                            keep_indexes[i] = False\n",
        "\n",
        "        total_loops += 1\n",
        "\n",
        "print(keep_indexes)\n",
        "\n",
        "keep_scores = random_test_sample_pred_scores[keep_indexes]\n",
        "keep_labels = random_test_sample_pred_labels[keep_indexes]\n",
        "keep_boxes = random_test_sample_pred_boxes[keep_indexes]\n",
        "\n",
        "print(len(random_test_sample_pred_boxes), len(random_test_sample_pred_labels), len(random_test_sample_pred_boxes))\n",
        "print(len(keep_scores), len(keep_labels), len(keep_boxes), sum(keep_indexes))\n",
        "\n",
        "print(f\"[INFO] Number of total loops: {total_loops}, max possible loops: {len(random_test_sample_pred_boxes)**2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sa3SQ8uKxZs"
      },
      "outputs": [],
      "source": [
        "# tensor([ True,  True,  True,  True,  True, False,  True, False])\n",
        "# tensor([ True,  True,  True,  True,  True, False,  True, False])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZC4lrnRPKxZt"
      },
      "outputs": [],
      "source": [
        "# Create a list of labels to plot on the boxes\n",
        "random_test_sample_labels_to_plot = [f\"Pred: {id2label[label_pred.item()]} ({round(score_pred.item(), 4)})\"\n",
        "                  for label_pred, score_pred in zip(random_test_sample_pred_labels, random_test_sample_pred_scores)]\n",
        "\n",
        "print(f\"[INFO] Labels with scores: {random_test_sample_labels_to_plot}\")\n",
        "\n",
        "# Plot the predicted boxes on the random test image\n",
        "test_image_with_preds_original = to_pil_image(\n",
        "    pic=draw_bounding_boxes(\n",
        "        image=pil_to_tensor(pic=dataset[\"test\"][random_test_pred_index][\"image\"]),\n",
        "        boxes=random_test_sample_pred_boxes,\n",
        "        labels=random_test_sample_labels_to_plot,\n",
        "        width=3\n",
        "    )\n",
        ")\n",
        "\n",
        "### Create image with filtered boxes\n",
        "\n",
        "# Create a list of labels to plot on the boxes\n",
        "random_test_sample_labels_to_plot_filtered = [f\"Pred: {id2label[label_pred.item()]} ({round(score_pred.item(), 4)})\"\n",
        "                  for label_pred, score_pred in zip(keep_labels, keep_scores)]\n",
        "\n",
        "print(f\"[INFO] Labels with scores: {random_test_sample_labels_to_plot_filtered}\")\n",
        "\n",
        "# Plot the predicted boxes on the random test image\n",
        "test_image_with_preds_filtered = to_pil_image(\n",
        "    pic=draw_bounding_boxes(\n",
        "        image=pil_to_tensor(pic=dataset[\"test\"][random_test_pred_index][\"image\"]),\n",
        "        boxes=keep_boxes,\n",
        "        labels=random_test_sample_labels_to_plot_filtered,\n",
        "        width=3\n",
        "    )\n",
        ")\n",
        "\n",
        "# Visualize the transformed image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a figure with two subplots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n",
        "\n",
        "# Display image 1\n",
        "axes[0].imshow(test_image_with_preds_original)\n",
        "axes[0].axis(\"off\")  # Hide axes\n",
        "axes[0].set_title(f\"Original Image Preds (total: {len(random_test_sample_pred_boxes)})\")\n",
        "\n",
        "# Display image 2\n",
        "axes[1].imshow(test_image_with_preds_filtered)\n",
        "axes[1].axis(\"off\")  # Hide axes\n",
        "axes[1].set_title(f\"Filtered Image Preds (total: {len(keep_boxes)})\")\n",
        "\n",
        "# Show the plot\n",
        "plt.suptitle(f\"Greedy IoU Filtering (General) - For boxes with IoU > {iou_general_threshold}, keep the higher scoring box\")\n",
        "plt.tight_layout()\n",
        "plt.show();\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wM5f3s3IKxZt"
      },
      "outputs": [],
      "source": [
        "# TK - more NMS logic:\n",
        "# If there are more than two hands, keep the one with the higher score..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBhW636FKxZu"
      },
      "source": [
        "# 23. Create a Demo with Simple NMS Filtering (only keep the highest scoring boxes per image)\n",
        "\n",
        "UPTOHERE:\n",
        "\n",
        "- upload the demo to Hugging Face Spaces as Trashify V3\n",
        "- Make sure the demo works\n",
        "- Go back through the code and start tidying up/explaining things\n",
        "    - Create a blog post to discuss different box formats in object detection\n",
        "    - Create a blog post for NMS + IoU filtering (can create an IoU function that colours in the intersection parts)\n",
        "    - Create an extension for longer training + synthetic data + evaluation metrics + deploying on transformers.js"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjYjSl3cKxZv"
      },
      "outputs": [],
      "source": [
        "# Make directory for demo\n",
        "from pathlib import Path\n",
        "\n",
        "trashify_data_aug_model_dir = Path(\"demos/trashify_object_detector_data_aug_model_with_nms/\")\n",
        "trashify_data_aug_model_dir.mkdir(exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWJcALmCKxZv"
      },
      "outputs": [],
      "source": [
        "%%writefile demos/trashify_object_detector_data_aug_model_with_nms/requirements.txt\n",
        "timm\n",
        "gradio\n",
        "torch\n",
        "transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEpFEbGUKxZw"
      },
      "outputs": [],
      "source": [
        "%%writefile demos/trashify_object_detector_data_aug_model_with_nms/README.md\n",
        "---\n",
        "title: Trashify Demo V3 ðŸš®\n",
        "emoji: ðŸ—‘ï¸\n",
        "colorFrom: purple\n",
        "colorTo: blue\n",
        "sdk: gradio\n",
        "sdk_version: 4.40.0\n",
        "app_file: app.py\n",
        "pinned: false\n",
        "license: apache-2.0\n",
        "---\n",
        "\n",
        "# ðŸš® Trashify Object Detector Demo V3\n",
        "\n",
        "Object detection demo to detect `trash`, `bin`, `hand`, `trash_arm`, `not_trash`, `not_bin`, `not_hand`.\n",
        "\n",
        "Used as example for encouraging people to cleanup their local area.\n",
        "\n",
        "If `trash`, `hand`, `bin` all detected = +1 point.\n",
        "\n",
        "## Dataset\n",
        "\n",
        "All Trashify models are trained on a custom hand-labelled dataset of people picking up trash and placing it in a bin.\n",
        "\n",
        "The dataset can be found on Hugging Face as [`mrdbourke/trashify_manual_labelled_images`](https://huggingface.co/datasets/mrdbourke/trashify_manual_labelled_images).\n",
        "\n",
        "## Demos\n",
        "\n",
        "* [V1](https://huggingface.co/spaces/mrdbourke/trashify_demo_v1) = Fine-tuned DETR model trained *without* data augmentation.\n",
        "* [V2](https://huggingface.co/spaces/mrdbourke/trashify_demo_v2) = Fine-tuned DETR model trained *with* data augmentation.\n",
        "* [V3](https://huggingface.co/spaces/mrdbourke/trashify_demo_v3) = Fine-tuned DETR model trained *with* data augmentation (same as V2) with an NMS (Non Maximum Suppression) post-processing step.\n",
        "\n",
        "TK - finish the README.md + update with links to materials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJIExh4BKxZx"
      },
      "outputs": [],
      "source": [
        "%%writefile demos/trashify_object_detector_data_aug_model_with_nms/app.py\n",
        "import gradio as gr\n",
        "import torch\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "from transformers import AutoImageProcessor\n",
        "from transformers import AutoModelForObjectDetection\n",
        "\n",
        "# Note: Can load from Hugging Face or can load from local.\n",
        "# You will have to replace {mrdbourke} for your own username if the model is on your Hugging Face account.\n",
        "model_save_path = \"mrdbourke/detr_finetuned_trashify_box_detector_with_data_aug\"\n",
        "\n",
        "# Load the model and preprocessor\n",
        "image_processor = AutoImageProcessor.from_pretrained(model_save_path)\n",
        "model = AutoModelForObjectDetection.from_pretrained(model_save_path)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "\n",
        "# Get the id2label dictionary from the model\n",
        "id2label = model.config.id2label\n",
        "\n",
        "# Set up a colour dictionary for plotting boxes with different colours\n",
        "color_dict = {\n",
        "    \"bin\": \"green\",\n",
        "    \"trash\": \"blue\",\n",
        "    \"hand\": \"purple\",\n",
        "    \"trash_arm\": \"yellow\",\n",
        "    \"not_trash\": \"red\",\n",
        "    \"not_bin\": \"red\",\n",
        "    \"not_hand\": \"red\",\n",
        "}\n",
        "\n",
        "# Create helper functions for seeing if items from one list are in another\n",
        "def any_in_list(list_a, list_b):\n",
        "    \"Returns True if any item from list_a is in list_b, otherwise False.\"\n",
        "    return any(item in list_b for item in list_a)\n",
        "\n",
        "def all_in_list(list_a, list_b):\n",
        "    \"Returns True if all items from list_a are in list_b, otherwise False.\"\n",
        "    return all(item in list_b for item in list_a)\n",
        "\n",
        "def filter_highest_scoring_box_per_class(boxes, labels, scores):\n",
        "    \"\"\"\n",
        "    Perform NMS (Non-max Supression) to only keep the top scoring box per class.\n",
        "\n",
        "    Args:\n",
        "        boxes: tensor of shape (N, 4)\n",
        "        labels: tensor of shape (N,)\n",
        "        scores: tensor of shape (N,)\n",
        "    Returns:\n",
        "        boxes: tensor of shape (N, 4) filtered for max scoring item per class\n",
        "        labels: tensor of shape (N,) filtered for max scoring item per class\n",
        "        scores: tensor of shape (N,) filtered for max scoring item per class\n",
        "    \"\"\"\n",
        "    # Start with a blank keep mask (e.g. all False and then update the boxes to keep with True)\n",
        "    keep_mask = torch.zeros(len(boxes), dtype=torch.bool)\n",
        "\n",
        "    # For each unique class\n",
        "    for class_id in labels.unique():\n",
        "        # Get the indicies for the target class\n",
        "        class_mask = labels == class_id\n",
        "\n",
        "        # If any of the labels match the current class_id\n",
        "        if class_mask.any():\n",
        "            # Find the index of highest scoring box for this specific class\n",
        "            class_scores = scores[class_mask]\n",
        "            highest_score_idx = class_scores.argmax()\n",
        "\n",
        "            # Convert back to the original index\n",
        "            original_idx = torch.where(class_mask)[0][highest_score_idx]\n",
        "\n",
        "            # Update the index in the keep mask to keep the highest scoring box\n",
        "            keep_mask[original_idx] = True\n",
        "\n",
        "    return boxes[keep_mask], labels[keep_mask], scores[keep_mask]\n",
        "\n",
        "def create_return_string(list_of_predicted_labels, target_items=[\"trash\", \"bin\", \"hand\"]):\n",
        "     # Setup blank string to print out\n",
        "    return_string = \"\"\n",
        "\n",
        "    # If no items detected or trash, bin, hand not in list, return notification\n",
        "    if (len(list_of_predicted_labels) == 0) or not (any_in_list(list_a=target_items, list_b=list_of_predicted_labels)):\n",
        "        return_string = f\"No trash, bin or hand detected at confidence threshold {conf_threshold}. Try another image or lowering the confidence threshold.\"\n",
        "        return return_string\n",
        "\n",
        "    # If there are some missing, print the ones which are missing\n",
        "    elif not all_in_list(list_a=target_items, list_b=list_of_predicted_labels):\n",
        "        missing_items = []\n",
        "        for item in target_items:\n",
        "            if item not in list_of_predicted_labels:\n",
        "                missing_items.append(item)\n",
        "        return_string = f\"Detected the following items: {list_of_predicted_labels} (total: {len(list_of_predicted_labels)}). But missing the following in order to get +1: {missing_items}. If this is an error, try another image or altering the confidence threshold. Otherwise, the model may need to be updated with better data.\"\n",
        "\n",
        "    # If all 3 trash, bin, hand occur = + 1\n",
        "    if all_in_list(list_a=target_items, list_b=list_of_predicted_labels):\n",
        "        return_string = f\"+1! Found the following items: {list_of_predicted_labels} (total: {len(list_of_predicted_labels)}), thank you for cleaning up the area!\"\n",
        "\n",
        "    print(return_string)\n",
        "\n",
        "    return return_string\n",
        "\n",
        "def predict_on_image(image, conf_threshold):\n",
        "    with torch.no_grad():\n",
        "        inputs = image_processor(images=[image], return_tensors=\"pt\")\n",
        "        outputs = model(**inputs.to(device))\n",
        "\n",
        "        target_sizes = torch.tensor([[image.size[1], image.size[0]]]) # height, width\n",
        "\n",
        "        results = image_processor.post_process_object_detection(outputs,\n",
        "                                                                threshold=conf_threshold,\n",
        "                                                                target_sizes=target_sizes)[0]\n",
        "    # Return all items in results to CPU\n",
        "    for key, value in results.items():\n",
        "        try:\n",
        "            results[key] = value.item().cpu() # can't get scalar as .item() so add try/except block\n",
        "        except:\n",
        "            results[key] = value.cpu()\n",
        "\n",
        "    # Can return results as plotted on a PIL image (then display the image)\n",
        "    draw = ImageDraw.Draw(image)\n",
        "\n",
        "    # Create a copy of the image to draw on it for NMS\n",
        "    image_nms = image.copy()\n",
        "    draw_nms = ImageDraw.Draw(image_nms)\n",
        "\n",
        "    # Get a font from ImageFont\n",
        "    font = ImageFont.load_default(size=20)\n",
        "\n",
        "    # Get class names as text for print out\n",
        "    class_name_text_labels = []\n",
        "\n",
        "    # TK - update this for NMS\n",
        "    class_name_text_labels_nms = []\n",
        "\n",
        "    # Get original boxes, scores, labels\n",
        "    original_boxes = results[\"boxes\"]\n",
        "    original_labels = results[\"labels\"]\n",
        "    original_scores = results[\"scores\"]\n",
        "\n",
        "    # Filter boxes and only keep 1x of each label with highest score\n",
        "    filtered_boxes, filtered_labels, filtered_scores = filter_highest_scoring_box_per_class(boxes=original_boxes,\n",
        "                                                                                            labels=original_labels,\n",
        "                                                                                            scores=original_scores)\n",
        "    # TODO: turn this into a function so it's cleaner?\n",
        "    for box, label, score in zip(original_boxes, original_labels, original_scores):\n",
        "        # Create coordinates\n",
        "        x, y, x2, y2 = tuple(box.tolist())\n",
        "\n",
        "        # Get label_name\n",
        "        label_name = id2label[label.item()]\n",
        "        targ_color = color_dict[label_name]\n",
        "        class_name_text_labels.append(label_name)\n",
        "\n",
        "        # Draw the rectangle\n",
        "        draw.rectangle(xy=(x, y, x2, y2),\n",
        "                       outline=targ_color,\n",
        "                       width=3)\n",
        "\n",
        "        # Create a text string to display\n",
        "        text_string_to_show = f\"{label_name} ({round(score.item(), 3)})\"\n",
        "\n",
        "        # Draw the text on the image\n",
        "        draw.text(xy=(x, y),\n",
        "                  text=text_string_to_show,\n",
        "                  fill=\"white\",\n",
        "                  font=font)\n",
        "\n",
        "    # TODO: turn this into a function so it's cleaner?\n",
        "    for box, label, score in zip(filtered_boxes, filtered_labels, filtered_scores):\n",
        "        # Create coordinates\n",
        "        x, y, x2, y2 = tuple(box.tolist())\n",
        "\n",
        "        # Get label_name\n",
        "        label_name = id2label[label.item()]\n",
        "        targ_color = color_dict[label_name]\n",
        "        class_name_text_labels_nms.append(label_name)\n",
        "\n",
        "        # Draw the rectangle\n",
        "        draw_nms.rectangle(xy=(x, y, x2, y2),\n",
        "                       outline=targ_color,\n",
        "                       width=3)\n",
        "\n",
        "        # Create a text string to display\n",
        "        text_string_to_show = f\"{label_name} ({round(score.item(), 3)})\"\n",
        "\n",
        "        # Draw the text on the image\n",
        "        draw_nms.text(xy=(x, y),\n",
        "                  text=text_string_to_show,\n",
        "                  fill=\"white\",\n",
        "                  font=font)\n",
        "\n",
        "\n",
        "    # Remove the draw each time\n",
        "    del draw\n",
        "    del draw_nms\n",
        "\n",
        "    # Create the return string\n",
        "    return_string = create_return_string(list_of_predicted_labels=class_name_text_labels)\n",
        "    return_string_nms = create_return_string(list_of_predicted_labels=class_name_text_labels_nms)\n",
        "\n",
        "    return image, return_string, image_nms, return_string_nms\n",
        "\n",
        "# Create the interface\n",
        "demo = gr.Interface(\n",
        "    fn=predict_on_image,\n",
        "    inputs=[\n",
        "        gr.Image(type=\"pil\", label=\"Target Image\"),\n",
        "        gr.Slider(minimum=0, maximum=1, value=0.25, label=\"Confidence Threshold\")\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Image(type=\"pil\", label=\"Image Output (no filtering)\"),\n",
        "        gr.Text(label=\"Text Output (no filtering)\"),\n",
        "        gr.Image(type=\"pil\", label=\"Image Output (with max score per class box filtering)\"),\n",
        "        gr.Text(label=\"Text Output (with max score per class box filtering)\")\n",
        "\n",
        "    ],\n",
        "    title=\"ðŸš® Trashify Object Detection Demo V3\",\n",
        "    description=\"\"\"Help clean up your local area! Upload an image and get +1 if there is all of the following items detected: trash, bin, hand.\n",
        "\n",
        "    The model in V3 is [same model](https://huggingface.co/mrdbourke/detr_finetuned_trashify_box_detector_with_data_aug) as in [V2](https://huggingface.co/spaces/mrdbourke/trashify_demo_v2) (trained with data augmentation) but has an additional post-processing step (NMS or [Non Maximum Suppression](https://paperswithcode.com/method/non-maximum-suppression)) to filter classes for only the highest scoring box of each class.\n",
        "    \"\"\",\n",
        "    # Examples come in the form of a list of lists, where each inner list contains elements to prefill the `inputs` parameter with\n",
        "    examples=[\n",
        "        [\"examples/trashify_example_1.jpeg\", 0.25],\n",
        "        [\"examples/trashify_example_2.jpeg\", 0.25],\n",
        "        [\"examples/trashify_example_3.jpeg\", 0.25]\n",
        "    ],\n",
        "    cache_examples=True\n",
        ")\n",
        "\n",
        "# Launch the demo\n",
        "demo.launch()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qH1YpUTKxZz"
      },
      "source": [
        "## 23.1  Upload our demo to the Hugging Face Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZ1Qzx8MKxZ0"
      },
      "outputs": [],
      "source": [
        "# 1. Import the required methods for uploading to the Hugging Face Hub\n",
        "from huggingface_hub import (\n",
        "    create_repo,\n",
        "    get_full_repo_name,\n",
        "    upload_file, # for uploading a single file (if necessary)\n",
        "    upload_folder # for uploading multiple files (in a folder)\n",
        ")\n",
        "\n",
        "# 2. Define the parameters we'd like to use for the upload\n",
        "LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD = \"demos/trashify_object_detector_data_aug_model_with_nms\" # TK - update this path\n",
        "HF_TARGET_SPACE_NAME = \"trashify_demo_v3\"\n",
        "HF_REPO_TYPE = \"space\" # we're creating a Hugging Face Space\n",
        "HF_SPACE_SDK = \"gradio\"\n",
        "HF_TOKEN = \"\" # optional: set to your Hugging Face token (but I'd advise storing this as an environment variable as previously discussed)\n",
        "\n",
        "# 3. Create a Space repository on Hugging Face Hub\n",
        "print(f\"[INFO] Creating repo on Hugging Face Hub with name: {HF_TARGET_SPACE_NAME}\")\n",
        "create_repo(\n",
        "    repo_id=HF_TARGET_SPACE_NAME,\n",
        "    # token=HF_TOKEN, # optional: set token manually (though it will be automatically recognized if it's available as an environment variable)\n",
        "    repo_type=HF_REPO_TYPE,\n",
        "    private=False, # set to True if you don't want your Space to be accessible to others\n",
        "    space_sdk=HF_SPACE_SDK,\n",
        "    exist_ok=True, # set to False if you want an error to raise if the repo_id already exists\n",
        ")\n",
        "\n",
        "# 4. Get the full repository name (e.g. {username}/{model_id} or {username}/{space_name})\n",
        "full_hf_repo_name = get_full_repo_name(model_id=HF_TARGET_SPACE_NAME)\n",
        "print(f\"[INFO] Full Hugging Face Hub repo name: {full_hf_repo_name}\")\n",
        "\n",
        "# 5. Upload our demo folder\n",
        "print(f\"[INFO] Uploading {LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD} to repo: {full_hf_repo_name}\")\n",
        "folder_upload_url = upload_folder(\n",
        "    repo_id=full_hf_repo_name,\n",
        "    folder_path=LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD,\n",
        "    path_in_repo=\".\", # upload our folder to the root directory (\".\" means \"base\" or \"root\", this is the default)\n",
        "    # token=HF_TOKEN, # optional: set token manually\n",
        "    repo_type=HF_REPO_TYPE,\n",
        "    commit_message=\"Uploading Trashify box detection model v3 app.py with NMS post processing\"\n",
        ")\n",
        "print(f\"[INFO] Demo folder successfully uploaded with commit URL: {folder_upload_url}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8QTvDYKKxZ1"
      },
      "source": [
        "## 23.2 Embed the Space to Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WP_RAdsGKxZ2"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "\n",
        "# You can get embeddable HTML code for your demo by clicking the \"Embed\" button on the demo page\n",
        "HTML(data='''\n",
        "<iframe\n",
        "    src=\"https://mrdbourke-trashify-demo-v3.hf.space\"\n",
        "    frameborder=\"0\"\n",
        "    width=\"1000\"\n",
        "    height=\"1600\"\n",
        "></iframe>\n",
        "''')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1m--1AYlKxZ3"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhx-0wIoKxZ3"
      },
      "outputs": [],
      "source": [
        "# UPTOHERE\n",
        "# Next, focus on a single input -> output âœ…\n",
        "# Show case what an output from the model looks like untrained (e.g. plot the next boxes on it) âœ…\n",
        "# After showcasing 1x prediction, move onto training a model and seeing if we can get it to improve âœ…\n",
        "# Continually focus on 1 input, 1 output until we can scale up âœ…\n",
        "# Create a demo of our model and upload it to Hugging Face âœ…\n",
        "    # Add examples to test the demo âœ…\n",
        "    # Write code to upload the demo to Hugging Face âœ…\n",
        "# Create visualization of input and output of data augmentation âœ…\n",
        "# Create demo of model with data augmentation âœ…\n",
        "# Model 2: Try improve our model with data augmentation âœ…\n",
        "    # Visualize data augmentation examples in and out of the model\n",
        "    # Note: looks like augmentation may hurt our results... ðŸ¤”, this is because our data is so similar, potentially could help with more diverse data, e.g. synthetic data\n",
        "    # Try in a demo and see how it works -> Trashify Demo V2 âœ…\n",
        "    # Extension: Also try a model training for longer\n",
        "# Model 3 (just improve with NMS): Create NMS option so only highest quality boxes are kept for each class âœ…\n",
        "\n",
        "# Next:\n",
        "\n",
        "# Go through notebook and clean it up for\n",
        "# Once we've got a better performing model, introduce evaluation metrics\n",
        "# End: three models, three demos, one without data augmentation, one with it, one with NMS (post-processing) + can have as an extension to train the model for longer and see what happens\n",
        "\n",
        "# Extensions:\n",
        "# Train a model for longer and see if it improves (e.g. 72 epochs)\n",
        "\n",
        "# Workflow:\n",
        "# Untrained model -> input/output -> poor results (always visualize, visualize, visualize!)\n",
        "# Trained model -> input/output -> better results (always visualize, visualize, visualize!)\n",
        "\n",
        "# Outline:\n",
        "# Single input/output with untrained model (bad output)\n",
        "# Train model to improve on single input/output\n",
        "# Introduce evaluation metric\n",
        "# Introduce data augmentation, see D-FINE paper for data augmentation options (we can keep it simple)\n",
        "    # See: https://arxiv.org/pdf/2410.13842\n",
        "    # \"The total batch size is 32 across all variants. Training schedules include 72 epochs with advanced augmentation (RandomPhotometricDistort, RandomZoomOut, RandomIoUCrop, and RMultiScaleInput)\n",
        "    # followed by 2 epochs without advanced augmentation for D-FINE-X and D-FINE-L, and 120 epochs with advanced augmentation followed by 4\n",
        "    # epochs without advanced augmentation for D-FINE-M and D-FINE-S (RT-DETRv2 Training Strategy (Lv et al., 2024) in Table 3)\"\n",
        "    # TODO: Read RT-DETRv2 training strategy from paper mentioned above\n",
        "    # TODO: Read PP-YOLO data augmentation paper (keep it simple to begin with, can increase when needed)\n",
        "# Create demo with Gradio\n",
        "# Create demo with Transformers.js, see: https://huggingface.co/docs/transformers.js/en/tutorials/vanilla-js"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "07ZTllwsrMbO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}