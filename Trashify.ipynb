{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNTYZzIDO0efF7hR2VdxJ3C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/banno-0720/Deep-Learning-Projects/blob/main/Trashify.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeLVmtH1H1ra"
      },
      "source": [
        "# 1. Overview\n",
        "\n",
        "Trashify = using ML to incentivize people to pick up trash in their local area\n",
        "\n",
        "We'll start with a collection of images with bounding box files as our dataset, fine-tune an existing computer vision model to detect items in an image and then share our model as a demo others can use.\n",
        "\n",
        "TK image - update cover image for object detection\n",
        "<!-- <figure style=\"text-align: center;\">\n",
        "    <!-- figtemplate -->\n",
        "    <img src=\"https://huggingface.co/datasets/mrdbourke/learn-hf-images/resolve/main/learn-hf-text-classification/00-project-food-not-food-overview.png\"\n",
        "     alt=\"Project overview image for 'Food Not Food' classification at Nutrify, a food app. The project involves building and deploying a binary text classification model to identify food-related text using Hugging Face Datasets, Transformers, and deploying with Hugging Face Hub/Spaces and Gradio. Examples include labels for 'A photo of sushi rolls on a white plate' (food), 'A serving of chicken curry in a blue bowl' (food), and 'A yellow tractor driving over a grassy hill' (not food). The process is visually depicted from data collection to model training and demo deployment.\"\n",
        "     style=\"width: 100%; max-width: 900px; height: auto;\"/>\n",
        "     <figcaption>We're going to put on our internship hats and build a food not food text classification model using tools from the Hugging Face ecosystem.</figcaption>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRUB8bFdH1rb"
      },
      "source": [
        "## 1.1 What we're going to build\n",
        "\n",
        "We're going to be bulding Trashify ðŸš®, an **object detection model** which incentivises people to pick up trash in their local area by detecting `bin`, `trash`, `hand`.\n",
        "\n",
        "If all three items are detected, a person gets +1 point!\n",
        "\n",
        "For example, say you were going for a walk around your neighbourhood and took a photo of yourself picking up a piece (with your **hand** or **trash arm**) of **trash** and putting it in the **bin**, you would get a point.\n",
        "\n",
        "With this object detection model, you could deploy it to an application which would automatically detect the target classes and then save the result to an online leaderboard.\n",
        "\n",
        "The incentive would be to score the most points, in turn, picking up the most piecces of trash, in a given area.\n",
        "\n",
        "More specifically, we're going to follow the following steps:\n",
        "\n",
        "1. **[Data](https://huggingface.co/datasets/mrdbourke/trashify_manual_labelled_images): Problem defintion and dataset preparation** - Getting a dataset/setting up the problem space.\n",
        "2. **[Model](https://huggingface.co/docs/transformers/en/model_doc/conditional_detr): Finding, training and evaluating a model** - Finding an object detection model suitable for our problem on Hugging Face and customizing it to our own dataset.\n",
        "3. **[Demo](https://huggingface.co/spaces/mrdbourke/trashify_demo_v3): Creating a demo and put our model into the real world** - Sharing our trained model in a way others can access and use.\n",
        "\n",
        "By the end of this project, you'll have a trained model and [demo on Hugging Face](https://huggingface.co/spaces/mrdbourke/trashify_demo_v3) you can share with others:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFJSfcj6H1rd",
        "outputId": "6ce987da-5b76-44d5-b58e-7e75407593c0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<iframe\n",
              "\tsrc=\"https://mrdbourke-trashify-demo-v3.hf.space\"\n",
              "\tframeborder=\"0\"\n",
              "\twidth=\"850\"\n",
              "\theight=\"850\"\n",
              "></iframe>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\"\"\"\n",
        "<iframe\n",
        "\tsrc=\"https://mrdbourke-trashify-demo-v3.hf.space\"\n",
        "\tframeborder=\"0\"\n",
        "\twidth=\"850\"\n",
        "\theight=\"850\"\n",
        "></iframe>\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQRCCeZUH1rh"
      },
      "source": [
        "## 1.2 What is object detection?\n",
        "\n",
        "Object detection is the process of identifying and locating an item in an image.\n",
        "\n",
        "Where *item* can mean almost anything.\n",
        "\n",
        "For example:\n",
        "\n",
        "* Detecting car **licence plates** in a video feed (videos are a series of images) for a parking lot entrance.\n",
        "* Detecting **delivery people** walking towards your front door on a security camera.\n",
        "* Detecting **defects** on a manufacturing line.\n",
        "* Detecting [**pot holes** in the road](https://ieeexplore.ieee.org/abstract/document/9968423) so repair works can automatically be scheduled.\n",
        "* Detecting **small pests (Varroa Mite)** on the bodies of bees.\n",
        "* Detecting [**weeds** in a field](https://ai.meta.com/blog/pytorch-drives-next-gen-intelligent-farming-machines/) so you know what to remove and what to keep.\n",
        "\n",
        "--\n",
        "\n",
        "Examples of actual trash identification projects, see:\n",
        "\n",
        "- Google using machine learning for trash identification â€” [https://sustainability.google/operating-sustainably/stories/circular-economy-marketplace/](https://sustainability.google/operating-sustainably/stories/circular-economy-marketplace/)\n",
        "- Trashify website for identifying trash â€” [https://www.trashify.tech/](https://www.trashify.tech/)\n",
        "- Waste management with deep learning â€” [https://www.sciencedirect.com/science/article/abs/pii/S0956053X23001915](https://www.sciencedirect.com/science/article/abs/pii/S0956053X23001915)\n",
        "- Label Studio being used for labelling a trash dataset â€” [https://labelstud.io/blog/ameru-labeling-for-a-greener-world/](https://labelstud.io/blog/ameru-labeling-for-a-greener-world/)\n",
        "\n",
        "**Image classification** deals with classifying an image as a whole into a single `class`, object detection endeavours to find the specific target item and *where* it is in an image.\n",
        "\n",
        "One of the most common ways of showing where an item is in an image is by displaying a **bounding box** (a rectangle-like box around the target item).\n",
        "\n",
        "An object detection model will often take an input image tensor in the shape `[3, 640, 640]` (`[colour_channels, height, width]`) and output a tensor in the form `[class_name, x_min, y_min, x_max, y_max]` or `[class_name, x1, y1, x2, y2]` (this is two ways to write the same example format, there are more formats, we'll see these below in @tbl-bbox-formats).\n",
        "\n",
        "Where:\n",
        "\n",
        "* `class_name` = The classification of the target item (e.g. `\"car\"`, `\"person\"`, `\"banana\"`, `\"piece_of_trash\"`, this could be almost anything).\n",
        "* `x_min` = The `x` value of the top left corner of the box.\n",
        "* `y_min` = The `y` value of the top left corner of the box.\n",
        "* `x_max` = The `x` value of the bottom right corner of the box.\n",
        "* `y_max` = The `y` value of the bottom right corner of the box."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnpWJg51H1rj"
      },
      "source": [
        "## 1.3 Why train your own object detection models?\n",
        "\n",
        "You can customize **pre-trained models** for object detection as well as API-powered models and LLMs such as [Gemini](https://ai.google.dev/gemini-api/docs/vision?lang=python#bbox), [LandingAI](https://landing.ai/agentic-object-detection) and [DINO-X](https://github.com/IDEA-Research/DINO-X-API).\n",
        "\n",
        "Depending on your requirements, there are several pros and cons for using your own model versus using an API.\n",
        "\n",
        "Training/fine-tuning your own model:\n",
        "\n",
        "| Pros | Cons |\n",
        "| :----- | :----- |\n",
        "| **Control:** Full control over model lifecycle. | Can be complex to get setup. |\n",
        "| No usage limits (aside from compute constraints). | Requires dedicated compute resources for training/inference. |\n",
        "| Can train once and deploy everywhere/whenever you want (for example, Tesla deploying a model to all self-driving cars). | Requires maintenance over time to ensure performance remains up to par. |\n",
        "| **Privacy:** Data can be kept in-house/app and doesnâ€™t need to go to a third party. | Can require longer development cycles compared to using existing APIs. |\n",
        "| **Speed:** Customizing a small model for a specific use case often means it runs much faster on local hardware, for example, modern object detection models can achieve 70-100+ FPS (frames per second) on modern GPU hardware. | |\n",
        "\n",
        "Using a pre-built model API:\n",
        "\n",
        "| Pros | Cons |\n",
        "| :----- | :----- |\n",
        "| **Ease of use:** often can be setup within a few lines of code. | If the model API goes down, your service goes down. |\n",
        "| No maintenance of compute resources. | Data is required to be sent to a third-party for processing. |\n",
        "| Access to the most advanced models. | The API may have usage limits per day/time period. |\n",
        "| Can scale if usage increases. | Can be much slower than using dedicated models due to requiring an API call. |\n",
        "\n",
        "For this project, we're going to focus on fine-tuning our own model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvY61ujjH1rk"
      },
      "source": [
        "## 1.4 Workflow we're going to follow\n",
        "\n",
        "Start with data (or skip this step and go straight to a model) -> get/customize a model -> build and share a demo.\n",
        "\n",
        "With this in mind, our motto is *data, model, demo!*\n",
        "\n",
        "More specifically, we're going to follow the rough workflow of:\n",
        "\n",
        "1. Create, preprocess and load data using [Hugging Face Datasets](https://huggingface.co/docs/datasets/index).\n",
        "2. Define the model we'd like use with [`transformers.AutoModelForObjectDetection`](https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForObjectDetection) (or another similar model class).\n",
        "3. Define training arguments (these are hyperparameters for our model) with [`transformers.TrainingArguments`](https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments).\n",
        "4. Pass `TrainingArguments` from 3 and target datasets to an instance of [`transformers.Trainer`](https://huggingface.co/docs/transformers/en/main_classes/trainer).\n",
        "5. Train the model by calling [`Trainer.train()`](https://huggingface.co/docs/transformers/v4.40.2/en/main_classes/trainer#transformers.Trainer.train).\n",
        "6. Save the model (to our local machine or to the Hugging Face Hub).\n",
        "7. Evaluate the trained model by making and inspecting predctions on the test data.\n",
        "8. Turn the model into a shareable demo.\n",
        "\n",
        "I say rough because machine learning projects are often non-linear in nature.\n",
        "\n",
        "As in, because machine learning projects involve many experiments, they can kind of be all over the place.\n",
        "\n",
        "But this worfklow will give us some good guidelines to follow.\n",
        "\n",
        "<figure style=\"text-align: center; display: inline-block;\">\n",
        "    <!-- figtemplate -->\n",
        "    <img src=\"https://huggingface.co/datasets/mrdbourke/learn-hf-images/resolve/main/learn-hf-text-classification/01-hugging-face-workflow.png\"\n",
        "     alt=\"The diagram shows the Hugging Face model development workflow, which includes the following steps: start with an idea or problem, get data ready (turn into tensors/create data splits), pick a pretrained model (to suit your problem), train/fine-tune the model on your custom data, evaluate the model, improve through experimentation, save and upload the fine-tuned model to the Hugging Face Hub, and turn your model into a shareable demo. Tools used in this workflow are Datasets/Tokenizers, Transformers/PEFT/Accelerate/timm, Hub/Spaces/Gradio, and Evaluate.\"\n",
        "     style=\"width: 100%; max-width: 900px; height: auto;\"/>\n",
        "     <figcaption style=\"width: 100%; box-sizing: border-box;\">A general Hugging Face workflow from idea to shared model and demo using tools from the Hugging Face ecosystem. You'll notice some of the steps don't match with our workflow outline above. This is because the text-based workflow outline above breaks some of the steps down for educational purposes. These kind of workflows are not set in stone and are more of guide than specific directions. See information on each of the tools in the <a href=\"https://huggingface.co\">Hugging Face documentation</a>.</figcaption>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MF5TShgEvyNo"
      },
      "source": [
        "# 2. Importing necessary libraries\n",
        "\n",
        "If you're running on your local computer, be sure to check out the getting [setup guide](https://www.learnhuggingface.com/extras/setup) to make sure you have everything you need.\n",
        "\n",
        "If you're using Google Colab, many of them the following libraries will be installed by default.\n",
        "\n",
        "However, we'll have to install a few extras to get everything working.\n",
        "\n",
        "We'll need to install the following libraries from the Hugging Face ecosystem:\n",
        "\n",
        "* [`transformers`](https://huggingface.co/docs/transformers/en/installation) - comes pre-installed on Google Colab but if you're running on your local machine, you can install it via `pip install transformers`.\n",
        "* [`datasets`](https://huggingface.co/docs/datasets/installation) - a library for accessing and manipulating datasets on and off the Hugging Face Hub, you can install it via `pip install datasets`.\n",
        "* [`evaluate`](https://huggingface.co/docs/evaluate/installation) - a library for evaluating machine learning model performance with various metrics, you can install it via `pip install evaluate`.\n",
        "* [`accelerate`](https://huggingface.co/docs/accelerate/basic_tutorials/install) - a library for training machine learning models faster, you can install it via `pip install accelerate`.\n",
        "* [`gradio`](https://www.gradio.app/guides/quickstart#installation) - a library for creating interactive demos of machine learning models, you can install it via `pip install gradio`.\n",
        "\n",
        "And the following library is not part of the Hugging Face ecosystem but it is helpful for evaluating our models:\n",
        "\n",
        "* [`torchmetrics`](https://lightning.ai/docs/torchmetrics/stable/) - a library containing many evaluation metrics compatible with PyTorch/Transformers, you can install it via `pip install torchmetrics`.\n",
        "\n",
        "We can also check the versions of our software with `package_name.__version__`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kRX3YggvyNq",
        "outputId": "4e66bf95-4949-4aae-fcc0-f7a296ac6b4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using transformers version: 4.48.3\n",
            "Using datasets version: 3.1.0\n",
            "Using torch version: 2.6.0+cu124\n",
            "Using torchmetrics version: 1.4.1\n"
          ]
        }
      ],
      "source": [
        "# Install/import dependencies (this is mostly for Google Colab, as the other dependences are available by default in Colab)\n",
        "try:\n",
        "  import datasets, evaluate, accelerate\n",
        "  import gradio as gr\n",
        "except ModuleNotFoundError:\n",
        "  !pip install -U datasets evaluate accelerate gradio # -U stands for \"upgrade\" so we'll get the latest version by default\n",
        "  import datasets, evaluate, accelerate\n",
        "  import gradio as gr\n",
        "\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "\n",
        "# Required for evaluation\n",
        "# Can install with !pip install torchmetrics[detection]\n",
        "import torchmetrics\n",
        "import pycocotools\n",
        "\n",
        "# Check versions (as long as you've got the following versions or higher, you should be good)\n",
        "print(f\"Using transformers version: {transformers.__version__}\")\n",
        "print(f\"Using datasets version: {datasets.__version__}\")\n",
        "print(f\"Using torch version: {torch.__version__}\")\n",
        "print(f\"Using torchmetrics version: {torchmetrics.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-p5u_cjhzoGw"
      },
      "source": [
        "# 3. Getting a dataset\n",
        "\n",
        "Okay, now we're got the required libraries, let's get a dataset.\n",
        "\n",
        "The dataset you often determines the type of model you use as well as the quality of the outputs of that model.\n",
        "\n",
        "Meaning, if you have a high quality dataset, chances are, your future model could also have high quality outputs.\n",
        "\n",
        "It also means if your dataset is of poor quality, your model will likely also have poor quality outputs.\n",
        "\n",
        "For an object detection problem, your dataset will likely come in the form of a group of images as well as a file with annotations belonging to those images.\n",
        "\n",
        "For example, you might have the following setup:\n",
        "\n",
        "```\n",
        "folder_of_images/\n",
        "    image_1.jpeg\n",
        "    image_2.jpeg\n",
        "    image_3.jpeg\n",
        "annotations.json\n",
        "```\n",
        "\n",
        "Where the `annotations.json` contains details about the contains of each image:\n",
        "\n",
        "```{.json filename=\"annotations.json\"}\n",
        "[\n",
        "    {\n",
        "        'image_path': 'image_1.jpeg',\n",
        "        'image_id': 42,\n",
        "        'annotations':\n",
        "            {\n",
        "                'file_name': ['image_1.jpeg'],\n",
        "                'image_id': [42],\n",
        "                'category_id': [1],\n",
        "                'bbox': [\n",
        "                            [360.20001220703125, 528.5, 177.1999969482422, 261.79998779296875],\n",
        "                        ],\n",
        "                'area': [46390.9609375]\n",
        "            },\n",
        "        'label_source': 'manual_prodigy_label',\n",
        "        'image_source': 'manual_taken_photo'\n",
        "    },\n",
        "\n",
        "    ...(more labels down here)\n",
        "]\n",
        "```\n",
        "\n",
        "Don't worry too much about the exact meaning of everything in the above `annotations.json` file for now (this is only one example, there are many different ways object detection information could be displayed).\n",
        "\n",
        "The main point is that each target image is paired with an assosciated label.\n",
        "\n",
        "Now like all good machine learning cooking shows, I've prepared a dataset from earlier.\n",
        "\n",
        "TK image - dataset on Hugging Face\n",
        "\n",
        "It's stored on Hugging Face Datasets (also called the Hugging Face Hub) under the name [`mrdbourke/trashify_manual_labelled_images`](https://huggingface.co/datasets/mrdbourke/trashify_manual_labelled_images).\n",
        "\n",
        "This is a dataset I've collected manually by hand (yes, by picking up 1000+ pieces of trash and photographing it) as well as labelled by hand (by drawing boxes on each image with a labelling tool called [Prodigy](https://prodi.gy/features/computer-vision))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTeSsSiRzoGx"
      },
      "source": [
        "\n",
        "## 3.1 Loading the dataset\n",
        "\n",
        "To load a dataset stored on the Hugging Face Hub we can use the [`datasets.load_dataset(path=NAME_OR_PATH_OF_DATASET)`](https://huggingface.co/docs/datasets/en/package_reference/loading_methods#datasets.load_dataset) function and pass it the name/path of the dataset we want to load.\n",
        "\n",
        "In our case, our dataset name is `mrdbourke/trashify_manual_labelled_images` (you can also change this for your own dataset).\n",
        "\n",
        "And since our dataset is hosted on Hugging Face, when we run the following code for the first time, it will download it.\n",
        "\n",
        "If your target dataset is quite large, this download may take a while.\n",
        "\n",
        "However, once the dataset is downloaded, subsequent reloads will be mush faster.\n",
        "\n",
        "Let's load our dataset and check it out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vAKDnQhzoGy",
        "outputId": "51db76d1-f865-4c1c-a20e-f2efb30d339a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['image', 'image_id', 'annotations', 'label_source', 'image_source'],\n",
              "        num_rows: 1128\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load our Trashify dataset\n",
        "dataset = load_dataset(path=\"mrdbourke/trashify_manual_labelled_images\")\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQ9-BnSRzoGz",
        "outputId": "1e3c7899-da3b-4d24-dcd0-b32b81fcaa95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Length of original dataset: 1128\n",
            "[INFO] Dataset features:\n",
            "{'annotations': Sequence(feature={'area': Value(dtype='float32', id=None),\n",
            "                                  'bbox': Sequence(feature=Value(dtype='float32',\n",
            "                                                                 id=None),\n",
            "                                                   length=4,\n",
            "                                                   id=None),\n",
            "                                  'category_id': ClassLabel(names=['bin',\n",
            "                                                                   'hand',\n",
            "                                                                   'not_bin',\n",
            "                                                                   'not_hand',\n",
            "                                                                   'not_trash',\n",
            "                                                                   'trash',\n",
            "                                                                   'trash_arm'],\n",
            "                                                            id=None),\n",
            "                                  'file_name': Value(dtype='string', id=None),\n",
            "                                  'image_id': Value(dtype='int64', id=None),\n",
            "                                  'iscrowd': Value(dtype='int64', id=None)},\n",
            "                         length=-1,\n",
            "                         id=None),\n",
            " 'image': Image(mode=None, decode=True, id=None),\n",
            " 'image_id': Value(dtype='int64', id=None),\n",
            " 'image_source': Value(dtype='string', id=None),\n",
            " 'label_source': Value(dtype='string', id=None)}\n"
          ]
        }
      ],
      "source": [
        "print(f\"[INFO] Length of original dataset: {len(dataset['train'])}\")\n",
        "print(f\"[INFO] Dataset features:\")\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "pprint(dataset['train'].features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldYYmSVBzoG0"
      },
      "source": [
        "## 3.2 Viewing a single sample from our data\n",
        "\n",
        "Let's check out a single sample from our dataset.\n",
        "\n",
        "We can index on a single sample of the `\"train\"` set just like indexing on a Python list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1sAP_DwzoG0",
        "outputId": "ecf9764b-f094-467e-d595-f8cd9ae18a2b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'image': <PIL.Image.Image image mode=RGB size=960x1280>,\n",
              " 'image_id': 745,\n",
              " 'annotations': {'file_name': ['094f4f41-dc07-4704-96d7-8d5e82c9edb9.jpeg',\n",
              "   '094f4f41-dc07-4704-96d7-8d5e82c9edb9.jpeg',\n",
              "   '094f4f41-dc07-4704-96d7-8d5e82c9edb9.jpeg'],\n",
              "  'image_id': [745, 745, 745],\n",
              "  'category_id': [5, 1, 0],\n",
              "  'bbox': [[333.1000061035156,\n",
              "    611.2000122070312,\n",
              "    244.89999389648438,\n",
              "    321.29998779296875],\n",
              "   [504.0, 612.9000244140625, 451.29998779296875, 650.7999877929688],\n",
              "   [202.8000030517578,\n",
              "    366.20001220703125,\n",
              "    532.9000244140625,\n",
              "    555.4000244140625]],\n",
              "  'iscrowd': [0, 0, 0],\n",
              "  'area': [78686.3671875, 293706.03125, 295972.65625]},\n",
              " 'label_source': 'manual_prodigy_label',\n",
              " 'image_source': 'manual_taken_photo'}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# View a single sample of the dataset\n",
        "dataset[\"train\"][42]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrJijvk_zoG1"
      },
      "source": [
        "We see a few more details here compared to just looking at the features.\n",
        "\n",
        "We notice the `image` is a [`PIL.Image`](https://pillow.readthedocs.io/en/stable/reference/Image.html) with size `960x1280` (width x height).\n",
        "\n",
        "And the `file_name` is a UUID (Universially Unique Identifier, made with [`uuid.uuid4()`](https://docs.python.org/3/library/uuid.html#uuid.uuid4)).\n",
        "\n",
        "The `bbox` field in the `annotations` key contains a list of bounding boxes assosciated with the image.\n",
        "\n",
        "In this case, there are 3 different bounding boxes.\n",
        "\n",
        "With the `category_id` values of `5`, `1`, `0` (we'll map these to class names shortly).\n",
        "\n",
        "Let's inspect a single bounding box."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTCFrEvtzoG1",
        "outputId": "d67d9cbf-eb06-40cb-f982-798be83dd376"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[333.1000061035156, 611.2000122070312, 244.89999389648438, 321.29998779296875]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[\"train\"][42][\"annotations\"][\"bbox\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19_vXniwzoG1"
      },
      "source": [
        "This array gives us the coordinates of a single bounding box in the format `XYWH`.\n",
        "\n",
        "Where:\n",
        "\n",
        "* `X` is the x-coordinate of the top left corner of the box (`333.1`).\n",
        "* `Y` is the y-coordinate of the top left corner of the box (`611.2`).\n",
        "* `W` is the width of the box (`244.9`).\n",
        "* `H` is the height of the box (`321.3`).\n",
        "\n",
        "All of these values are in absolute pixel values (meaning an x-coordinate of `333.1` is `333.1` pixels across on the x-axis)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zddfu7ACzoG1"
      },
      "source": [
        "## 3.3 Extracting the category names from our data\n",
        "\n",
        "Before we start to visualize our sample image and bounding boxes, let's extract the category names from our dataset.\n",
        "\n",
        "We can do so by accessing the `features` attribute our of `dataset` and then following it through to find the `category_id` feature, this contains a list of our text-based class names.\n",
        "\n",
        "Let's access the class names in our dataset and save them to a variable `categories`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yljRJC75zoG2",
        "outputId": "45c1d3ad-d017-4a5d-b687-7cb89a947219"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['bin', 'hand', 'not_bin', 'not_hand', 'not_trash', 'trash', 'trash_arm']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get the categories from the dataset\n",
        "# Note: This requires the dataset to have been uploaded with this information setup, not all datasets will have this available.\n",
        "categories = dataset[\"train\"].features[\"annotations\"].feature[\"category_id\"]\n",
        "\n",
        "# Get the names attribute\n",
        "categories.names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQCIpuXhzoG2"
      },
      "source": [
        "We get the following class names:\n",
        "\n",
        "* `bin` - A rubbish bin or trash can.\n",
        "* `hand` - A person's hand.\n",
        "* `not_bin` - Negative version of `bin` for items that look like a `bin` but shouldn't be identified as one.\n",
        "* `not_hand` - Negative version of `hand` for items that look like a `hand` but shouldn't be identified as one.\n",
        "* `not_trash` - Negative version of `trash` for items that look like `trash` but shouldn't be identified as it.\n",
        "* `trash` - An item of trash you might find on a walk such as an old plastic bottle, food wrapper, cigarette butt or used coffee cup.\n",
        "* `trash_arm` - A mechanical arm used for picking up trash.\n",
        "\n",
        "The goal of our computer vision model will be: given an image, detect items belonging to these target classes if they are present."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qV6PkwfhzoG3"
      },
      "source": [
        "## 3.4 Creating a mapping from numbers to labels\n",
        "\n",
        "Now we've got our text-based class names, let's create a mapping from label to ID and ID to label.\n",
        "\n",
        "For each of these, Hugging Face use the terminology `label2id` and `id2label` respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A38XayuWzoG3",
        "outputId": "9548386a-d206-4715-a0b8-0815abc07fe0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label to ID mapping:\n",
            "{'bin': 0, 'hand': 1, 'not_bin': 2, 'not_hand': 3, 'not_trash': 4, 'trash': 5, 'trash_arm': 6}\n",
            "\n",
            "ID to label mapping:\n",
            "{0: 'bin', 1: 'hand', 2: 'not_bin', 3: 'not_hand', 4: 'not_trash', 5: 'trash', 6: 'trash_arm'}\n"
          ]
        }
      ],
      "source": [
        "# Map ID's to class names and vice versa\n",
        "id2label = {i: class_name for i, class_name in enumerate(categories.names)}\n",
        "label2id = {value: key for key, value in id2label.items()}\n",
        "\n",
        "print(f\"Label to ID mapping:\\n{label2id}\\n\")\n",
        "print(f\"ID to label mapping:\\n{id2label}\")\n",
        "# id2label, label2id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_jGjnxmzoG3"
      },
      "source": [
        "## 3.5 Creating a colour palette\n",
        "\n",
        "Ok we know which class name matches to which ID, now let's create a dictionary of different colours we can use to display our bounding boxes.\n",
        "\n",
        "It's one thing to plot bounding boxes, it's another thing to make them look nice.\n",
        "\n",
        "And we always want our plots looking nice!\n",
        "\n",
        "We'll colour the positive classes `bin`, `hand`, `trash`, `trash_arm` in nice bright colours.\n",
        "\n",
        "And the negative classes `not_bin`, `not_hand`, `not_trash` in a light red colour to indicate they're the negative versions.\n",
        "\n",
        "Our colour dictionary will map `class_name` -> `(red, green, blue)`  (or [RGB](https://en.wikipedia.org/wiki/RGB_color_model)) colour values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMtFPn9fzoG4"
      },
      "outputs": [],
      "source": [
        "# Make colour dictionary\n",
        "colour_palette = {\n",
        "    'bin': (0, 0, 224),         # Bright Blue (High contrast with greenery) in format (red, green, blue)\n",
        "    'not_bin': (255, 80, 80),   # Light Red to indicate negative class\n",
        "\n",
        "    'hand': (148, 0, 211),      # Dark Purple (Contrasts well with skin tones)\n",
        "    'not_hand': (255, 80, 80),  # Light Red to indicate negative class\n",
        "\n",
        "    'trash': (0, 255, 0),       # Bright Green (For trash-related items)\n",
        "    'not_trash': (255, 80, 80), # Light Red to indicate negative class\n",
        "\n",
        "    'trash_arm': (255, 140, 0), # Deep Orange (Highly visible)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6Hqz5mBzoG5",
        "outputId": "bc052050-e99f-4056-b732-92fe8fd35bf4"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x100 with 7 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Normalize RGB values to 0-1 range\n",
        "def normalize_rgb(rgb_tuple):\n",
        "    return tuple(x/255 for x in rgb_tuple)\n",
        "\n",
        "# Turn colors into normalized RGB values for matplotlib\n",
        "colors_and_labels_rgb = [(key, normalize_rgb(value)) for key, value in colour_palette.items()]\n",
        "\n",
        "# Create figure and axis\n",
        "fig, ax = plt.subplots(1, 7, figsize=(8, 1))\n",
        "\n",
        "# Flatten the axis array for easier iteration\n",
        "ax = ax.flatten()\n",
        "\n",
        "# Plot each color square\n",
        "for idx, (label, color) in enumerate(colors_and_labels_rgb):\n",
        "    ax[idx].add_patch(plt.Rectangle(xy=(0, 0),\n",
        "                                    width=1,\n",
        "                                    height=1,\n",
        "                                    facecolor=color))\n",
        "    ax[idx].set_title(label)\n",
        "    ax[idx].set_xlim(0, 1)\n",
        "    ax[idx].set_ylim(0, 1)\n",
        "    ax[idx].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "07ZTllwsrMbO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}