{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/banno-0720/Deep-Learning-Projects/blob/main/Belief_State_Transformers_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jcWA_NNQyLw"
      },
      "source": [
        "# Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtBL73gMQkLe"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets gradio huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7njkW3uQkIm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config, GPT2Model\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import HfApi, notebook_login\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUS-rYcxQ2RN"
      },
      "source": [
        "# Loading Movie Dialogue Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECY9GgpRQkFM"
      },
      "outputs": [],
      "source": [
        "# 1) Download and unzip the raw Cornell dataset\n",
        "!wget -q http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip -O cornell.zip\n",
        "!unzip -q cornell.zip -d cornell_data\n",
        "\n",
        "# 2) Read raw lines and conversations\n",
        "import os\n",
        "import re\n",
        "from itertools import islice\n",
        "from datasets import Dataset\n",
        "\n",
        "# Load the lines into a dict: lineID → text\n",
        "lines_path = \"cornell_data/cornell movie-dialogs corpus/movie_lines.txt\"\n",
        "conv_path  = \"cornell_data/cornell movie-dialogs corpus/movie_conversations.txt\"\n",
        "\n",
        "id2line = {}\n",
        "with open(lines_path, encoding=\"latin-1\") as f:\n",
        "    for line in f:\n",
        "        # Format: lineID +++$+++ characterID +++$+++ movieID +++$+++ character name +++$+++ text\n",
        "        parts = line.strip().split(\" +++$+++ \")\n",
        "        if len(parts) == 5:\n",
        "            id2line[parts[0]] = parts[4]\n",
        "\n",
        "# 3) Build (context, response) pairs from the first 1,000 conversations\n",
        "examples = []\n",
        "with open(conv_path, encoding=\"latin-1\") as f:\n",
        "    for conv in islice(f, 1000):\n",
        "        parts = conv.strip().split(\" +++$+++ \")\n",
        "        # parts[-1] is a string like \"['L1045','L1044',...]\"\n",
        "        ids = re.findall(r\"L[0-9]+\", parts[-1])\n",
        "        # For each adjacent pair in the conversation, get context/response\n",
        "        for i in range(len(ids) - 1):\n",
        "            c_id, r_id = ids[i], ids[i+1]\n",
        "            if c_id in id2line and r_id in id2line:\n",
        "                examples.append({\n",
        "                    \"context\":  id2line[c_id],\n",
        "                    \"response\": id2line[r_id]\n",
        "                })\n",
        "\n",
        "# 4) Take a small slice and create a HuggingFace Dataset\n",
        "small = examples[:3000]   # 3K pairs for Colab‐friendly speed\n",
        "data  = Dataset.from_list(small)\n",
        "split = data.train_test_split(test_size=0.2, shuffle=True)\n",
        "train_ds, val_ds = split[\"train\"], split[\"test\"]\n",
        "\n",
        "print(f\"Loaded {len(examples)} pairs, using {len(train_ds)} train and {len(val_ds)} val examples.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktDwkLzQQj-A"
      },
      "source": [
        "# Data Preprocessing and Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bN87axTpQj6V"
      },
      "outputs": [],
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token  # so pad_id == eos_id\n",
        "MAX_LEN = 128\n",
        "\n",
        "def tokenize_function(example):\n",
        "    # 1) tokenize separately, no special tokens\n",
        "    ctx_ids = tokenizer.encode(example['context'], add_special_tokens=False)\n",
        "    resp_ids = tokenizer.encode(example['response'], add_special_tokens=False)\n",
        "\n",
        "    # 2) build the full sequence with EOS separators\n",
        "    eos = tokenizer.eos_token_id\n",
        "    input_ids = ctx_ids + [eos] + resp_ids + [eos]\n",
        "\n",
        "    # 3) truncate or pad\n",
        "    input_ids = input_ids[:MAX_LEN]\n",
        "    padding_length = MAX_LEN - len(input_ids)\n",
        "    input_ids = input_ids + [tokenizer.pad_token_id] * padding_length\n",
        "\n",
        "    # 4) build labels: mask all context + the first EOS, keep only response tokens\n",
        "    labels = [-100] * MAX_LEN\n",
        "    # response starts at index len(ctx_ids) + 1\n",
        "    start = len(ctx_ids) + 1\n",
        "    end   = start + len(resp_ids)\n",
        "    end   = min(end, MAX_LEN)  # in case of truncation\n",
        "\n",
        "    labels[start:end] = input_ids[start:end]\n",
        "\n",
        "    return {\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask': [1 if i < end else 0 for i in range(MAX_LEN)],\n",
        "        'labels': labels\n",
        "    }\n",
        "\n",
        "# Then remap:\n",
        "train_tokenized = train_ds.map(tokenize_function, batched=False)\n",
        "val_tokenized   = val_ds.map(tokenize_function, batched=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoS4ViILQj2k"
      },
      "source": [
        "# Baseline Transformer: GPT-2 Chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ows88ulLQjzj"
      },
      "outputs": [],
      "source": [
        "# Load GPT-2 small model\n",
        "config = GPT2Config.from_pretrained('gpt2')\n",
        "config.pad_token_id = tokenizer.pad_token_id\n",
        "model_gpt = GPT2LMHeadModel.from_pretrained('gpt2', config=config)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_gpt.to(device)\n",
        "\n",
        "# Simple training loop for a few epochs (scaled down for demo)\n",
        "optimizer = torch.optim.AdamW(model_gpt.parameters(), lr=5e-5)\n",
        "model_gpt.train()\n",
        "for epoch in range(1):\n",
        "    total_loss = 0\n",
        "    for batch in train_tokenized.shuffle().select(range(1000)):  # only 1000 samples for speed\n",
        "        input_ids = torch.tensor([batch['input_ids']]).to(device)\n",
        "        labels = torch.tensor([batch['labels']]).to(device)\n",
        "        outputs = model_gpt(input_ids, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1} - Loss: {total_loss/1000:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5g5Gwo8VQjv_"
      },
      "source": [
        "# Belief State Transformer Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2R3H6L_RQjs6"
      },
      "outputs": [],
      "source": [
        "class BeliefStateTransformer(nn.Module):\n",
        "    def __init__(self, hidden_size=768, vocab_size=None):\n",
        "        super().__init__()\n",
        "        # Use GPT-2 Transformer encoders for forward and backward contexts\n",
        "        self.forward_encoder  = GPT2Model(config)  # encodes prefix\n",
        "        self.backward_encoder = GPT2Model(config)  # encodes suffix (we will input reversed suffix)\n",
        "        # Linear output heads that take concatenated forward+back states and produce vocab logits for 2 predictions\n",
        "        # We output 2 vocab-size logits for (next token, prev token) jointly\n",
        "        self.text_head = nn.Sequential(\n",
        "            nn.Linear(hidden_size*2, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, vocab_size*2)  # 2 * vocab for two token predictions\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, suffix_ids):\n",
        "        # input_ids: [batch, T] prefix tokens\n",
        "        # suffix_ids: [batch, T] suffix tokens to encode (we will reverse them)\n",
        "        # Forward encoding of prefix (last hidden state)\n",
        "        forward_outputs = self.forward_encoder(input_ids)[0]  # [batch, T, hidden]\n",
        "        forward_state = forward_outputs[:, -1, :]  # encoding of last token\n",
        "\n",
        "        # Backward encoding: reverse suffix sequence for GPT2-style encoding\n",
        "        reversed_suffix = torch.flip(suffix_ids, dims=[1])\n",
        "        backward_outputs = self.backward_encoder(reversed_suffix)[0]\n",
        "        backward_outputs = torch.flip(backward_outputs, dims=[1])  # flip back\n",
        "        backward_state = backward_outputs[:, 0, :]  # encoding of first token of suffix\n",
        "\n",
        "        # Concatenate forward and backward representations\n",
        "        combined = torch.cat([forward_state, backward_state], dim=-1)  # [batch, hidden*2]\n",
        "        logits = self.text_head(combined)  # [batch, 2*vocab]\n",
        "        # Split logits into next-token and prev-token parts\n",
        "        next_logits, prev_logits = torch.chunk(logits, 2, dim=-1)  # each [batch, vocab]\n",
        "        return next_logits, prev_logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3fduQDHQjmr"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "bst_model = BeliefStateTransformer(hidden_size=768, vocab_size=len(tokenizer))\n",
        "bst_model.to(device)\n",
        "optimizer_bst = torch.optim.AdamW(bst_model.parameters(), lr=5e-5)\n",
        "bst_model.train()\n",
        "\n",
        "# Prepare simple training: split each response into prefix/suffix halves\n",
        "for epoch in range(1):\n",
        "    total_loss = 0.0\n",
        "    # Use only a small number of examples for demonstration\n",
        "    for i in range(0, 1000, 2):\n",
        "        batch_inputs      = []\n",
        "        batch_suffix      = []\n",
        "        batch_next_labels = []\n",
        "        batch_prev_labels = []\n",
        "\n",
        "        # Build a batch of 2 samples\n",
        "        for j in range(2):\n",
        "            text = train_ds[i+j]['response']\n",
        "            tokens = tokenizer.encode(text, max_length=50, truncation=True)\n",
        "            if len(tokens) < 4:\n",
        "                continue\n",
        "            # Split tokens into prefix and suffix parts\n",
        "            mid = len(tokens) // 2\n",
        "            prefix, suffix = tokens[:mid], tokens[mid:]\n",
        "\n",
        "            # Prepare input and suffix tensors\n",
        "            inp_ids    = torch.tensor(prefix + [tokenizer.eos_token_id]).squeeze(0)\n",
        "            suff_ids   = torch.tensor([tokenizer.eos_token_id] + suffix).squeeze(0)\n",
        "            next_label = torch.tensor([suffix[0]])   # next-token ground truth\n",
        "            prev_label = torch.tensor([prefix[-1]])  # prev-token ground truth\n",
        "\n",
        "            batch_inputs.append(inp_ids)\n",
        "            batch_suffix.append(suff_ids)\n",
        "            batch_next_labels.append(next_label)\n",
        "            batch_prev_labels.append(prev_label)\n",
        "\n",
        "        if not batch_inputs:\n",
        "            continue\n",
        "\n",
        "        # Pad all sequences in the batch to the same length\n",
        "        pad_id = tokenizer.eos_token_id\n",
        "        input_ids  = pad_sequence(batch_inputs, batch_first=True, padding_value=pad_id).to(device)\n",
        "        suffix_ids = pad_sequence(batch_suffix, batch_first=True, padding_value=pad_id).to(device)\n",
        "\n",
        "        # Stack label tensors\n",
        "        next_labels = torch.cat(batch_next_labels).to(device)\n",
        "        prev_labels = torch.cat(batch_prev_labels).to(device)\n",
        "\n",
        "        # Forward pass and loss\n",
        "        next_logits, prev_logits = bst_model(input_ids, suffix_ids)\n",
        "        loss_next = nn.CrossEntropyLoss()(next_logits, next_labels)\n",
        "        loss_prev = nn.CrossEntropyLoss()(prev_logits, prev_labels)\n",
        "        loss = loss_next + loss_prev\n",
        "\n",
        "        # Backward & optimize\n",
        "        loss.backward()\n",
        "        optimizer_bst.step()\n",
        "        optimizer_bst.zero_grad()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"BST Epoch {epoch+1} - Loss: {total_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCRsa1NARqB7"
      },
      "source": [
        "# Model Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33SQ24VbRtiH"
      },
      "outputs": [],
      "source": [
        "model_gpt.eval()\n",
        "bst_model.eval()\n",
        "\n",
        "def reply_baseline(user_input):\n",
        "    # Tokenize with padding/truncation so we get an attention_mask\n",
        "    encoding = tokenizer(\n",
        "        user_input,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    ).to(device)\n",
        "\n",
        "    # Generate up to 50 new tokens, block repeated trigrams\n",
        "    gen_ids = model_gpt.generate(\n",
        "        input_ids=encoding[\"input_ids\"],\n",
        "        attention_mask=encoding[\"attention_mask\"],\n",
        "        max_new_tokens=50,\n",
        "        no_repeat_ngram_size=3,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    # Strip off the prompt tokens\n",
        "    reply = tokenizer.decode(\n",
        "        gen_ids[0, encoding[\"input_ids\"].size(1):],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "    return reply\n",
        "\n",
        "def reply_bst(user_input):\n",
        "    # 1) Tokenize the prefix with mask\n",
        "    encoding = tokenizer(\n",
        "        user_input,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    ).to(device)\n",
        "\n",
        "    input_ids     = encoding[\"input_ids\"]\n",
        "    attn_mask     = encoding[\"attention_mask\"]\n",
        "    # 2) Build an “empty” suffix (just EOS) with mask\n",
        "    suffix_ids    = torch.tensor([[tokenizer.eos_token_id]], device=device)\n",
        "    suffix_mask   = torch.ones_like(suffix_ids, device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Pass both through BST\n",
        "        next_logits, prev_logits = bst_model(input_ids, suffix_ids)\n",
        "\n",
        "        # Extract belief‐state vector\n",
        "        fwd = bst_model.forward_encoder(input_ids, attention_mask=attn_mask)[0][:, -1, :]\n",
        "        bwd = bst_model.backward_encoder(torch.flip(suffix_ids, [1]))[0]\n",
        "        bwd = torch.flip(bwd, [1])[:, 0, :]\n",
        "        belief_state = (fwd + bwd).cpu().numpy().flatten()[:10]  # first 10 dims\n",
        "\n",
        "        # For reply we again use GPT-2 but with proper mask\n",
        "        gen_ids = model_gpt.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attn_mask,\n",
        "            max_new_tokens=30,\n",
        "            no_repeat_ngram_size=3,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "        bst_reply = tokenizer.decode(\n",
        "            gen_ids[0, input_ids.size(1):],\n",
        "            skip_special_tokens=True\n",
        "        )\n",
        "\n",
        "    # Format belief as string\n",
        "    belief_str = \", \".join(f\"{x:.3f}\" for x in belief_state)\n",
        "    return bst_reply, belief_str\n",
        "\n",
        "# Example turn:\n",
        "context = \"Hi, how are you?\"\n",
        "print(\"User:\", context)\n",
        "print(\"GPT-2 Baseline Reply:\", reply_baseline(context))\n",
        "bst_resp, bst_belief = reply_bst(context)\n",
        "print(\"Belief-State Model Reply:\", bst_resp)\n",
        "print(\"Belief State (first 10 dims):\", bst_belief)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnY6jWFVRysj"
      },
      "source": [
        "# Interactive Chat Demo with Gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtD1sS6kR2_0"
      },
      "outputs": [],
      "source": [
        "# Make sure your reply functions return exactly what we expect:\n",
        "\n",
        "def reply_baseline(user_input):\n",
        "    encoding = tokenizer(\n",
        "        user_input,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    ).to(device)\n",
        "    gen_ids = model_gpt.generate(\n",
        "        input_ids=encoding[\"input_ids\"],\n",
        "        attention_mask=encoding[\"attention_mask\"],\n",
        "        max_new_tokens=50,\n",
        "        no_repeat_ngram_size=3,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    return tokenizer.decode(\n",
        "        gen_ids[0, encoding[\"input_ids\"].size(1):],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "def reply_bst(user_input):\n",
        "    encoding = tokenizer(\n",
        "        user_input,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    ).to(device)\n",
        "    input_ids   = encoding[\"input_ids\"]\n",
        "    attn_mask   = encoding[\"attention_mask\"]\n",
        "    suffix_ids  = torch.tensor([[tokenizer.eos_token_id]], device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # belief‐state vector\n",
        "        fwd = bst_model.forward_encoder(input_ids, attention_mask=attn_mask)[0][:, -1, :]\n",
        "        bwd = bst_model.backward_encoder(torch.flip(suffix_ids, [1]))[0]\n",
        "        bwd = torch.flip(bwd, [1])[:, 0, :]\n",
        "        belief_vec = (fwd + bwd).cpu().numpy().flatten()[:10]\n",
        "\n",
        "        # reply text via GPT-2\n",
        "        gen_ids = model_gpt.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attn_mask,\n",
        "            max_new_tokens=30,\n",
        "            no_repeat_ngram_size=3,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "        bst_reply = tokenizer.decode(\n",
        "            gen_ids[0, input_ids.size(1):],\n",
        "            skip_special_tokens=True\n",
        "        )\n",
        "\n",
        "    # format belief vector as comma-separated string\n",
        "    belief_str = \", \".join(f\"{x:.3f}\" for x in belief_vec)\n",
        "    return bst_reply, belief_str\n",
        "\n",
        "# Corrected chat_models:\n",
        "def chat_models(user_input):\n",
        "    baseline_resp = reply_baseline(user_input)\n",
        "    bst_resp, belief_str = reply_bst(user_input)\n",
        "    return baseline_resp, bst_resp, belief_str\n",
        "\n",
        "# Rebuild the interface:\n",
        "import gradio as gr\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=chat_models,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Type your message...\", label=\"Your Message\"),\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Baseline (GPT-2) Reply\"),\n",
        "        gr.Textbox(label=\"Belief-State Model Reply\"),\n",
        "        gr.Textbox(label=\"Belief State (first 10 dims)\")\n",
        "    ],\n",
        "    title=\"Movie Dialogue Belief-State Chatbot\",\n",
        "    description=\"Enter a movie-style dialogue line; see both models' replies and the BST's hidden state.\"\n",
        ")\n",
        "\n",
        "# Launch (you can add share=True if you need a public URL)\n",
        "iface.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0ac7J-wR9Vw"
      },
      "source": [
        "# Saving and Pushing Models to Hugging Face Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gY55XyO2SEQi"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import HfApi\n",
        "import os\n",
        "import torch\n",
        "import json\n",
        "\n",
        "# 1) Make sure you’ve already created the repo on HF Hub:\n",
        "api = HfApi()\n",
        "repo_bst = \"HimanshuGoyal2004/movie-dialog-beliefstate\"\n",
        "api.create_repo(repo_bst, exist_ok=True)\n",
        "\n",
        "# 2) Save your model’s state_dict + a minimal “config.json”\n",
        "#    and the tokenizer to a local folder\n",
        "bst_dir = \"bst_model/\"\n",
        "os.makedirs(bst_dir, exist_ok=True)\n",
        "\n",
        "# Save weights\n",
        "torch.save(bst_model.state_dict(), os.path.join(bst_dir, \"pytorch_model.bin\"))\n",
        "\n",
        "# Save a simple config so users know how to rebuild:\n",
        "config = {\n",
        "    \"hidden_size\": 768,\n",
        "    \"vocab_size\": len(tokenizer),\n",
        "    # (add any other hyperparams your __init__ needs)\n",
        "}\n",
        "with open(os.path.join(bst_dir, \"config.json\"), \"w\") as f:\n",
        "    json.dump(config, f)\n",
        "\n",
        "# Save the tokenizer files too\n",
        "tokenizer.save_pretrained(bst_dir)\n",
        "\n",
        "# 3) Upload the entire folder to the Hub\n",
        "api.upload_folder(\n",
        "    folder_path=bst_dir,\n",
        "    repo_id=repo_bst,\n",
        "    repo_type=\"model\",\n",
        "    commit_message=\"Add Belief State Transformer model files\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deploying to Hugging Face Spaces"
      ],
      "metadata": {
        "id": "MmiCyEUjAj0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (In a local or Colab terminal)\n",
        "!huggingface-cli login\n",
        "!huggingface-cli repo create HimanshuGoyal2004/movie-dialog-belief-demo --space-sdk gradio --type space"
      ],
      "metadata": {
        "id": "SqidsNjsApbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "api = HfApi()\n",
        "# Replace with your username and desired space name\n",
        "space_id = \"HimanshuGoyal2004/movie-dialog-belief-demo\"\n",
        "\n",
        "# repo_type=\"space\" and space_sdk=\"gradio\" tell HF this is a Gradio Space\n",
        "api.create_repo(\n",
        "    repo_id=space_id,\n",
        "    repo_type=\"space\",\n",
        "    space_sdk=\"gradio\",\n",
        "    exist_ok=True   # won't fail if it already exists\n",
        ")\n",
        "\n",
        "print(f\"✅ Created space: https://huggingface.co/spaces/{space_id}\")"
      ],
      "metadata": {
        "id": "hDzsfVblA6-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "\n",
        "# You can get embeddable HTML code for your demo by clicking the \"Embed\" button on the demo page\n",
        "HTML(data='''\n",
        "<iframe\n",
        "\tsrc=\"https://himanshugoyal2004-movie-dialog-belief-demo.hf.space\"\n",
        "\tframeborder=\"0\"\n",
        "\twidth=\"850\"\n",
        "\theight=\"450\"\n",
        "></iframe>\n",
        "''')"
      ],
      "metadata": {
        "id": "r05X6eBYnTbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7Nh1C6T-rB0E"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}