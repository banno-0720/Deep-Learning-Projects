{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPTn8IB7HZTtLO4Mij6OEcO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/banno-0720/Deep-Learning-Projects/blob/main/IntrusionDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install opendatasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05hGqQrVcG4X",
        "outputId": "915c0a24-af6a-4e85-9979-d6db8f3e9a49"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from opendatasets) (4.67.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (from opendatasets) (1.6.17)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from opendatasets) (8.1.8)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (1.17.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2.32.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2.3.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (6.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach->kaggle->opendatasets) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.11/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle->opendatasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle->opendatasets) (3.10)\n",
            "Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5JkdgbazXOt0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import requests\n",
        "from pathlib import Path\n",
        "import opendatasets as od"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Load Dataset"
      ],
      "metadata": {
        "id": "dKIIyw2BYdrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "od.download(\"https://www.kaggle.com/datasets/chethuhn/network-intrusion-dataset\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiZrh4TfDZmO",
        "outputId": "5cdb465a-4c06-4ab6-a32f-405520482437"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: himanshugoyal2004\n",
            "Your Kaggle Key: ··········\n",
            "Dataset URL: https://www.kaggle.com/datasets/chethuhn/network-intrusion-dataset\n",
            "Downloading network-intrusion-dataset.zip to ./network-intrusion-dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 230M/230M [00:10<00:00, 23.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "    # Download dataset\n",
        "    od.download(\"https://www.kaggle.com/datasets/chethuhn/network-intrusion-dataset\")\n",
        "    dataset_file = \"/content/network-intrusion-dataset/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\"\n",
        "\n",
        "    # Load dataset into pandas DataFrame\n",
        "    df = pd.read_csv(dataset_file)\n",
        "\n",
        "    # Clean up column names (remove spaces and make lower case)\n",
        "    df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
        "\n",
        "    # Inspect cleaned column names\n",
        "    print(\"Cleaned Columns in the dataset:\")\n",
        "    print(df.columns)\n",
        "\n",
        "    # Filter relevant columns\n",
        "    features = ['flow_duration', 'total_fwd_packets', 'total_backward_packets',\n",
        "                'fwd_packet_length_mean', 'bwd_packet_length_mean',\n",
        "                'flow_packets/s', 'label']\n",
        "    df = df[features]\n",
        "    df.dropna(inplace=True)  # Handle missing values\n",
        "    return df\n",
        "\n",
        "# # Test the function\n",
        "# data = load_data()\n",
        "# print(data.head())\n"
      ],
      "metadata": {
        "id": "oKlc5W7V6XOC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Preprocess Data"
      ],
      "metadata": {
        "id": "IrRdLS107CP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(data):\n",
        "    # Convert labels to binary (e.g., 'DDoS' -> 1, others -> 0)\n",
        "    data['label'] = data['label'].apply(lambda x: 1 if x == 'DDoS' else 0)\n",
        "\n",
        "    # Replace infinite values with NaN and drop them\n",
        "    data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "    # Drop rows with NaN values\n",
        "    data.dropna(inplace=True)\n",
        "\n",
        "    # Normalize numeric features\n",
        "    scaler = MinMaxScaler()\n",
        "    X = scaler.fit_transform(data.drop(columns=['label']))\n",
        "    y = data['label'].values\n",
        "\n",
        "    # Group data into sequences (example: group by batches of 10 rows)\n",
        "    sequence_length = 10\n",
        "    X_sequences, y_sequences = [], []\n",
        "    for i in range(0, len(X) - sequence_length):\n",
        "        X_sequences.append(X[i:i+sequence_length])\n",
        "        y_sequences.append(y[i+sequence_length - 1])  # Label for the sequence\n",
        "\n",
        "    return np.array(X_sequences), np.array(y_sequences)\n"
      ],
      "metadata": {
        "id": "kJF4mEE77Gmi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. PyTorch Dataset Class"
      ],
      "metadata": {
        "id": "8KUdqE__7JJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DDoSDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]"
      ],
      "metadata": {
        "id": "oYqIDR3I7L--"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. LSTM Model"
      ],
      "metadata": {
        "id": "nfLVk5mV7OeG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DDoSLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):\n",
        "        super(DDoSLSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, (hn, cn) = self.lstm(x)\n",
        "        out = self.fc(hn[-1])  # Use the last hidden state for classification\n",
        "        return self.sigmoid(out)"
      ],
      "metadata": {
        "id": "EGCjc8vv7Qbu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Training Function"
      ],
      "metadata": {
        "id": "jhWGgYHO7R_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, epochs, learning_rate):\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_batch).squeeze()\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_preds, val_targets = [], []\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch in val_loader:\n",
        "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "                outputs = model(X_batch).squeeze()\n",
        "                loss = criterion(outputs, y_batch)\n",
        "                val_loss += loss.item()\n",
        "                val_preds.extend((outputs > 0.5).cpu().numpy())\n",
        "                val_targets.extend(y_batch.cpu().numpy())\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}, Val Accuracy: {accuracy_score(val_targets, val_preds):.4f}\")"
      ],
      "metadata": {
        "id": "CytUFk-f7T8q"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Main Execution"
      ],
      "metadata": {
        "id": "BoervOo69Txr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_data()\n",
        "X, y = preprocess_data(data)\n",
        "\n",
        "# Split into train and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataset = DDoSDataset(X_train, y_train)\n",
        "val_dataset = DDoSDataset(X_val, y_val)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define model\n",
        "input_size = X.shape[2]  # Number of features\n",
        "hidden_size = 64\n",
        "num_layers = 2\n",
        "output_size = 1\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = DDoSLSTM(input_size, hidden_size, num_layers, output_size).to(device)\n",
        "\n",
        "# Train model\n",
        "train_model(model, train_loader, val_loader, epochs=10, learning_rate=0.001)\n",
        "\n",
        "# Evaluate final performance\n",
        "print(\"Training Complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GW9uwrt-7WW1",
        "outputId": "49740ad7-e9f1-470a-b188-c6693434abbd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping, found downloaded files in \"./network-intrusion-dataset\" (use force=True to force download)\n",
            "Cleaned Columns in the dataset:\n",
            "Index(['destination_port', 'flow_duration', 'total_fwd_packets',\n",
            "       'total_backward_packets', 'total_length_of_fwd_packets',\n",
            "       'total_length_of_bwd_packets', 'fwd_packet_length_max',\n",
            "       'fwd_packet_length_min', 'fwd_packet_length_mean',\n",
            "       'fwd_packet_length_std', 'bwd_packet_length_max',\n",
            "       'bwd_packet_length_min', 'bwd_packet_length_mean',\n",
            "       'bwd_packet_length_std', 'flow_bytes/s', 'flow_packets/s',\n",
            "       'flow_iat_mean', 'flow_iat_std', 'flow_iat_max', 'flow_iat_min',\n",
            "       'fwd_iat_total', 'fwd_iat_mean', 'fwd_iat_std', 'fwd_iat_max',\n",
            "       'fwd_iat_min', 'bwd_iat_total', 'bwd_iat_mean', 'bwd_iat_std',\n",
            "       'bwd_iat_max', 'bwd_iat_min', 'fwd_psh_flags', 'bwd_psh_flags',\n",
            "       'fwd_urg_flags', 'bwd_urg_flags', 'fwd_header_length',\n",
            "       'bwd_header_length', 'fwd_packets/s', 'bwd_packets/s',\n",
            "       'min_packet_length', 'max_packet_length', 'packet_length_mean',\n",
            "       'packet_length_std', 'packet_length_variance', 'fin_flag_count',\n",
            "       'syn_flag_count', 'rst_flag_count', 'psh_flag_count', 'ack_flag_count',\n",
            "       'urg_flag_count', 'cwe_flag_count', 'ece_flag_count', 'down/up_ratio',\n",
            "       'average_packet_size', 'avg_fwd_segment_size', 'avg_bwd_segment_size',\n",
            "       'fwd_header_length.1', 'fwd_avg_bytes/bulk', 'fwd_avg_packets/bulk',\n",
            "       'fwd_avg_bulk_rate', 'bwd_avg_bytes/bulk', 'bwd_avg_packets/bulk',\n",
            "       'bwd_avg_bulk_rate', 'subflow_fwd_packets', 'subflow_fwd_bytes',\n",
            "       'subflow_bwd_packets', 'subflow_bwd_bytes', 'init_win_bytes_forward',\n",
            "       'init_win_bytes_backward', 'act_data_pkt_fwd', 'min_seg_size_forward',\n",
            "       'active_mean', 'active_std', 'active_max', 'active_min', 'idle_mean',\n",
            "       'idle_std', 'idle_max', 'idle_min', 'label'],\n",
            "      dtype='object')\n",
            "Epoch 1/10, Train Loss: 0.0902, Val Loss: 0.0194, Val Accuracy: 0.9967\n",
            "Epoch 2/10, Train Loss: 0.0212, Val Loss: 0.0089, Val Accuracy: 0.9982\n",
            "Epoch 3/10, Train Loss: 0.0136, Val Loss: 0.0081, Val Accuracy: 0.9987\n",
            "Epoch 4/10, Train Loss: 0.0092, Val Loss: 0.0052, Val Accuracy: 0.9989\n",
            "Epoch 5/10, Train Loss: 0.0067, Val Loss: 0.0032, Val Accuracy: 0.9990\n",
            "Epoch 6/10, Train Loss: 0.0065, Val Loss: 0.0032, Val Accuracy: 0.9991\n",
            "Epoch 7/10, Train Loss: 0.0060, Val Loss: 0.0036, Val Accuracy: 0.9990\n",
            "Epoch 8/10, Train Loss: 0.0053, Val Loss: 0.0026, Val Accuracy: 0.9991\n",
            "Epoch 9/10, Train Loss: 0.0052, Val Loss: 0.0095, Val Accuracy: 0.9972\n",
            "Epoch 10/10, Train Loss: 0.0048, Val Loss: 0.0028, Val Accuracy: 0.9990\n",
            "Training Complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cg7NmRk39iax"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}